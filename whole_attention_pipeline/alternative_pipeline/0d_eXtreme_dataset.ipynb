{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eXtreme Alternative Attention Dataset Preparation - Temperature-Sharpened MAX\n",
    "\n",
    "**Purpose:** Prepare dataset using temperature-sharpened MAX of concept similarities.\n",
    "\n",
    "**Key Difference from 0c_prepare_max**:\n",
    "- 0c: `post_score = max(similarity_to_each_concept)`\n",
    "- This (0d): `post_score = max(similarity_to_each_concept / COSINE_TEMPERATURE)`\n",
    "\n",
    "Temperature sharpening amplifies strong cosine similarities BEFORE taking MAX, creating more extreme post selection.\n",
    "Lower temperature → more extreme (winner-take-all), higher temperature → smoother.\n",
    "\n",
    "**Runtime:** ~40-50 minutes (same as 0c)\n",
    "\n",
    "This notebook:\n",
    "1. Loads training and test data from XML files\n",
    "2. Uses SBERT to retrieve top-50 concept-relevant posts per subject\n",
    "3. Applies temperature sharpening BEFORE MAX\n",
    "4. Pools post embeddings using temperature-sharpened attention weights\n",
    "5. Saves everything to `data/processed/extreme_alternative_attention_pipeline/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "# Detect device (MPS/CUDA/CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU (will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configured\n",
      "  Project root: /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study\n",
      "  Data save dir: /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/extreme_alternative_attention_pipeline\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATA_RAW = os.path.join(PROJECT_ROOT, \"data/raw\")\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "\n",
    "# Training data paths\n",
    "POS_DIR = os.path.join(DATA_RAW, \"train/positive_examples_anonymous_chunks\")\n",
    "NEG_DIR = os.path.join(DATA_RAW, \"train/negative_examples_anonymous_chunks\")\n",
    "\n",
    "# Test data paths\n",
    "TEST_DIR = os.path.join(DATA_RAW, \"test\")\n",
    "TEST_LABELS = os.path.join(TEST_DIR, \"test_golden_truth.txt\")\n",
    "\n",
    "# Concept labels\n",
    "CONCEPTS_FILE = os.path.join(DATA_PROCESSED, \"merged_questionnaires.csv\")\n",
    "\n",
    "# Output directory - CHANGED FOR EXTREME ALTERNATIVE PIPELINE\n",
    "SAVE_DIR = os.path.join(DATA_PROCESSED, \"extreme_alternative_attention_pipeline\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data save dir: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined 21 BDI-II concepts\n"
     ]
    }
   ],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters configured:\n",
      "  k_posts: 50\n",
      "  sbert_model: all-MiniLM-L6-v2\n",
      "  embedding_dim: 384\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    \"k_posts\": 50,              # Top-k posts per subject\n",
    "    \"sbert_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"embedding_dim\": 384,\n",
    "}\n",
    "# =========================\n",
    "# DEBUG / SANITY CHECK CONFIG\n",
    "# =========================\n",
    "DEBUG = True\n",
    "DEBUG_N_SUBJECTS = 3          # how many subjects to inspect\n",
    "DEBUG_TOP_N_POSTS = 5         # how many top posts to print\n",
    "DEBUG_PRINT_CONCEPTS = True   # print per-concept similarity stats\n",
    "\n",
    "print(\"✓ Hyperparameters configured:\")\n",
    "for k, v in HYPERPARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cosine temperature configured: 0.5\n",
      "  Lower temp = amplifies strong similarities (more extreme)\n",
      "  Higher temp = smoother similarity distribution\n"
     ]
    }
   ],
   "source": [
    "# Temperature sharpening parameter\n",
    "COSINE_TEMPERATURE = 0.5  # Lower = more extreme (winner-take-all), Higher = smoother\n",
    "# Try: 0.3 (extreme), 0.5 (moderate), 0.7 (mild)\n",
    "\n",
    "print(f\"✓ Cosine temperature configured: {COSINE_TEMPERATURE}\")\n",
    "print(f\"  Lower temp = amplifies strong similarities (more extreme)\")\n",
    "print(f\"  Higher temp = smoother similarity distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memory configuration:\n",
      "  post_batch_size: 32\n",
      "  subject_cache_interval: 10\n",
      "  use_no_grad: True\n",
      "  move_to_cpu_immediately: True\n"
     ]
    }
   ],
   "source": [
    "# Memory Management Configuration\n",
    "MEMORY_CONFIG = {\n",
    "    \"post_batch_size\": 32,        # Encode N posts at a time\n",
    "    \"subject_cache_interval\": 10,  # Clear GPU cache every N subjects\n",
    "    \"use_no_grad\": True,           # Disable gradient tracking\n",
    "    \"move_to_cpu_immediately\": True # Move results to CPU after computation\n",
    "}\n",
    "\n",
    "print(\"✓ Memory configuration:\")\n",
    "for k, v in MEMORY_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU cache clearing utility defined\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache and run garbage collection.\"\"\"\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    elif DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✓ GPU cache clearing utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Training Data\n",
    "\n",
    "Extract 486 training subjects with posts and concept labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for XML parsing\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing null chars and extra whitespace.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\u0000\", \"\")\n",
    "    text = WHITESPACE_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_posts_from_xml(xml_path, min_chars=10):\n",
    "    \"\"\"Extract posts from a single XML file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Failed to parse {xml_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    posts = []\n",
    "    for writing in root.findall(\"WRITING\"):\n",
    "        title = writing.findtext(\"TITLE\") or \"\"\n",
    "        text = writing.findtext(\"TEXT\") or \"\"\n",
    "        \n",
    "        combined = normalize_text(f\"{title} {text}\".strip())\n",
    "        if len(combined) >= min_chars:\n",
    "            posts.append(combined)\n",
    "    \n",
    "    return posts\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "  Processing positive examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing positive examples: 100%|██████████| 830/830 [00:00<00:00, 971.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 29868 posts from positive subjects\n",
      "  Processing negative examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing negative examples: 100%|██████████| 4031/4031 [00:05<00:00, 779.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded training data in 6.4s\n",
      "  Total posts: 286,740\n",
      "  Unique subjects: 486\n",
      "  Label distribution:\n",
      "label\n",
      "0    403\n",
      "1     83\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Parse training XML files\n",
    "print(\"Loading training data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_data = []\n",
    "\n",
    "# Process positive examples\n",
    "print(\"  Processing positive examples...\")\n",
    "pos_files = glob.glob(os.path.join(POS_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in tqdm(pos_files, desc=\"Processing positive examples\"):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 1,  # Positive (depression)\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "print(f\"  Loaded {sum(d['label'] == 1 for d in train_data)} posts from positive subjects\")\n",
    "\n",
    "# Process negative examples\n",
    "print(\"  Processing negative examples...\")\n",
    "neg_files = glob.glob(os.path.join(NEG_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in tqdm(neg_files, desc=\"Processing negative examples\"):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 0,  # Negative (control)\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "train_posts_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"\\n✓ Loaded training data in {time.time()-start_time:.1f}s\")\n",
    "print(f\"  Total posts: {len(train_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {train_posts_df['subject_id'].nunique()}\")\n",
    "print(f\"  Label distribution:\")\n",
    "print(train_posts_df.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading concept labels...\n",
      "✓ Loaded concept labels for 486 subjects\n"
     ]
    }
   ],
   "source": [
    "# Load concept labels from questionnaires\n",
    "print(\"Loading concept labels...\")\n",
    "\n",
    "concepts_df = pd.read_csv(CONCEPTS_FILE)\n",
    "concepts_df[\"subject_id\"] = concepts_df[\"Subject\"].str.replace(\"train_\", \"\", regex=True)\n",
    "\n",
    "# Binarize concept values\n",
    "concept_cols = [col for col in concepts_df.columns if col in CONCEPT_NAMES]\n",
    "for col in concept_cols:\n",
    "    concepts_df[col] = (concepts_df[col] > 0).astype(int)\n",
    "\n",
    "print(f\"✓ Loaded concept labels for {len(concepts_df)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Load Test Data\n",
    "\n",
    "Load all 401 test subjects from test folder (will be used entirely as test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test data...\n",
      "  Temp directory: /tmp/test_chunks_il2oiktf\n",
      "  Extracted chunk 3/10\n",
      "  Extracted chunk 6/10\n",
      "  Extracted chunk 9/10\n",
      "✓ Test data extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract test ZIP files to temporary directory\n",
    "print(\"Extracting test data...\")\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"test_chunks_\")\n",
    "print(f\"  Temp directory: {temp_dir}\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    zip_path = os.path.join(TEST_DIR, f\"chunk {i}.zip\")\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(temp_dir, f\"chunk_{i}\"))\n",
    "        if i % 3 == 0:\n",
    "            print(f\"  Extracted chunk {i}/10\")\n",
    "\n",
    "print(\"✓ Test data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test labels for 401 subjects\n",
      "  Label distribution:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load test labels\n",
    "test_labels_df = pd.read_csv(TEST_LABELS, sep='\\t', header=None, names=['subject_id', 'label'])\n",
    "test_labels_df['subject_id'] = test_labels_df['subject_id'].str.strip()\n",
    "\n",
    "print(f\"✓ Loaded test labels for {len(test_labels_df)} subjects\")\n",
    "print(f\"  Label distribution:\")\n",
    "print(test_labels_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test posts...\n",
      "  Found 4010 XML files\n",
      "✓ Loaded test posts\n",
      "  Total posts: 229,746\n",
      "  Unique subjects: 401\n"
     ]
    }
   ],
   "source": [
    "# Parse test XML files\n",
    "print(\"Loading test posts...\")\n",
    "test_data = []\n",
    "\n",
    "test_xml_files = glob.glob(os.path.join(temp_dir, \"**\", \"*.xml\"), recursive=True)\n",
    "print(f\"  Found {len(test_xml_files)} XML files\")\n",
    "\n",
    "for xml_file in test_xml_files:\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"(test_subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        label_row = test_labels_df[test_labels_df['subject_id'] == subject_id]\n",
    "        if len(label_row) > 0:\n",
    "            label = label_row.iloc[0]['label']\n",
    "            posts = extract_posts_from_xml(xml_file)\n",
    "            for post in posts:\n",
    "                test_data.append({\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"label\": label,\n",
    "                    \"text\": post\n",
    "                })\n",
    "\n",
    "test_posts_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"✓ Loaded test posts\")\n",
    "print(f\"  Total posts: {len(test_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {test_posts_df['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training data into train (80%) and validation (20%)...\n",
      "✓ Split complete\n",
      "  Training: 388 subjects (80% of original train)\n",
      "  Validation: 98 subjects (20% of original train)\n",
      "  Test: 401 subjects (100% of test folder)\n",
      "\n",
      "  Training label distribution:\n",
      "label\n",
      "0    322\n",
      "1     66\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "  Validation label distribution:\n",
      "label\n",
      "0    81\n",
      "1    17\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "  Test label distribution:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split TRAINING data into train and validation (80/20)\n",
    "print(\"Splitting training data into train (80%) and validation (20%)...\")\n",
    "\n",
    "train_subjects = train_posts_df.groupby('subject_id')['label'].first().reset_index()\n",
    "\n",
    "train_subjects_final, val_subjects = train_test_split(\n",
    "    train_subjects['subject_id'],\n",
    "    test_size=0.2,\n",
    "    stratify=train_subjects['label'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Create new train dataframe with only 80% of subjects\n",
    "train_posts_df_final = train_posts_df[train_posts_df['subject_id'].isin(train_subjects_final)].copy()\n",
    "\n",
    "# Create validation dataframe from remaining 20% of training subjects\n",
    "val_posts_df = train_posts_df[train_posts_df['subject_id'].isin(val_subjects)].copy()\n",
    "\n",
    "# Keep ALL test data as test set (no split)\n",
    "test_posts_df_final = test_posts_df.copy()\n",
    "\n",
    "print(f\"✓ Split complete\")\n",
    "print(f\"  Training: {train_posts_df_final['subject_id'].nunique()} subjects (80% of original train)\")\n",
    "print(f\"  Validation: {val_posts_df['subject_id'].nunique()} subjects (20% of original train)\")\n",
    "print(f\"  Test: {test_posts_df_final['subject_id'].nunique()} subjects (100% of test folder)\")\n",
    "\n",
    "# Show label distributions\n",
    "print(f\"\\n  Training label distribution:\")\n",
    "print(train_posts_df_final.groupby('label')['subject_id'].nunique())\n",
    "print(f\"\\n  Validation label distribution:\")\n",
    "print(val_posts_df.groupby('label')['subject_id'].nunique())\n",
    "print(f\"\\n  Test label distribution:\")\n",
    "print(test_posts_df_final.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: SBERT Setup & Concept Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model: all-MiniLM-L6-v2\n",
      "✓ SBERT model loaded on cuda\n",
      "  Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Load SBERT model\n",
    "print(f\"Loading SBERT model: {HYPERPARAMS['sbert_model']}\")\n",
    "sbert_model = SentenceTransformer(HYPERPARAMS['sbert_model'])\n",
    "sbert_model = sbert_model.to(DEVICE)\n",
    "\n",
    "print(f\"✓ SBERT model loaded on {DEVICE}\")\n",
    "print(f\"  Embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for 21 concepts...\n",
      "✓ Concept embeddings created\n",
      "  Shape: torch.Size([21, 384])\n"
     ]
    }
   ],
   "source": [
    "# Create concept embeddings\n",
    "print(f\"Creating embeddings for {N_CONCEPTS} concepts...\")\n",
    "concept_embeddings = sbert_model.encode(\n",
    "    CONCEPT_NAMES,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "print(f\"✓ Concept embeddings created\")\n",
    "print(f\"  Shape: {concept_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batched post retrieval function defined (MAX-based)\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k_posts_max(subject_id, posts_df, concept_embs, sbert, k=50, batch_size=32, debug=False):\n",
    "    \"\"\"\n",
    "    Retrieve top-k posts for a subject based on MAX of concept similarities.\n",
    "    OPTIMIZED: Uses batching to prevent memory exhaustion.\n",
    "    \n",
    "    For each post, takes MAX similarity across all 21 concepts.\n",
    "    Selects posts that are highly relevant to at least ONE concept.\n",
    "    \"\"\"\n",
    "    subj_posts = posts_df[posts_df['subject_id'] == subject_id]['text'].tolist()\n",
    "\n",
    "    if len(subj_posts) == 0:\n",
    "        return []\n",
    "\n",
    "    if len(subj_posts) <= k:\n",
    "        if len(subj_posts) < k:\n",
    "            extra_needed = k - len(subj_posts)\n",
    "            padding = list(np.random.choice(subj_posts, size=extra_needed, replace=True))\n",
    "            return subj_posts + padding\n",
    "        else:\n",
    "            return subj_posts\n",
    "\n",
    "    # Batch encoding to prevent memory issues\n",
    "    max_sim_scores = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for i in range(0, len(subj_posts), batch_size):\n",
    "            batch_posts = subj_posts[i:i + batch_size]\n",
    "\n",
    "            # Encode batch\n",
    "            batch_embeddings = sbert.encode(\n",
    "                batch_posts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "\n",
    "            # Compute similarities for this batch\n",
    "            cos_scores = util.cos_sim(batch_embeddings, concept_embs)  # [batch, 21]\n",
    "            # KEY: Take MAX instead of SUM\n",
    "            batch_max_scores = cos_scores.max(dim=1)[0].cpu().numpy()  # [0] gets values, not indices\n",
    "\n",
    "            max_sim_scores.extend(batch_max_scores)\n",
    "\n",
    "            # Clear references\n",
    "            del batch_embeddings, cos_scores, batch_max_scores\n",
    "\n",
    "    max_sim_scores = np.array(max_sim_scores)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"[DEBUG] Subject: {subject_id}\")\n",
    "        print(f\"[DEBUG] Total posts: {len(subj_posts)}\")\n",
    "        print(\"[DEBUG] Max similarity stats:\")\n",
    "        print(f\"  min={max_sim_scores.min():.4f} \"\n",
    "              f\"max={max_sim_scores.max():.4f} \"\n",
    "              f\"mean={max_sim_scores.mean():.4f} \"\n",
    "              f\"std={max_sim_scores.std():.4f}\")\n",
    "\n",
    "        top_idx_sorted = np.argsort(-max_sim_scores)\n",
    "        print(f\"\\n[DEBUG] Top-{DEBUG_TOP_N_POSTS} retrieved posts:\")\n",
    "        for rank, i in enumerate(top_idx_sorted[:DEBUG_TOP_N_POSTS]):\n",
    "            print(f\"\\n  Rank {rank+1}\")\n",
    "            print(f\"  Score: {max_sim_scores[i]:.4f}\")\n",
    "            print(f\"  Text: {subj_posts[i][:300]}\")\n",
    "\n",
    "    # Select top-k posts\n",
    "    top_k_indices = np.argpartition(-max_sim_scores, range(min(k, len(subj_posts))))[:k]\n",
    "\n",
    "    return [subj_posts[i] for i in top_k_indices]\n",
    "\n",
    "print(\"✓ Batched post retrieval function defined (MAX-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top-50 posts (MAX-based scoring with batching)...\n",
      "⏰ This will be faster and more memory-efficient\n",
      "  Processing training subjects (80% of train data)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects:   1%|          | 2/388 [00:00<00:46,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject1199\n",
      "[DEBUG] Total posts: 148\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0184 max=0.5202 mean=0.2018 std=0.0833\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.5202\n",
      "  Text: To be fair, I'm a recovering suicidal depressive and I disagree with you guys. Suicidal does not equal homicidal, so a suicide does not mean one less mass shooting. And depression does not make one an irresponsible gun owner. But being suicidal does. If you kill yourself, the universality of human r\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5140\n",
      "  Text: I know exactly the feeling. Crying is not bad. Feeling bad is bad. Crying is good: it helps us vent and to communicate that feeling with others. Consider communicating that feeling in other ways: art, music, journaling. It can be just as relieving. And if you need help, send me a pm. I'll do what I \n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4624\n",
      "  Text: Man, I don't want to take up a lot of your time, but this is such a great resource, I don't want to let you go. At this point in the story, the kid likes to hurt animals. Do you have any insight on that thought process or what kind of feelings would motivate someone to enjoy causing pain?\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4328\n",
      "  Text: 4) Last thing, I swear, because this post has gone on super long and its time to wrap it up. Little victories. For several months of my life, the only reason I was alive was because I really, really wanted to see the last Batman film. There is nothing wrong with this. People like me and you don't ha\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.4307\n",
      "  Text: The consensus here seems to be that these are poor arguments on an important topic. What could we do to strengthen the arguments? Definitions: Person: a rational, living actor. Suicide: the intentional act of ending one's life. Assisted suicide: planned euthanasia with the intent to lessen harm to b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects:   1%|          | 3/388 [00:00<02:23,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject669\n",
      "[DEBUG] Total posts: 995\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=-0.0026 max=0.5456 mean=0.1905 std=0.0782\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.5456\n",
      "  Text: I wondered the same thing with that trans kid who stepped out in front of a truck. Why scar someone else for life? I don't think people with suicidal depression give a whole lot of critical thought to their actions.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5202\n",
      "  Text: Do the tryptophan supplements make you tired at all?\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.5036\n",
      "  Text: Someone is actually using critical thinking in this thread.\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4840\n",
      "  Text: No tears, only Biden now.\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.4648\n",
      "  Text: The silence of disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects: 100%|██████████| 388/388 [03:25<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Processing validation subjects (20% of train data)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val subjects:   2%|▏         | 2/98 [00:01<01:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject5080\n",
      "[DEBUG] Total posts: 1167\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0073 max=0.5488 mean=0.1928 std=0.0894\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.5488\n",
      "  Text: There are plenty of times I don't eat when I'm hungry, but the constant fixation on food is what is really getting to me.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5355\n",
      "  Text: According to my doctor (on mobile so can't bring up studies myself) ALL types of hormonal birth controls have been shown to have zero effect on weight. Yes, the hormones could cause your appetite to increase and if you eat more calories than you need you'll gain weight. However the hormones in the p\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.5221\n",
      "  Text: No matter what I eat I am always ravenous I have done LCHF/keto/Atkins and the 70% fat intake didn't suppress my appetite. I've tried fiber, water, gum, protein, everything suggested on here that didn't require a trip to my doctor. I'm pretty sure I have some form of disordered eating. Literally the\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.5216\n",
      "  Text: Am I the only one here who has never been suicidal? I mean, being suicidal isn't a qualifier for every depression case. I've actually got a little hypochondria and have a fear of death, but every time I come across something about depression on reddit, someone mentions how they want to kill themselv\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.5212\n",
      "  Text: When do babies stop sleeping like newborns? My LO is 4 weeks old and every day he is a little more awake than the day before. When does his sleep schedule get more...normal? When should I start setting a designated \"bedtime\" and naptimes? Right now we are just playing it by ear based on when he eats\n",
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject1979\n",
      "[DEBUG] Total posts: 161\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0161 max=0.4732 mean=0.2201 std=0.0782\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4732\n",
      "  Text: How about we stop the suicide train?\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.4502\n",
      "  Text: My depression. My self hate, I'm so used to feeling this way that it's easier than anything else.\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4129\n",
      "  Text: *cries impatiently*\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4081\n",
      "  Text: I'm on /r/nosleep every night while lying in bed and 99% of the time I end up under the covers.\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.4069\n",
      "  Text: suicidal, but I know it's because I don't know how to feel. I starting using around 13 or 14...and before that had been diagnosed with major depressive disorder, an anxiety disorder and obsessive compulsive disorder. I started getting High to feel numb as repress everything, and I've basicaly condit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val subjects: 100%|██████████| 98/98 [00:55<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Processing test subjects (100% of test folder)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test subjects:   0%|          | 2/401 [00:01<04:17,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: test_subject2459\n",
      "[DEBUG] Total posts: 1146\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0031 max=0.5566 mean=0.1865 std=0.0817\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.5566\n",
      "  Text: I honestly can't remember who said it but I remember reading: I am not a a pessimist for they look at the world with negative views and thoughts. Nor am I an optimist for they look at the world and often try and see the good where there is none. I am a realist. I am happy when things are good and fa\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5510\n",
      "  Text: Aaah, well lets just say he is tired and wants you to simply ask him :)\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.5245\n",
      "  Text: You numb your dopamine receptors and it makes sex with real people less pleasureable and fun things in general but only with excessive masturbating. You **CAN** become dependent. It can lower self esteem and make you lazier (and more but this doesn't happen to everyone, like me I chose to stop for p\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4972\n",
      "  Text: Depends. If you cry when you get physically hurt e.g falling it's a sign of weakness. Cry emotionally e.g after the death of a loved one and it's a sign of humanity, love and strength.\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.4931\n",
      "  Text: Ways to get rid of sexual frustration? Hi, I'm 15 so I obviously haven't had sex yet and I don't want to... well not yet, not for at least another 3-5 years and I would only do it with a girl I really care about. Anyway I want to know if there's anyway to get rid of sexual frustration? Since some of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test subjects: 100%|██████████| 401/401 [03:41<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Post retrieval complete in 482.1s (8.0 min)\n",
      "  Memory-optimized processing: 887 subjects\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top-k posts for all subjects\n",
    "print(f\"Retrieving top-{HYPERPARAMS['k_posts']} posts (MAX-based scoring with batching)...\")\n",
    "print(\"⏰ This will be faster and more memory-efficient\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Training subjects (80% of original training data)\n",
    "print(\"  Processing training subjects (80% of train data)...\")\n",
    "train_selected = {}\n",
    "train_subjects = train_posts_df_final['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(train_subjects, desc=\"Train subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        train_posts_df_final,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    train_selected[subject_id] = selected\n",
    "\n",
    "    # Clear GPU cache periodically\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Validation subjects (20% of original training data)\n",
    "print(\"\\n  Processing validation subjects (20% of train data)...\")\n",
    "val_selected = {}\n",
    "val_subjects = val_posts_df['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(val_subjects, desc=\"Val subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        val_posts_df,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    val_selected[subject_id] = selected\n",
    "\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Test subjects (100% of test folder)\n",
    "print(\"\\n  Processing test subjects (100% of test folder)...\")\n",
    "test_selected = {}\n",
    "test_subjects = test_posts_df_final['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(test_subjects, desc=\"Test subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        test_posts_df_final,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    test_selected[subject_id] = selected\n",
    "\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Final cache clear\n",
    "clear_gpu_cache()\n",
    "\n",
    "print(f\"\\n✓ Post retrieval complete in {time.time()-start_time:.1f}s ({(time.time()-start_time)/60:.1f} min)\")\n",
    "print(f\"  Memory-optimized processing: {len(train_subjects) + len(val_subjects) + len(test_subjects)} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memory-optimized attention pooling function defined (MAX-based)\n"
     ]
    }
   ],
   "source": [
    "def encode_and_attention_pool_max(selected_posts_dict, sbert, concept_embs,\n",
    "                                   normalize=True, debug=False):\n",
    "    \"\"\"\n",
    "    Encode posts and pool using MAX of concept similarities for attention.\n",
    "    OPTIMIZED: Includes memory management for stability.\n",
    "    \n",
    "    For each post:\n",
    "    1. Compute similarities to all 21 concepts\n",
    "    2. Take MAX similarity as the post's relevance score\n",
    "    3. Use softmax(max_scores / temperature) for attention weights\n",
    "    4. Weighted sum pooling to create final embedding\n",
    "    \"\"\"\n",
    "    subject_ids = list(selected_posts_dict.keys())\n",
    "    pooled_embeddings = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for idx, subject_id in enumerate(subject_ids):\n",
    "            posts = selected_posts_dict[subject_id]\n",
    "\n",
    "            # Handle empty posts\n",
    "            if len(posts) == 0:\n",
    "                print(f\"WARNING: No posts for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(384))\n",
    "                continue\n",
    "\n",
    "            # Filter out empty posts\n",
    "            posts = [p for p in posts if p.strip()]\n",
    "            if len(posts) == 0:\n",
    "                print(f\"WARNING: All posts empty for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(384))\n",
    "                continue\n",
    "\n",
    "            # Encode posts\n",
    "            post_embs = sbert.encode(\n",
    "                posts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "\n",
    "            if post_embs.shape[0] == 0 or post_embs.shape[1] == 0:\n",
    "                print(f\"WARNING: Empty embeddings for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(384))\n",
    "                continue\n",
    "\n",
    "            # Compute similarity to concepts\n",
    "            cos_scores = util.cos_sim(post_embs, concept_embs)\n",
    "\n",
    "            # Temperature sharpening on cosine similarities\n",
    "            cos_scores = cos_scores / COSINE_TEMPERATURE\n",
    "\n",
    "            # Take MAX after sharpening\n",
    "\n",
    "            post_scores = cos_scores.max(dim=1)[0]  # [0] gets values, not indices\n",
    "\n",
    "            # Clamp after MAX\n",
    "\n",
    "            post_scores = torch.clamp(post_scores, min=0.0)\n",
    "\n",
    "            # Attention weights\n",
    "            TEMPERATURE = 0.2  \n",
    "            attn_weights = torch.softmax(post_scores / TEMPERATURE, dim=0)\n",
    "\n",
    "            if debug and idx < DEBUG_N_SUBJECTS:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(f\"[DEBUG][ATTENTION] Subject: {subject_id}\")\n",
    "                attn_np = attn_weights.cpu().numpy()\n",
    "                print(\"[DEBUG][ATTENTION] Weight stats:\")\n",
    "                print(f\"  min={attn_np.min():.6f} \"\n",
    "                      f\"max={attn_np.max():.6f} \"\n",
    "                      f\"mean={attn_np.mean():.6f} \"\n",
    "                      f\"entropy={-np.sum(attn_np * np.log(attn_np + 1e-12)):.4f}\")\n",
    "\n",
    "                top_attn_idx = np.argsort(-attn_np)[:DEBUG_TOP_N_POSTS]\n",
    "                print(f\"\\n[DEBUG][ATTENTION] Top-{DEBUG_TOP_N_POSTS} attended posts:\")\n",
    "                for rank, i in enumerate(top_attn_idx):\n",
    "                    print(f\"\\n  Rank {rank+1}\")\n",
    "                    print(f\"  Attention: {attn_np[i]:.6f}\")\n",
    "                    print(f\"  Text: {posts[i][:300]}\")\n",
    "\n",
    "            # Weighted sum pooling\n",
    "            pooled = torch.sum(attn_weights.unsqueeze(1) * post_embs, dim=0)\n",
    "            pooled_embeddings.append(pooled.cpu().numpy())\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del post_embs, cos_scores, attn_weights, pooled\n",
    "\n",
    "    return np.vstack(pooled_embeddings), subject_ids\n",
    "\n",
    "print(\"✓ Memory-optimized attention pooling function defined (MAX-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding and pooling embeddings (MAX-based attention, memory-optimized)...\n",
      "  Training set...\n",
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject7042\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.005281 max=0.058432 mean=0.020000 entropy=3.7100\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.058432\n",
      "  Text: Terrible title\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.058199\n",
      "  Text: Probably feels amazing. Female.\n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.054595\n",
      "  Text: I hope she feels better now :(\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.046873\n",
      "  Text: *Ruthless* adaptable pragmatism\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.043838\n",
      "  Text: ...Then you were in the right place.\n",
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject1199\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.007132 max=0.137633 mean=0.020000 entropy=3.4209\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.137633\n",
      "  Text: To be fair, I'm a recovering suicidal depressive and I disagree with you guys. Suicidal does not equal homicidal, so a suicide does not mean one less mass shooting. And depression does not make one an irresponsible gun owner. But being suicidal does. If you kill yourself, the universality of human r\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.129459\n",
      "  Text: I know exactly the feeling. Crying is not bad. Feeling bad is bad. Crying is good: it helps us vent and to communicate that feeling with others. Consider communicating that feeling in other ways: art, music, journaling. It can be just as relieving. And if you need help, send me a pm. I'll do what I \n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.077276\n",
      "  Text: Man, I don't want to take up a lot of your time, but this is such a great resource, I don't want to let you go. At this point in the story, the kid likes to hurt animals. Do you have any insight on that thought process or what kind of feelings would motivate someone to enjoy causing pain?\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.057427\n",
      "  Text: 4) Last thing, I swear, because this post has gone on super long and its time to wrap it up. Little victories. For several months of my life, the only reason I was alive was because I really, really wanted to see the last Batman film. There is nothing wrong with this. People like me and you don't ha\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.056234\n",
      "  Text: The consensus here seems to be that these are poor arguments on an important topic. What could we do to strengthen the arguments? Definitions: Person: a rational, living actor. Suicide: the intentional act of ending one's life. Assisted suicide: planned euthanasia with the intent to lessen harm to b\n",
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject669\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.009027 max=0.085147 mean=0.020000 entropy=3.6864\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.085147\n",
      "  Text: I wondered the same thing with that trans kid who stepped out in front of a truck. Why scar someone else for life? I don't think people with suicidal depression give a whole lot of critical thought to their actions.\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.066015\n",
      "  Text: Do the tryptophan supplements make you tired at all?\n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.055961\n",
      "  Text: Someone is actually using critical thinking in this thread.\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.045962\n",
      "  Text: No tears, only Biden now.\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.037957\n",
      "  Text: The silence of disappointment\n",
      "    X_train shape: (388, 384)\n",
      "  Validation set...\n",
      "    X_val shape: (98, 384)\n",
      "  Test set...\n",
      "    X_test shape: (401, 384)\n",
      "\n",
      "✓ Encoding complete in 33.4s (0.6 min)\n"
     ]
    }
   ],
   "source": [
    "# Encode and pool for all splits\n",
    "print(\"Encoding and pooling embeddings (MAX-based attention, memory-optimized)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"  Training set...\")\n",
    "X_train, train_subject_ids = encode_and_attention_pool_max(\n",
    "    train_selected,\n",
    "    sbert_model,\n",
    "    concept_embeddings,\n",
    "    normalize=True,\n",
    "    debug=DEBUG\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_train shape: {X_train.shape}\")\n",
    "\n",
    "print(\"  Validation set...\")\n",
    "X_val, val_subject_ids = encode_and_attention_pool_max(\n",
    "    val_selected, sbert_model, concept_embeddings, normalize=True\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_val shape: {X_val.shape}\")\n",
    "\n",
    "print(\"  Test set...\")\n",
    "X_test, test_subject_ids = encode_and_attention_pool_max(\n",
    "    test_selected, sbert_model, concept_embeddings, normalize=True\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\n✓ Encoding complete in {time.time()-start_time:.1f}s ({(time.time()-start_time)/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Build Concept Matrices and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building concept matrices and labels...\n",
      "✓ Matrices built\n",
      "  Train: X=(388, 384), C=(388, 21), y=(388,)\n",
      "  Val:   X=(98, 384), C=(98, 21), y=(98,)\n",
      "  Test:  X=(401, 384), C=(401, 21), y=(401,)\n",
      "\n",
      "  Training label distribution: [322  66]\n",
      "  Validation label distribution: [81 17]\n",
      "  Test label distribution: [349  52]\n"
     ]
    }
   ],
   "source": [
    "# Build concept matrices and label vectors\n",
    "print(\"Building concept matrices and labels...\")\n",
    "\n",
    "# Training: get concepts from questionnaires (80% of training data)\n",
    "C_train = []\n",
    "y_train = []\n",
    "for subject_id in train_subject_ids:\n",
    "    label = train_posts_df_final[train_posts_df_final['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_train.append(label)\n",
    "    \n",
    "    concept_row = concepts_df[concepts_df['subject_id'] == subject_id]\n",
    "    if len(concept_row) > 0:\n",
    "        concepts = concept_row[concept_cols].values[0]\n",
    "    else:\n",
    "        concepts = np.zeros(N_CONCEPTS)\n",
    "    C_train.append(concepts)\n",
    "\n",
    "C_train = np.array(C_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "# Validation: get concepts from questionnaires (20% of training data)\n",
    "C_val = []\n",
    "y_val = []\n",
    "for subject_id in val_subject_ids:\n",
    "    label = val_posts_df[val_posts_df['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_val.append(label)\n",
    "    \n",
    "    concept_row = concepts_df[concepts_df['subject_id'] == subject_id]\n",
    "    if len(concept_row) > 0:\n",
    "        concepts = concept_row[concept_cols].values[0]\n",
    "    else:\n",
    "        concepts = np.zeros(N_CONCEPTS)\n",
    "    C_val.append(concepts)\n",
    "\n",
    "C_val = np.array(C_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "\n",
    "# Test: zeros for concepts (no ground truth available)\n",
    "C_test = np.zeros((len(test_subject_ids), N_CONCEPTS), dtype=np.float32)\n",
    "y_test = []\n",
    "for subject_id in test_subject_ids:\n",
    "    label = test_posts_df_final[test_posts_df_final['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_test.append(label)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "print(\"✓ Matrices built\")\n",
    "print(f\"  Train: X={X_train.shape}, C={C_train.shape}, y={y_train.shape}\")\n",
    "print(f\"  Val:   X={X_val.shape}, C={C_val.shape}, y={y_val.shape}\")\n",
    "print(f\"  Test:  X={X_test.shape}, C={C_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\n  Training label distribution: {np.bincount(y_train.astype(int))}\")\n",
    "print(f\"  Validation label distribution: {np.bincount(y_val.astype(int))}\")\n",
    "print(f\"  Test label distribution: {np.bincount(y_test.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance:\n",
      "  Negative samples: 322\n",
      "  Positive samples: 66\n",
      "  Ratio: 1:4.88\n",
      "  Computed pos_weight: 4.8788\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "n_negative = int(np.sum(y_train == 0))\n",
    "n_positive = int(np.sum(y_train == 1))\n",
    "pos_weight = n_negative / n_positive\n",
    "\n",
    "print(f\"Class imbalance:\")\n",
    "print(f\"  Negative samples: {n_negative}\")\n",
    "print(f\"  Positive samples: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")\n",
    "print(f\"  Computed pos_weight: {pos_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Save All Datasets\n",
    "\n",
    "Save everything for fast loading by training pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets...\n",
      "✓ Datasets saved to /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/extreme_alternative_attention_pipeline\n",
      "  train_data.npz: 388 samples\n",
      "  val_data.npz:   98 samples\n",
      "  test_data.npz:  401 samples\n",
      "  class_weights.json\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets to disk\n",
    "print(\"Saving datasets...\")\n",
    "\n",
    "# Save numpy arrays\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"train_data.npz\"),\n",
    "    X=X_train,\n",
    "    C=C_train,\n",
    "    y=y_train,\n",
    "    subject_ids=np.array(train_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"val_data.npz\"),\n",
    "    X=X_val,\n",
    "    C=C_val,\n",
    "    y=y_val,\n",
    "    subject_ids=np.array(val_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"test_data.npz\"),\n",
    "    X=X_test,\n",
    "    C=C_test,\n",
    "    y=y_test,\n",
    "    subject_ids=np.array(test_subject_ids)\n",
    ")\n",
    "\n",
    "# Save class weights info\n",
    "class_info = {\n",
    "    \"n_positive\": n_positive,\n",
    "    \"n_negative\": n_negative,\n",
    "    \"pos_weight\": float(pos_weight)\n",
    "}\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"class_weights.json\"), 'w') as f:\n",
    "    json.dump(class_info, f, indent=4)\n",
    "\n",
    "print(f\"✓ Datasets saved to {SAVE_DIR}\")\n",
    "print(f\"  train_data.npz: {X_train.shape[0]} samples\")\n",
    "print(f\"  val_data.npz:   {X_val.shape[0]} samples\")\n",
    "print(f\"  test_data.npz:  {X_test.shape[0]} samples\")\n",
    "print(f\"  class_weights.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned up temporary directory: /tmp/test_chunks_il2oiktf\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary directory\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"✓ Cleaned up temporary directory: {temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Failed to clean up temporary directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "      EXTREME ALTERNATIVE DATASET PREPARATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Saved files:\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/extreme_alternative_attention_pipeline/train_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/extreme_alternative_attention_pipeline/val_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/extreme_alternative_attention_pipeline/test_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/extreme_alternative_attention_pipeline/class_weights.json\n",
      "\n",
      "Data split strategy:\n",
      "  - Training: 80% of train folder (~389 subjects)\n",
      "  - Validation: 20% of train folder (~97 subjects)\n",
      "  - Test: 100% of test folder (401 subjects)\n",
      "\n",
      "Key difference from 0c_prepare_max:\n",
      "  - Uses temperature-sharpened MAX of concept similarities\n",
      "  - Amplifies strong cosine similarities before MAX\n",
      "  - Temperature sharpening (T=0.5) creates more extreme post selection\n",
      "\n",
      "Use this data with CEM/CBM training notebooks!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"      EXTREME ALTERNATIVE DATASET PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSaved files:\")\n",
    "print(f\"  {SAVE_DIR}/train_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/val_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/test_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/class_weights.json\")\n",
    "print(\"\\nData split strategy:\")\n",
    "print(\"  - Training: 80% of train folder (~389 subjects)\")\n",
    "print(\"  - Validation: 20% of train folder (~97 subjects)\")\n",
    "print(\"  - Test: 100% of test folder (401 subjects)\")\n",
    "print(\"\\nKey difference from 0c_prepare_max:\")\n",
    "print(\"  - Uses temperature-sharpened MAX of concept similarities\")\n",
    "print(\"  - Amplifies strong cosine similarities before MAX\")\n",
    "print(f\"  - Temperature sharpening (T={COSINE_TEMPERATURE}) creates more extreme post selection\")\n",
    "print(\"\\nUse this data with CEM/CBM training notebooks!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
