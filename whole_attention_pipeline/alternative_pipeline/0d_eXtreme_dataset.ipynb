{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eXtreme Alternative Attention Dataset Preparation - Temperature-Sharpened MAX\n",
    "\n",
    "**Purpose:** Prepare dataset using temperature-sharpened MAX of concept similarities.\n",
    "\n",
    "**Key Difference from 0c_prepare_max**:\n",
    "- 0c: `post_score = max(similarity_to_each_concept)`\n",
    "- This (0d): `post_score = max(similarity_to_each_concept / COSINE_TEMPERATURE)`\n",
    "\n",
    "Temperature sharpening amplifies strong cosine similarities BEFORE taking MAX, creating more extreme post selection.\n",
    "Lower temperature \u2192 more extreme (winner-take-all), higher temperature \u2192 smoother.\n",
    "\n",
    "**Runtime:** ~40-50 minutes (same as 0c)\n",
    "\n",
    "This notebook:\n",
    "1. Loads training and test data from XML files\n",
    "2. Uses SBERT to retrieve top-50 concept-relevant posts per subject\n",
    "3. Applies temperature sharpening BEFORE MAX\n",
    "4. Pools post embeddings using temperature-sharpened attention weights\n",
    "5. Saves everything to `data/processed/extreme_alternative_attention_pipeline/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\u2713 All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"\u2713 Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Using CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "# Detect device (MPS/CUDA/CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"\u2713 Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"\u2713 Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"\u26a0 Using CPU (will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Paths configured\n",
      "  Project root: /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study\n",
      "  Data save dir: /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/max_alternative_attention_pipeline\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATA_RAW = os.path.join(PROJECT_ROOT, \"data/raw\")\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "\n",
    "# Training data paths\n",
    "POS_DIR = os.path.join(DATA_RAW, \"train/positive_examples_anonymous_chunks\")\n",
    "NEG_DIR = os.path.join(DATA_RAW, \"train/negative_examples_anonymous_chunks\")\n",
    "\n",
    "# Test data paths\n",
    "TEST_DIR = os.path.join(DATA_RAW, \"test\")\n",
    "TEST_LABELS = os.path.join(TEST_DIR, \"test_golden_truth.txt\")\n",
    "\n",
    "# Concept labels\n",
    "CONCEPTS_FILE = os.path.join(DATA_PROCESSED, \"merged_questionnaires.csv\")\n",
    "\n",
    "# Output directory - CHANGED FOR EXTREME ALTERNATIVE PIPELINE\n",
    "SAVE_DIR = os.path.join(DATA_PROCESSED, \"extreme_alternative_attention_pipeline\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\u2713 Paths configured\")\n",
    "print(f\"  Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data save dir: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Defined 21 BDI-II concepts\n"
     ]
    }
   ],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"\u2713 Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Hyperparameters configured:\n",
      "  k_posts: 50\n",
      "  sbert_model: all-MiniLM-L6-v2\n",
      "  embedding_dim: 384\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    \"k_posts\": 50,              # Top-k posts per subject\n",
    "    \"sbert_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"embedding_dim\": 384,\n",
    "}\n",
    "# =========================\n",
    "# DEBUG / SANITY CHECK CONFIG\n",
    "# =========================\n",
    "DEBUG = True\n",
    "DEBUG_N_SUBJECTS = 3          # how many subjects to inspect\n",
    "DEBUG_TOP_N_POSTS = 5         # how many top posts to print\n",
    "DEBUG_PRINT_CONCEPTS = True   # print per-concept similarity stats\n",
    "\n",
    "print(\"\u2713 Hyperparameters configured:\")\n",
    "for k, v in HYPERPARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature sharpening parameter\n",
    "COSINE_TEMPERATURE = 0.5  # Lower = more extreme (winner-take-all), Higher = smoother\n",
    "# Try: 0.3 (extreme), 0.5 (moderate), 0.7 (mild)\n",
    "\n",
    "print(f\"\u2713 Cosine temperature configured: {COSINE_TEMPERATURE}\")\n",
    "print(f\"  Lower temp = amplifies strong similarities (more extreme)\")\n",
    "print(f\"  Higher temp = smoother similarity distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Memory configuration:\n",
      "  post_batch_size: 32\n",
      "  subject_cache_interval: 10\n",
      "  use_no_grad: True\n",
      "  move_to_cpu_immediately: True\n"
     ]
    }
   ],
   "source": [
    "# Memory Management Configuration\n",
    "MEMORY_CONFIG = {\n",
    "    \"post_batch_size\": 32,        # Encode N posts at a time\n",
    "    \"subject_cache_interval\": 10,  # Clear GPU cache every N subjects\n",
    "    \"use_no_grad\": True,           # Disable gradient tracking\n",
    "    \"move_to_cpu_immediately\": True # Move results to CPU after computation\n",
    "}\n",
    "\n",
    "print(\"\u2713 Memory configuration:\")\n",
    "for k, v in MEMORY_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 GPU cache clearing utility defined\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache and run garbage collection.\"\"\"\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    elif DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\u2713 GPU cache clearing utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Training Data\n",
    "\n",
    "Extract 486 training subjects with posts and concept labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for XML parsing\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing null chars and extra whitespace.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\u0000\", \"\")\n",
    "    text = WHITESPACE_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_posts_from_xml(xml_path, min_chars=10):\n",
    "    \"\"\"Extract posts from a single XML file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Failed to parse {xml_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    posts = []\n",
    "    for writing in root.findall(\"WRITING\"):\n",
    "        title = writing.findtext(\"TITLE\") or \"\"\n",
    "        text = writing.findtext(\"TEXT\") or \"\"\n",
    "        \n",
    "        combined = normalize_text(f\"{title} {text}\".strip())\n",
    "        if len(combined) >= min_chars:\n",
    "            posts.append(combined)\n",
    "    \n",
    "    return posts\n",
    "\n",
    "print(\"\u2713 Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "  Processing positive examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing positive examples: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 830/830 [00:00<00:00, 1268.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 29868 posts from positive subjects\n",
      "  Processing negative examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing negative examples: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4031/4031 [00:05<00:00, 805.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2713 Loaded training data in 6.0s\n",
      "  Total posts: 286,740\n",
      "  Unique subjects: 486\n",
      "  Label distribution:\n",
      "label\n",
      "0    403\n",
      "1     83\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Parse training XML files\n",
    "print(\"Loading training data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_data = []\n",
    "\n",
    "# Process positive examples\n",
    "print(\"  Processing positive examples...\")\n",
    "pos_files = glob.glob(os.path.join(POS_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in tqdm(pos_files, desc=\"Processing positive examples\"):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 1,  # Positive (depression)\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "print(f\"  Loaded {sum(d['label'] == 1 for d in train_data)} posts from positive subjects\")\n",
    "\n",
    "# Process negative examples\n",
    "print(\"  Processing negative examples...\")\n",
    "neg_files = glob.glob(os.path.join(NEG_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in tqdm(neg_files, desc=\"Processing negative examples\"):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 0,  # Negative (control)\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "train_posts_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"\\n\u2713 Loaded training data in {time.time()-start_time:.1f}s\")\n",
    "print(f\"  Total posts: {len(train_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {train_posts_df['subject_id'].nunique()}\")\n",
    "print(f\"  Label distribution:\")\n",
    "print(train_posts_df.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading concept labels...\n",
      "\u2713 Loaded concept labels for 486 subjects\n"
     ]
    }
   ],
   "source": [
    "# Load concept labels from questionnaires\n",
    "print(\"Loading concept labels...\")\n",
    "\n",
    "concepts_df = pd.read_csv(CONCEPTS_FILE)\n",
    "concepts_df[\"subject_id\"] = concepts_df[\"Subject\"].str.replace(\"train_\", \"\", regex=True)\n",
    "\n",
    "# Binarize concept values\n",
    "concept_cols = [col for col in concepts_df.columns if col in CONCEPT_NAMES]\n",
    "for col in concept_cols:\n",
    "    concepts_df[col] = (concepts_df[col] > 0).astype(int)\n",
    "\n",
    "print(f\"\u2713 Loaded concept labels for {len(concepts_df)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Load Test Data\n",
    "\n",
    "Load all 401 test subjects from test folder (will be used entirely as test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test data...\n",
      "  Temp directory: /tmp/test_chunks_q60_s85r\n",
      "  Extracted chunk 3/10\n",
      "  Extracted chunk 6/10\n",
      "  Extracted chunk 9/10\n",
      "\u2713 Test data extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract test ZIP files to temporary directory\n",
    "print(\"Extracting test data...\")\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"test_chunks_\")\n",
    "print(f\"  Temp directory: {temp_dir}\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    zip_path = os.path.join(TEST_DIR, f\"chunk {i}.zip\")\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(temp_dir, f\"chunk_{i}\"))\n",
    "        if i % 3 == 0:\n",
    "            print(f\"  Extracted chunk {i}/10\")\n",
    "\n",
    "print(\"\u2713 Test data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Loaded test labels for 401 subjects\n",
      "  Label distribution:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load test labels\n",
    "test_labels_df = pd.read_csv(TEST_LABELS, sep='\\t', header=None, names=['subject_id', 'label'])\n",
    "test_labels_df['subject_id'] = test_labels_df['subject_id'].str.strip()\n",
    "\n",
    "print(f\"\u2713 Loaded test labels for {len(test_labels_df)} subjects\")\n",
    "print(f\"  Label distribution:\")\n",
    "print(test_labels_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test posts...\n",
      "  Found 4010 XML files\n",
      "\u2713 Loaded test posts\n",
      "  Total posts: 229,746\n",
      "  Unique subjects: 401\n"
     ]
    }
   ],
   "source": [
    "# Parse test XML files\n",
    "print(\"Loading test posts...\")\n",
    "test_data = []\n",
    "\n",
    "test_xml_files = glob.glob(os.path.join(temp_dir, \"**\", \"*.xml\"), recursive=True)\n",
    "print(f\"  Found {len(test_xml_files)} XML files\")\n",
    "\n",
    "for xml_file in test_xml_files:\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"(test_subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        label_row = test_labels_df[test_labels_df['subject_id'] == subject_id]\n",
    "        if len(label_row) > 0:\n",
    "            label = label_row.iloc[0]['label']\n",
    "            posts = extract_posts_from_xml(xml_file)\n",
    "            for post in posts:\n",
    "                test_data.append({\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"label\": label,\n",
    "                    \"text\": post\n",
    "                })\n",
    "\n",
    "test_posts_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"\u2713 Loaded test posts\")\n",
    "print(f\"  Total posts: {len(test_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {test_posts_df['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training data into train (80%) and validation (20%)...\n",
      "\u2713 Split complete\n",
      "  Training: 388 subjects (80% of original train)\n",
      "  Validation: 98 subjects (20% of original train)\n",
      "  Test: 401 subjects (100% of test folder)\n",
      "\n",
      "  Training label distribution:\n",
      "label\n",
      "0    322\n",
      "1     66\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "  Validation label distribution:\n",
      "label\n",
      "0    81\n",
      "1    17\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "  Test label distribution:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split TRAINING data into train and validation (80/20)\n",
    "print(\"Splitting training data into train (80%) and validation (20%)...\")\n",
    "\n",
    "train_subjects = train_posts_df.groupby('subject_id')['label'].first().reset_index()\n",
    "\n",
    "train_subjects_final, val_subjects = train_test_split(\n",
    "    train_subjects['subject_id'],\n",
    "    test_size=0.2,\n",
    "    stratify=train_subjects['label'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Create new train dataframe with only 80% of subjects\n",
    "train_posts_df_final = train_posts_df[train_posts_df['subject_id'].isin(train_subjects_final)].copy()\n",
    "\n",
    "# Create validation dataframe from remaining 20% of training subjects\n",
    "val_posts_df = train_posts_df[train_posts_df['subject_id'].isin(val_subjects)].copy()\n",
    "\n",
    "# Keep ALL test data as test set (no split)\n",
    "test_posts_df_final = test_posts_df.copy()\n",
    "\n",
    "print(f\"\u2713 Split complete\")\n",
    "print(f\"  Training: {train_posts_df_final['subject_id'].nunique()} subjects (80% of original train)\")\n",
    "print(f\"  Validation: {val_posts_df['subject_id'].nunique()} subjects (20% of original train)\")\n",
    "print(f\"  Test: {test_posts_df_final['subject_id'].nunique()} subjects (100% of test folder)\")\n",
    "\n",
    "# Show label distributions\n",
    "print(f\"\\n  Training label distribution:\")\n",
    "print(train_posts_df_final.groupby('label')['subject_id'].nunique())\n",
    "print(f\"\\n  Validation label distribution:\")\n",
    "print(val_posts_df.groupby('label')['subject_id'].nunique())\n",
    "print(f\"\\n  Test label distribution:\")\n",
    "print(test_posts_df_final.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: SBERT Setup & Concept Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model: all-MiniLM-L6-v2\n",
      "\u2713 SBERT model loaded on cuda\n",
      "  Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Load SBERT model\n",
    "print(f\"Loading SBERT model: {HYPERPARAMS['sbert_model']}\")\n",
    "sbert_model = SentenceTransformer(HYPERPARAMS['sbert_model'])\n",
    "sbert_model = sbert_model.to(DEVICE)\n",
    "\n",
    "print(f\"\u2713 SBERT model loaded on {DEVICE}\")\n",
    "print(f\"  Embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for 21 concepts...\n",
      "\u2713 Concept embeddings created\n",
      "  Shape: torch.Size([21, 384])\n"
     ]
    }
   ],
   "source": [
    "# Create concept embeddings\n",
    "print(f\"Creating embeddings for {N_CONCEPTS} concepts...\")\n",
    "concept_embeddings = sbert_model.encode(\n",
    "    CONCEPT_NAMES,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Concept embeddings created\")\n",
    "print(f\"  Shape: {concept_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Batched post retrieval function defined (MAX-based)\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k_posts_max(subject_id, posts_df, concept_embs, sbert, k=50, batch_size=32, debug=False):\n",
    "    \"\"\"\n",
    "    Retrieve top-k posts for a subject based on MAX of concept similarities.\n",
    "    OPTIMIZED: Uses batching to prevent memory exhaustion.\n",
    "    \n",
    "    For each post, takes MAX similarity across all 21 concepts.\n",
    "    Selects posts that are highly relevant to at least ONE concept.\n",
    "    \"\"\"\n",
    "    subj_posts = posts_df[posts_df['subject_id'] == subject_id]['text'].tolist()\n",
    "\n",
    "    if len(subj_posts) == 0:\n",
    "        return []\n",
    "\n",
    "    if len(subj_posts) <= k:\n",
    "        if len(subj_posts) < k:\n",
    "            extra_needed = k - len(subj_posts)\n",
    "            padding = list(np.random.choice(subj_posts, size=extra_needed, replace=True))\n",
    "            return subj_posts + padding\n",
    "        else:\n",
    "            return subj_posts\n",
    "\n",
    "    # Batch encoding to prevent memory issues\n",
    "    max_sim_scores = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for i in range(0, len(subj_posts), batch_size):\n",
    "            batch_posts = subj_posts[i:i + batch_size]\n",
    "\n",
    "            # Encode batch\n",
    "            batch_embeddings = sbert.encode(\n",
    "                batch_posts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "\n",
    "            # Compute similarities for this batch\n",
    "            cos_scores = util.cos_sim(batch_embeddings, concept_embs)  # [batch, 21]\n",
    "            # KEY: Take MAX instead of SUM\n",
    "            batch_max_scores = cos_scores.max(dim=1)[0].cpu().numpy()  # [0] gets values, not indices\n",
    "\n",
    "            max_sim_scores.extend(batch_max_scores)\n",
    "\n",
    "            # Clear references\n",
    "            del batch_embeddings, cos_scores, batch_max_scores\n",
    "\n",
    "    max_sim_scores = np.array(max_sim_scores)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"[DEBUG] Subject: {subject_id}\")\n",
    "        print(f\"[DEBUG] Total posts: {len(subj_posts)}\")\n",
    "        print(\"[DEBUG] Max similarity stats:\")\n",
    "        print(f\"  min={max_sim_scores.min():.4f} \"\n",
    "              f\"max={max_sim_scores.max():.4f} \"\n",
    "              f\"mean={max_sim_scores.mean():.4f} \"\n",
    "              f\"std={max_sim_scores.std():.4f}\")\n",
    "\n",
    "        top_idx_sorted = np.argsort(-max_sim_scores)\n",
    "        print(f\"\\n[DEBUG] Top-{DEBUG_TOP_N_POSTS} retrieved posts:\")\n",
    "        for rank, i in enumerate(top_idx_sorted[:DEBUG_TOP_N_POSTS]):\n",
    "            print(f\"\\n  Rank {rank+1}\")\n",
    "            print(f\"  Score: {max_sim_scores[i]:.4f}\")\n",
    "            print(f\"  Text: {subj_posts[i][:300]}\")\n",
    "\n",
    "    # Select top-k posts\n",
    "    top_k_indices = np.argpartition(-max_sim_scores, range(min(k, len(subj_posts))))[:k]\n",
    "\n",
    "    return [subj_posts[i] for i in top_k_indices]\n",
    "\n",
    "print(\"\u2713 Batched post retrieval function defined (MAX-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top-50 posts (MAX-based scoring with batching)...\n",
      "\u23f0 This will be faster and more memory-efficient\n",
      "  Processing training subjects (80% of train data)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects:   1%|          | 2/388 [00:01<04:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject9115\n",
      "[DEBUG] Total posts: 1254\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=-0.0131 max=0.6024 mean=0.2002 std=0.0868\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.6024\n",
      "  Text: As someone who knows what it's like to suffer depression, I've always condoned suicide if the person really feels they have nothing left.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5655\n",
      "  Text: \"It's a permanent solution to a temporary problem\" really angers me. I'm talking about suicide, of course. My mom says that phrase all the damn time, and I just want to say, \"Or it could be a permanent solution to a problem that never goes away and eats at you until you have to do something to end i\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.5450\n",
      "  Text: \"Suicide is a permanent solution to a temporary problem.\"\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.5372\n",
      "  Text: What is the purpose of crying? I understand you might cry if you need to get something out of your eye, but what purpose does crying when you're hurt or because you're sad serve? Oh, I don't mean why do people cry, I mean why is that a response that's possible?\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.5305\n",
      "  Text: My views of suicide...extremely triggering, please don't take this warning lightly I really don't mean for this to be a pro-suicide message. I was just wondering if anyone else ever felt this way. Most people see what they think as their eventual suicide in a negative light, but I've seen it positiv\n",
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject7326\n",
      "[DEBUG] Total posts: 118\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0131 max=0.4619 mean=0.1764 std=0.0727\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4619\n",
      "  Text: Thank you for sharing. I'll look into that link deeper tomorrow, but I always thought that maybe it's my biological clock that is not working correctly rather than the actual sleep. I can sleep during afternoon quite easily, i.e. something like 2pm-6pm, but it's so hard to do at night...\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.3985\n",
      "  Text: I have similar experiences. I have often HUGE problems falling asleep and lying around waiting until my body decides it's ok now to fall asleep is just super annoying and exhausting for me. I'll mostly procrastinate at the PC too - \"let's just check reddit\" \"let's just answer this mail\", \"let's just\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.3252\n",
      "  Text: \"actively sabotage\" lol. He offered her food. That's about it. He didn't fill her food with hidden chocolate and neither did he shove down pizza down her throat when she was asleep. There's NO point of his behavior that would have done anything if she just said \"no thanks.\" Also, would be interestin\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.3210\n",
      "  Text: History. Actually I'm in 2nd semester already, but better late than never is guess...?\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.3041\n",
      "  Text: I actually DO think that playing ego-shooters increases your willingness to use violence and causes emotional blunting. edit: gettin' downvoted on an unpopular opinion thread, i did it reddit! ppl playing shooters are mad as fuck.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 388/388 [03:16<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Processing validation subjects (20% of train data)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val subjects:   3%|\u258e         | 3/98 [00:00<00:28,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject1095\n",
      "[DEBUG] Total posts: 1019\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0050 max=0.4646 mean=0.1622 std=0.0623\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4646\n",
      "  Text: The quality of 0 is still null.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.4530\n",
      "  Text: I lost my virginity.\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4064\n",
      "  Text: Suiciders has been pretty good.\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4053\n",
      "  Text: It is a thing. But you totally probably have a sadness fetish.\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.3968\n",
      "  Text: Older women having high libido, being so rare, it gets its own term, hence \"cougar\"?\n",
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject1457\n",
      "[DEBUG] Total posts: 93\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0673 max=0.4384 mean=0.2524 std=0.0822\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4384\n",
      "  Text: When I was a teenager, my method of suicide was drugs. I thought I had pumped enough into my system to just finally end it all. But then I woke up. The immediate thoughts were \"I'm such a fuck-up I can't even kill myself right.\" It was - for me at least - embarrassing. And I never told anyone about \n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.4380\n",
      "  Text: Agreed - as a fan of the genre myself, that does sound very metal. And as for what deertribe said, they're right. The sheer fact that you're acknowledging your depression and that it's effecting you in such a way is a step in the right direction. The fact that you chose to sit down and express yours\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4165\n",
      "  Text: I'm not an expert, but... maybe talk to your doctor about the meds you're taking? Again, not an expert, but what I did noticed is the marked change from \"crushing sadness\" to just \"meh\", with you being put on medications right in the middle of that. Again, not an expert, but potential possibility?\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4008\n",
      "  Text: Thank you for that. And I know it's hard to watch your mother cry - when my uncle and grandfather (her father) died, she was in tears almost constantly, which had me in tears constantly. So instead of talking, she just held me in her arms and we cried together. During this time of grief for your fam\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.3944\n",
      "  Text: Happens to me too when I witness massive amounts of love and compassion. I've found myself fighting tears more than a few times today while exploring this sub. You're not alone there. :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val subjects: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98/98 [00:52<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Processing test subjects (100% of test folder)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test subjects:   0%|          | 2/401 [00:00<01:49,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: test_subject6048\n",
      "[DEBUG] Total posts: 621\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0102 max=0.4379 mean=0.1624 std=0.0683\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4379\n",
      "  Text: Came here to say basically the same thing- so frustrated at the moment.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.4201\n",
      "  Text: Your probably waking up pretty dehydrated like most people, which is probably making your muscles pretty cramped. Combine this with having your body being completely stationary for the last 8 or so hours and you're gonna have a bumpy start. I'd say just keep getting up and doing it, your body will n\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4090\n",
      "  Text: How was the sex?\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.3784\n",
      "  Text: It's an illness, probably has diarrhea\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.3749\n",
      "  Text: Its pretty much become impossible not to like him.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test subjects: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 401/401 [03:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2713 Post retrieval complete in 459.7s (7.7 min)\n",
      "  Memory-optimized processing: 887 subjects\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top-k posts for all subjects\n",
    "print(f\"Retrieving top-{HYPERPARAMS['k_posts']} posts (MAX-based scoring with batching)...\")\n",
    "print(\"\u23f0 This will be faster and more memory-efficient\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Training subjects (80% of original training data)\n",
    "print(\"  Processing training subjects (80% of train data)...\")\n",
    "train_selected = {}\n",
    "train_subjects = train_posts_df_final['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(train_subjects, desc=\"Train subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        train_posts_df_final,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    train_selected[subject_id] = selected\n",
    "\n",
    "    # Clear GPU cache periodically\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Validation subjects (20% of original training data)\n",
    "print(\"\\n  Processing validation subjects (20% of train data)...\")\n",
    "val_selected = {}\n",
    "val_subjects = val_posts_df['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(val_subjects, desc=\"Val subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        val_posts_df,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    val_selected[subject_id] = selected\n",
    "\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Test subjects (100% of test folder)\n",
    "print(\"\\n  Processing test subjects (100% of test folder)...\")\n",
    "test_selected = {}\n",
    "test_subjects = test_posts_df_final['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(test_subjects, desc=\"Test subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        test_posts_df_final,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    test_selected[subject_id] = selected\n",
    "\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Final cache clear\n",
    "clear_gpu_cache()\n",
    "\n",
    "print(f\"\\n\u2713 Post retrieval complete in {time.time()-start_time:.1f}s ({(time.time()-start_time)/60:.1f} min)\")\n",
    "print(f\"  Memory-optimized processing: {len(train_subjects) + len(val_subjects) + len(test_subjects)} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Memory-optimized attention pooling function defined (MAX-based)\n"
     ]
    }
   ],
   "source": [
    "def encode_and_attention_pool_max(selected_posts_dict, sbert, concept_embs,\n",
    "                                   normalize=True, debug=False):\n",
    "    \"\"\"\n",
    "    Encode posts and pool using MAX of concept similarities for attention.\n",
    "    OPTIMIZED: Includes memory management for stability.\n",
    "    \n",
    "    For each post:\n",
    "    1. Compute similarities to all 21 concepts\n",
    "    2. Take MAX similarity as the post's relevance score\n",
    "    3. Use softmax(max_scores / temperature) for attention weights\n",
    "    4. Weighted sum pooling to create final embedding\n",
    "    \"\"\"\n",
    "    subject_ids = list(selected_posts_dict.keys())\n",
    "    pooled_embeddings = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for idx, subject_id in enumerate(subject_ids):\n",
    "            posts = selected_posts_dict[subject_id]\n",
    "\n",
    "            # Handle empty posts\n",
    "            if len(posts) == 0:\n",
    "                print(f\"WARNING: No posts for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(384))\n",
    "                continue\n",
    "\n",
    "            # Filter out empty posts\n",
    "            posts = [p for p in posts if p.strip()]\n",
    "            if len(posts) == 0:\n",
    "                print(f\"WARNING: All posts empty for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(384))\n",
    "                continue\n",
    "\n",
    "            # Encode posts\n",
    "            post_embs = sbert.encode(\n",
    "                posts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "\n",
    "            if post_embs.shape[0] == 0 or post_embs.shape[1] == 0:\n",
    "                print(f\"WARNING: Empty embeddings for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(384))\n",
    "                continue\n",
    "\n",
    "            # Compute similarity to concepts\n",
    "            cos_scores = util.cos_sim(post_embs, concept_embs)\n",
    "\n",
    "            # Temperature sharpening on cosine similarities\n",
    "            cos_scores = cos_scores / COSINE_TEMPERATURE\n",
    "\n",
    "            # Take MAX after sharpening\n",
    "\n",
    "            post_scores = cos_scores.max(dim=1)[0]  # [0] gets values, not indices\n",
    "\n",
    "            # Clamp after MAX\n",
    "\n",
    "            post_scores = torch.clamp(post_scores, min=0.0)\n",
    "\n",
    "            # Attention weights\n",
    "            TEMPERATURE = 0.2  \n",
    "            attn_weights = torch.softmax(post_scores / TEMPERATURE, dim=0)\n",
    "\n",
    "            if debug and idx < DEBUG_N_SUBJECTS:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(f\"[DEBUG][ATTENTION] Subject: {subject_id}\")\n",
    "                attn_np = attn_weights.cpu().numpy()\n",
    "                print(\"[DEBUG][ATTENTION] Weight stats:\")\n",
    "                print(f\"  min={attn_np.min():.6f} \"\n",
    "                      f\"max={attn_np.max():.6f} \"\n",
    "                      f\"mean={attn_np.mean():.6f} \"\n",
    "                      f\"entropy={-np.sum(attn_np * np.log(attn_np + 1e-12)):.4f}\")\n",
    "\n",
    "                top_attn_idx = np.argsort(-attn_np)[:DEBUG_TOP_N_POSTS]\n",
    "                print(f\"\\n[DEBUG][ATTENTION] Top-{DEBUG_TOP_N_POSTS} attended posts:\")\n",
    "                for rank, i in enumerate(top_attn_idx):\n",
    "                    print(f\"\\n  Rank {rank+1}\")\n",
    "                    print(f\"  Attention: {attn_np[i]:.6f}\")\n",
    "                    print(f\"  Text: {posts[i][:300]}\")\n",
    "\n",
    "            # Weighted sum pooling\n",
    "            pooled = torch.sum(attn_weights.unsqueeze(1) * post_embs, dim=0)\n",
    "            pooled_embeddings.append(pooled.cpu().numpy())\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del post_embs, cos_scores, attn_weights, pooled\n",
    "\n",
    "    return np.vstack(pooled_embeddings), subject_ids\n",
    "\n",
    "print(\"\u2713 Memory-optimized attention pooling function defined (MAX-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding and pooling embeddings (MAX-based attention, memory-optimized)...\n",
      "  Training set...\n",
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject9115\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.014175 max=0.044475 mean=0.020000 entropy=3.8648\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.044475\n",
      "  Text: As someone who knows what it's like to suffer depression, I've always condoned suicide if the person really feels they have nothing left.\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.036982\n",
      "  Text: \"It's a permanent solution to a temporary problem\" really angers me. I'm talking about suicide, of course. My mom says that phrase all the damn time, and I just want to say, \"Or it could be a permanent solution to a problem that never goes away and eats at you until you have to do something to end i\n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.033384\n",
      "  Text: \"Suicide is a permanent solution to a temporary problem.\"\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.032098\n",
      "  Text: What is the purpose of crying? I understand you might cry if you need to get something out of your eye, but what purpose does crying when you're hurt or because you're sad serve? Oh, I don't mean why do people cry, I mean why is that a response that's possible?\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.031047\n",
      "  Text: My views of suicide...extremely triggering, please don't take this warning lightly I really don't mean for this to be a pro-suicide message. I was just wondering if anyone else ever felt this way. Most people see what they think as their eventual suicide in a negative light, but I've seen it positiv\n",
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject7326\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.014403 max=0.057679 mean=0.020000 entropy=3.8593\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.057679\n",
      "  Text: Thank you for sharing. I'll look into that link deeper tomorrow, but I always thought that maybe it's my biological clock that is not working correctly rather than the actual sleep. I can sleep during afternoon quite easily, i.e. something like 2pm-6pm, but it's so hard to do at night...\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.041993\n",
      "  Text: I have similar experiences. I have often HUGE problems falling asleep and lying around waiting until my body decides it's ok now to fall asleep is just super annoying and exhausting for me. I'll mostly procrastinate at the PC too - \"let's just check reddit\" \"let's just answer this mail\", \"let's just\n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.029116\n",
      "  Text: \"actively sabotage\" lol. He offered her food. That's about it. He didn't fill her food with hidden chocolate and neither did he shove down pizza down her throat when she was asleep. There's NO point of his behavior that would have done anything if she just said \"no thanks.\" Also, would be interestin\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.028512\n",
      "  Text: History. Actually I'm in 2nd semester already, but better late than never is guess...?\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.026203\n",
      "  Text: I actually DO think that playing ego-shooters increases your willingness to use violence and causes emotional blunting. edit: gettin' downvoted on an unpopular opinion thread, i did it reddit! ppl playing shooters are mad as fuck.\n",
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject416\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.007495 max=0.034760 mean=0.020000 entropy=3.8329\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.034760\n",
      "  Text: Its really weird, a year ago I couldn't even have imagined ever being this happy again. Even if I thought things were better, I still felt really bad and just assumed this is what \"normal\" is like. It feels really strange sometimes now.\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.034760\n",
      "  Text: Its really weird, a year ago I couldn't even have imagined ever being this happy again. Even if I thought things were better, I still felt really bad and just assumed this is what \"normal\" is like. It feels really strange sometimes now.\n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.032526\n",
      "  Text: I think it was a combination of both, the lack of energy I felt from being depressed as well as the incredible social anxiety of meeting somebdoy who knows me who might ask me about what the fuck I am doing and why I have been hiding for weeks ... Some days I was not able to get out of bed and I mis\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.032355\n",
      "  Text: Wednesday it was a holiday here and I went sports climbing. When I came home I was so tired, I sat on my little balcony and the sun was shining in my face, this song http://www.youtube.com/watch?v=CTX1HJI6n0A was playing in the background and I was suddenly thinking \"You know, this feels pretty good\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.032355\n",
      "  Text: Wednesday it was a holiday here and I went sports climbing. When I came home I was so tired, I sat on my little balcony and the sun was shining in my face, this song http://www.youtube.com/watch?v=CTX1HJI6n0A was playing in the background and I was suddenly thinking \"You know, this feels pretty good\n",
      "    X_train shape: (388, 384)\n",
      "  Validation set...\n",
      "    X_val shape: (98, 384)\n",
      "  Test set...\n",
      "    X_test shape: (401, 384)\n",
      "\n",
      "\u2713 Encoding complete in 32.0s (0.5 min)\n"
     ]
    }
   ],
   "source": [
    "# Encode and pool for all splits\n",
    "print(\"Encoding and pooling embeddings (MAX-based attention, memory-optimized)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"  Training set...\")\n",
    "X_train, train_subject_ids = encode_and_attention_pool_max(\n",
    "    train_selected,\n",
    "    sbert_model,\n",
    "    concept_embeddings,\n",
    "    normalize=True,\n",
    "    debug=DEBUG\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_train shape: {X_train.shape}\")\n",
    "\n",
    "print(\"  Validation set...\")\n",
    "X_val, val_subject_ids = encode_and_attention_pool_max(\n",
    "    val_selected, sbert_model, concept_embeddings, normalize=True\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_val shape: {X_val.shape}\")\n",
    "\n",
    "print(\"  Test set...\")\n",
    "X_test, test_subject_ids = encode_and_attention_pool_max(\n",
    "    test_selected, sbert_model, concept_embeddings, normalize=True\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\n\u2713 Encoding complete in {time.time()-start_time:.1f}s ({(time.time()-start_time)/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Build Concept Matrices and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building concept matrices and labels...\n",
      "\u2713 Matrices built\n",
      "  Train: X=(388, 384), C=(388, 21), y=(388,)\n",
      "  Val:   X=(98, 384), C=(98, 21), y=(98,)\n",
      "  Test:  X=(401, 384), C=(401, 21), y=(401,)\n",
      "\n",
      "  Training label distribution: [322  66]\n",
      "  Validation label distribution: [81 17]\n",
      "  Test label distribution: [349  52]\n"
     ]
    }
   ],
   "source": [
    "# Build concept matrices and label vectors\n",
    "print(\"Building concept matrices and labels...\")\n",
    "\n",
    "# Training: get concepts from questionnaires (80% of training data)\n",
    "C_train = []\n",
    "y_train = []\n",
    "for subject_id in train_subject_ids:\n",
    "    label = train_posts_df_final[train_posts_df_final['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_train.append(label)\n",
    "    \n",
    "    concept_row = concepts_df[concepts_df['subject_id'] == subject_id]\n",
    "    if len(concept_row) > 0:\n",
    "        concepts = concept_row[concept_cols].values[0]\n",
    "    else:\n",
    "        concepts = np.zeros(N_CONCEPTS)\n",
    "    C_train.append(concepts)\n",
    "\n",
    "C_train = np.array(C_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "# Validation: get concepts from questionnaires (20% of training data)\n",
    "C_val = []\n",
    "y_val = []\n",
    "for subject_id in val_subject_ids:\n",
    "    label = val_posts_df[val_posts_df['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_val.append(label)\n",
    "    \n",
    "    concept_row = concepts_df[concepts_df['subject_id'] == subject_id]\n",
    "    if len(concept_row) > 0:\n",
    "        concepts = concept_row[concept_cols].values[0]\n",
    "    else:\n",
    "        concepts = np.zeros(N_CONCEPTS)\n",
    "    C_val.append(concepts)\n",
    "\n",
    "C_val = np.array(C_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "\n",
    "# Test: zeros for concepts (no ground truth available)\n",
    "C_test = np.zeros((len(test_subject_ids), N_CONCEPTS), dtype=np.float32)\n",
    "y_test = []\n",
    "for subject_id in test_subject_ids:\n",
    "    label = test_posts_df_final[test_posts_df_final['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_test.append(label)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "print(\"\u2713 Matrices built\")\n",
    "print(f\"  Train: X={X_train.shape}, C={C_train.shape}, y={y_train.shape}\")\n",
    "print(f\"  Val:   X={X_val.shape}, C={C_val.shape}, y={y_val.shape}\")\n",
    "print(f\"  Test:  X={X_test.shape}, C={C_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\n  Training label distribution: {np.bincount(y_train.astype(int))}\")\n",
    "print(f\"  Validation label distribution: {np.bincount(y_val.astype(int))}\")\n",
    "print(f\"  Test label distribution: {np.bincount(y_test.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance:\n",
      "  Negative samples: 322\n",
      "  Positive samples: 66\n",
      "  Ratio: 1:4.88\n",
      "  Computed pos_weight: 4.8788\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "n_negative = int(np.sum(y_train == 0))\n",
    "n_positive = int(np.sum(y_train == 1))\n",
    "pos_weight = n_negative / n_positive\n",
    "\n",
    "print(f\"Class imbalance:\")\n",
    "print(f\"  Negative samples: {n_negative}\")\n",
    "print(f\"  Positive samples: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")\n",
    "print(f\"  Computed pos_weight: {pos_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Save All Datasets\n",
    "\n",
    "Save everything for fast loading by training pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets...\n",
      "\u2713 Datasets saved to /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/max_alternative_attention_pipeline\n",
      "  train_data.npz: 388 samples\n",
      "  val_data.npz:   98 samples\n",
      "  test_data.npz:  401 samples\n",
      "  class_weights.json\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets to disk\n",
    "print(\"Saving datasets...\")\n",
    "\n",
    "# Save numpy arrays\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"train_data.npz\"),\n",
    "    X=X_train,\n",
    "    C=C_train,\n",
    "    y=y_train,\n",
    "    subject_ids=np.array(train_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"val_data.npz\"),\n",
    "    X=X_val,\n",
    "    C=C_val,\n",
    "    y=y_val,\n",
    "    subject_ids=np.array(val_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"test_data.npz\"),\n",
    "    X=X_test,\n",
    "    C=C_test,\n",
    "    y=y_test,\n",
    "    subject_ids=np.array(test_subject_ids)\n",
    ")\n",
    "\n",
    "# Save class weights info\n",
    "class_info = {\n",
    "    \"n_positive\": n_positive,\n",
    "    \"n_negative\": n_negative,\n",
    "    \"pos_weight\": float(pos_weight)\n",
    "}\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"class_weights.json\"), 'w') as f:\n",
    "    json.dump(class_info, f, indent=4)\n",
    "\n",
    "print(f\"\u2713 Datasets saved to {SAVE_DIR}\")\n",
    "print(f\"  train_data.npz: {X_train.shape[0]} samples\")\n",
    "print(f\"  val_data.npz:   {X_val.shape[0]} samples\")\n",
    "print(f\"  test_data.npz:  {X_test.shape[0]} samples\")\n",
    "print(f\"  class_weights.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Cleaned up temporary directory: /tmp/test_chunks_q60_s85r\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary directory\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"\u2713 Cleaned up temporary directory: {temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 Failed to clean up temporary directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "      MAX ALTERNATIVE DATASET PREPARATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Saved files:\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/max_alternative_attention_pipeline/train_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/max_alternative_attention_pipeline/val_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/max_alternative_attention_pipeline/test_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/max_alternative_attention_pipeline/class_weights.json\n",
      "\n",
      "Data split strategy:\n",
      "  - Training: 80% of train folder (~389 subjects)\n",
      "  - Validation: 20% of train folder (~97 subjects)\n",
      "  - Test: 100% of test folder (401 subjects)\n",
      "\n",
      "Key difference from original:\n",
      "  - Uses MAX of concept similarities instead of SUM\n",
      "  - Captures posts highly relevant to at least ONE concept\n",
      "  - Focuses on 'specialist' posts rather than 'generalist' posts\n",
      "\n",
      "Use this data with CEM/CBM training notebooks!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"      EXTREME ALTERNATIVE DATASET PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSaved files:\")\n",
    "print(f\"  {SAVE_DIR}/train_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/val_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/test_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/class_weights.json\")\n",
    "print(\"\\nData split strategy:\")\n",
    "print(\"  - Training: 80% of train folder (~389 subjects)\")\n",
    "print(\"  - Validation: 20% of train folder (~97 subjects)\")\n",
    "print(\"  - Test: 100% of test folder (401 subjects)\")\n",
    "print(\"\\nKey difference from 0c_prepare_max:\")\n",
    "print(\"  - Uses temperature-sharpened MAX of concept similarities\")\n",
    "print(\"  - Amplifies strong cosine similarities before MAX\")\n",
    "print(f\"  - Temperature sharpening (T={COSINE_TEMPERATURE}) creates more extreme post selection\")\n",
    "print(\"\\nUse this data with CEM/CBM training notebooks!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}