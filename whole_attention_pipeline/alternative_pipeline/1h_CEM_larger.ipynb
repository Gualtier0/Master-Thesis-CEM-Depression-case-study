{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CEM Model - Larger MPNet + 2022 Data (768-dim)\n\n**Runtime:** ~30-40 minutes (longer due to larger embeddings and expanded dataset)\n\nThis notebook:\n1. Trains **CEM** using larger MPNet-based concept similarity data with 2022 data added (768-dim MAX)\n2. Uses **LDAM Loss** for severe class imbalance\n3. Tests threshold optimization to maximize minority class detection\n4. Evaluates on test set with balanced metrics\n\n**Data Source:** `larger2022_max_alternative_attention_pipeline` (MPNet, 768-dim)\n- Model: all-mpnet-base-v2 (768 dimensions)\n- Higher quality embeddings than MiniLM (384-dim)\n- Uses MAX of concept similarities\n- Includes 2022 data (~1,400 subjects added to training pool)\n\n**Prerequisites:** Run `0i_larger2022_max_dataset.ipynb` first!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define paths\nPROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\nDATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\nDATASET_DIR = os.path.join(DATA_PROCESSED, \"larger2022_max_alternative_attention_pipeline\")\nOUTPUT_DIR = \"outputs_cem_larger2022\"  # Using 2022 expanded dataset\n\nprint(\"✓ Paths configured\")\nprint(f\"  Dataset dir: {DATASET_DIR}\")\nprint(f\"  Output dir: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    # Model architecture\n",
    "    \"embedding_dim\": 768,\n",
    "    \"n_concepts\": 21,\n",
    "    \"n_tasks\": 1,\n",
    "    \"emb_size\": 96,  # Reduced from 192 to prevent overfitting\n",
    "    \n",
    "    # CEM-specific\n",
    "    \"shared_prob_gen\": True,        # Share probability generator across concepts\n",
    "    \"intervention_prob\": 0.25,      # Training intervention probability\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size_train\": 32,\n",
    "    \"batch_size_eval\": 64,\n",
    "    \"max_epochs\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 4e-4,          # Increased from 4e-5 for stronger regularization\n",
    "    \n",
    "    # Loss\n",
    "    \"concept_loss_weight\": 1.0,\n",
    "    \n",
    "    # LDAM Loss\n",
    "    \"use_ldam_loss\": True,          # Enable for class imbalance\n",
    "    \"n_positive\": None,             # Will be set after loading data\n",
    "    \"n_negative\": None,             # Will be set after loading data\n",
    "    \"ldam_max_margin\": 0.5,         # Try: 0.3, 0.5, 0.7, 1.0\n",
    "    \"ldam_scale\": 20,               # Try: 20, 30, 40, 50\n",
    "    \n",
    "    # Weighted Sampler\n",
    "    \"use_weighted_sampler\": False,\n",
    "\n",
    "    # Threshold selection\n",
    "    \"manual_threshold\": None,  # Auto-select threshold from validation set\n",
    "}\n",
    "\n",
    "print(\"✓ Hyperparameters configured\")\n",
    "if HYPERPARAMS['use_ldam_loss']:\n",
    "    print(f\"  Using LDAM LOSS (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\n",
    "else:\n",
    "    print(f\"  Using standard BCE loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading preprocessed datasets...\")\n",
    "\n",
    "train_data = np.load(os.path.join(DATASET_DIR, \"train_data.npz\"))\n",
    "X_train = train_data['X']\n",
    "C_train = train_data['C']\n",
    "y_train = train_data['y']\n",
    "train_subject_ids = train_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded training data: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "val_data = np.load(os.path.join(DATASET_DIR, \"val_data.npz\"))\n",
    "X_val = val_data['X']\n",
    "C_val = val_data['C']\n",
    "y_val = val_data['y']\n",
    "val_subject_ids = val_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded validation data: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.load(os.path.join(DATASET_DIR, \"test_data.npz\"))\n",
    "X_test = test_data['X']\n",
    "C_test = test_data['C']\n",
    "y_test = test_data['y']\n",
    "test_subject_ids = test_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class weights\n",
    "with open(os.path.join(DATASET_DIR, \"class_weights.json\"), 'r') as f:\n",
    "    class_info = json.load(f)\n",
    "\n",
    "n_positive = class_info['n_positive']\n",
    "n_negative = class_info['n_negative']\n",
    "pos_weight = class_info['pos_weight']\n",
    "\n",
    "# Update HYPERPARAMS with actual class counts for LDAM\n",
    "HYPERPARAMS['n_positive'] = n_positive\n",
    "HYPERPARAMS['n_negative'] = n_negative\n",
    "\n",
    "print(f\"✓ Loaded class weights:\")\n",
    "print(f\"  Negative: {n_negative}, Positive: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: PyTorch Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEMDataset(Dataset):\n",
    "    def __init__(self, X, C, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.C = torch.tensor(C, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.C[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CEMDataset(X_train, C_train, y_train)\n",
    "val_dataset = CEMDataset(X_val, C_val, y_val)\n",
    "test_dataset = CEMDataset(X_test, C_test, y_test)\n",
    "\n",
    "# Create WeightedRandomSampler for batch-level oversampling (if enabled)\n",
    "if HYPERPARAMS['use_weighted_sampler']:\n",
    "    # Compute class sample counts\n",
    "    class_sample_counts = np.bincount(y_train.astype(int))  # [n_negative, n_positive]\n",
    "    weights = 1. / class_sample_counts\n",
    "    sample_weights = weights[y_train.astype(int)]\n",
    "    \n",
    "    # Create sampler\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True  # Allow positive samples to appear multiple times\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ WeightedRandomSampler created:\")\n",
    "    print(f\"  Negative weight: {weights[0]:.4f}\")\n",
    "    print(f\"  Positive weight: {weights[1]:.4f}\")\n",
    "    print(f\"  Expected positive ratio per batch: ~{weights[1]/(weights[0]+weights[1]):.1%}\")\n",
    "    \n",
    "    # Create train loader with sampler (shuffle=False when using sampler)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], sampler=train_sampler)\n",
    "else:\n",
    "    # Standard train loader with shuffle\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], shuffle=True)\n",
    "    print(\"✓ Using standard DataLoader (shuffle=True)\")\n",
    "\n",
    "# Validation and test loaders (no sampling)\n",
    "val_loader = DataLoader(val_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "\n",
    "print(\"✓ All DataLoaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Custom CEM Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAM Loss (for class imbalance)\n",
    "class LDAMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Label-Distribution-Aware Margin (LDAM) Loss for long-tailed recognition.\n",
    "    \n",
    "    Creates class-dependent margins to make decision boundaries harder for minority classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_positive, n_negative, max_margin=0.5, scale=30):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        self.max_margin = max_margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Compute class frequencies\n",
    "        total = n_positive + n_negative\n",
    "        freq_pos = n_positive / total\n",
    "        freq_neg = n_negative / total\n",
    "        \n",
    "        # Compute margins: minority class gets larger margin\n",
    "        margin_pos = max_margin * (freq_pos ** (-0.25))\n",
    "        margin_neg = max_margin * (freq_neg ** (-0.25))\n",
    "        \n",
    "        self.register_buffer('margin_pos', torch.tensor(margin_pos))\n",
    "        self.register_buffer('margin_neg', torch.tensor(margin_neg))\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1).float()\n",
    "        \n",
    "        # Apply class-dependent margins\n",
    "        margin = targets * self.margin_pos + (1 - targets) * (-self.margin_neg)\n",
    "        adjusted_logits = (logits - margin) * self.scale\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(adjusted_logits, targets, reduction='mean')\n",
    "\n",
    "\n",
    "# Custom CEM Implementation\n",
    "class CustomCEM(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Custom Concept Embedding Model (CEM) implementation.\n",
    "    \n",
    "    Architecture:\n",
    "      X → concept_extractor → context_layers → prob_generator → dual_embeddings → task_classifier → y\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_concepts=21,\n",
    "        emb_size=128,\n",
    "        input_dim=384,\n",
    "        shared_prob_gen=True,\n",
    "        intervention_prob=0.25,\n",
    "        concept_loss_weight=1.0,\n",
    "        learning_rate=0.01,\n",
    "        weight_decay=4e-05,\n",
    "        use_ldam_loss=True,\n",
    "        n_positive=83,\n",
    "        n_negative=403,\n",
    "        ldam_max_margin=0.5,\n",
    "        ldam_scale=30,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.n_concepts = n_concepts\n",
    "        self.emb_size = emb_size\n",
    "        self.intervention_prob = intervention_prob\n",
    "        self.concept_loss_weight = concept_loss_weight\n",
    "        \n",
    "        # Stage 1: Concept Extractor (X → Pre-Concept Features)\n",
    "        self.concept_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),      # Reduced from 384 to prevent overfitting\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),                # Increased from 0.4 for stronger regularization\n",
    "            nn.Linear(256, 256)             # Reduced from 384\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Context Generators (Features → Dual Embeddings)\n",
    "        # Each concept gets its own context generator\n",
    "        self.context_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, emb_size * 2),  # Input changed from 384 to 256\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(0.5)                # Added dropout for regularization\n",
    "            ) for _ in range(n_concepts)\n",
    "        ])\n",
    "        \n",
    "        # Stage 3: Probability Generator (Contexts → Concept Probabilities)\n",
    "        if shared_prob_gen:\n",
    "            # Single shared generator for all concepts\n",
    "            self.prob_generator = nn.Linear(emb_size * 2, 1)\n",
    "        else:\n",
    "            # Per-concept probability generators\n",
    "            self.prob_generators = nn.ModuleList([\n",
    "                nn.Linear(emb_size * 2, 1) for _ in range(n_concepts)\n",
    "            ])\n",
    "        \n",
    "        self.shared_prob_gen = shared_prob_gen\n",
    "        \n",
    "        # Stage 4: Task Classifier (Concept Embeddings → Task Output)\n",
    "        self.task_classifier = nn.Sequential(\n",
    "            nn.Linear(n_concepts * emb_size, 96),  # Reduced from 192 to prevent overfitting\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),                       # Increased from 0.3 for stronger regularization\n",
    "            nn.Linear(96, 1)                       # Input changed from 192\n",
    "        )\n",
    "        \n",
    "        # Loss functions\n",
    "        self.concept_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        if use_ldam_loss:\n",
    "            self.task_loss_fn = LDAMLoss(n_positive, n_negative, ldam_max_margin, ldam_scale)\n",
    "        else:\n",
    "            self.task_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, x, c_true=None, train=False):\n",
    "        # Step 1: Extract pre-concept features\n",
    "        pre_features = self.concept_extractor(x)  # (B, 256) - reduced from 384\n",
    "        \n",
    "        # Step 2: Generate contexts and probabilities per concept\n",
    "        contexts = []\n",
    "        c_logits_list = []\n",
    "        \n",
    "        for i, context_layer in enumerate(self.context_layers):\n",
    "            context = context_layer(pre_features)  # (B, emb_size*2)\n",
    "            \n",
    "            # Get probability logit\n",
    "            if self.shared_prob_gen:\n",
    "                logit = self.prob_generator(context)  # (B, 1)\n",
    "            else:\n",
    "                logit = self.prob_generators[i](context)\n",
    "            \n",
    "            contexts.append(context)\n",
    "            c_logits_list.append(logit)\n",
    "        \n",
    "        c_logits = torch.cat(c_logits_list, dim=1)  # (B, 21)\n",
    "        c_probs = torch.sigmoid(c_logits)           # (B, 21)\n",
    "        \n",
    "        # Step 3: Apply intervention (optional during training)\n",
    "        if train and self.intervention_prob > 0 and c_true is not None:\n",
    "            intervention_mask = torch.bernoulli(\n",
    "                torch.ones_like(c_probs) * self.intervention_prob\n",
    "            )\n",
    "            c_probs = c_probs * (1 - intervention_mask) + c_true * intervention_mask\n",
    "        \n",
    "        # Step 4: Mix dual embeddings based on probabilities\n",
    "        concept_embeddings = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            # Split into true/false embeddings\n",
    "            emb_true = context[:, :self.emb_size]       # First half\n",
    "            emb_false = context[:, self.emb_size:]      # Second half\n",
    "            \n",
    "            # Weight by probability\n",
    "            prob = c_probs[:, i:i+1]  # (B, 1)\n",
    "            mixed_emb = emb_true * prob + emb_false * (1 - prob)\n",
    "            concept_embeddings.append(mixed_emb)\n",
    "        \n",
    "        # Concatenate all concept embeddings\n",
    "        c_embeddings = torch.cat(concept_embeddings, dim=1)  # (B, 21*emb_size)\n",
    "        \n",
    "        # Step 5: Task prediction\n",
    "        y_logits = self.task_classifier(c_embeddings)  # (B, 1)\n",
    "        \n",
    "        return c_logits, y_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, c_true = batch\n",
    "        c_logits, y_logits = self.forward(x, c_true=c_true, train=True)\n",
    "        \n",
    "        # Task loss (LDAM)\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss (BCE)\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = task_loss + self.concept_loss_weight * concept_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('train_concept_loss', concept_loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, c_true = batch\n",
    "        c_logits, y_logits = self.forward(x, c_true=c_true, train=False)\n",
    "        \n",
    "        # Task loss\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = task_loss + self.concept_loss_weight * concept_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('val_concept_loss', concept_loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "print(\"✓ Custom CEM model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Custom CEM model\ncustom_cem = CustomCEM(\n    n_concepts=HYPERPARAMS['n_concepts'],\n    emb_size=HYPERPARAMS['emb_size'],\n    input_dim=HYPERPARAMS['embedding_dim'],\n    shared_prob_gen=HYPERPARAMS['shared_prob_gen'],\n    intervention_prob=HYPERPARAMS['intervention_prob'],\n    concept_loss_weight=HYPERPARAMS['concept_loss_weight'],\n    learning_rate=HYPERPARAMS['learning_rate'],\n    weight_decay=HYPERPARAMS['weight_decay'],\n    use_ldam_loss=HYPERPARAMS['use_ldam_loss'],\n    n_positive=HYPERPARAMS['n_positive'],\n    n_negative=HYPERPARAMS['n_negative'],\n    ldam_max_margin=HYPERPARAMS['ldam_max_margin'],\n    ldam_scale=HYPERPARAMS['ldam_scale']\n)\n\nprint(\"✓ Custom CEM model initialized\")\nprint(f\"  Using LDAM Loss (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\nprint(f\"  Concept embedding size: {HYPERPARAMS['emb_size']}\")\nprint(f\"  Intervention probability: {HYPERPARAMS['intervention_prob']}\")\nprint(f\"  Shared probability generator: {HYPERPARAMS['shared_prob_gen']}\")\nprint(f\"  Class counts: {HYPERPARAMS['n_positive']} positive, {HYPERPARAMS['n_negative']} negative\")\nprint(f\"  Data source: Larger MPNet + 2022 data (768-dim MAX)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Add early stopping callback to prevent overfitting\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=15,\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(OUTPUT_DIR, \"models\"),\n",
    "    filename=\"cem-larger-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=HYPERPARAMS['max_epochs'],\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    logger=CSVLogger(save_dir=os.path.join(OUTPUT_DIR, \"logs\"), name=\"cem_larger\"),\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],  # Added early stopping\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured with early stopping (patience=15)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(OUTPUT_DIR, \"models\"),\n",
    "    filename=\"cem-max-gold-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=HYPERPARAMS['max_epochs'],\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    logger=CSVLogger(save_dir=os.path.join(OUTPUT_DIR, \"logs\"), name=\"cem_max_gold\"),\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\\n\")\n",
    "trainer.fit(custom_cem, train_loader, val_loader)\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation threshold optimization\n",
    "\n",
    "print(\"\\nRunning inference on validation set for threshold selection...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "y_val_true = []\n",
    "y_val_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, _ in val_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "\n",
    "        _, y_logits = custom_cem(x_batch)\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "\n",
    "        y_val_true.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_val_prob.extend(\n",
    "            y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs]\n",
    "        )\n",
    "\n",
    "y_val_true = np.array(y_val_true)\n",
    "y_val_prob = np.array(y_val_prob)\n",
    "\n",
    "print(\"✓ Validation inference complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSelecting decision threshold on VALIDATION set...\")\n",
    "\n",
    "if HYPERPARAMS['manual_threshold'] is not None:\n",
    "    # MANUAL threshold override\n",
    "    best_threshold = HYPERPARAMS['manual_threshold']\n",
    "    print(f\"✓ Using MANUAL threshold: {best_threshold:.3f}\")\n",
    "    print(f\"  (Set 'manual_threshold' to None in HYPERPARAMS for automatic selection)\")\n",
    "else:\n",
    "    # AUTOMATIC threshold selection\n",
    "    best_threshold = 0.5\n",
    "    best_metric = -1.0\n",
    "\n",
    "    THRESHOLDS = np.linspace(0.05, 0.95, 91)\n",
    "\n",
    "    for threshold in THRESHOLDS:\n",
    "        y_pred_temp = (y_val_prob >= threshold).astype(int)\n",
    "\n",
    "        if y_pred_temp.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # choose ONE objective, uncomment the one you want\n",
    "\n",
    "        # For recall maximization\n",
    "        metric = recall_score(y_val_true, y_pred_temp)\n",
    "\n",
    "        # For F1\n",
    "        #metric = f1_score(y_val_true, y_pred_temp)\n",
    "\n",
    "        # For MCC\n",
    "        #metric = matthews_corrcoef(y_val_true, y_pred_temp)\n",
    "\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"✓ Auto-selected validation threshold: {best_threshold:.3f}\")\n",
    "    print(f\"  Optimizing for: recall (change metric in code to optimize differently)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept Prediction Evaluation on Validation Set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"      VALIDATION SET: CONCEPT PREDICTION EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run inference on validation set to get concept predictions\n",
    "print(\"\\nRunning inference on validation set for concept evaluation...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "val_concept_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in val_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "\n",
    "        c_logits, y_logits = custom_cem(x_batch)\n",
    "        c_probs = torch.sigmoid(c_logits).cpu().numpy()\n",
    "\n",
    "        val_concept_probs_list.extend(c_probs.tolist())\n",
    "\n",
    "val_concept_probs = np.array(val_concept_probs_list)  # Shape: (98, 21)\n",
    "\n",
    "print(f\"✓ Validation concept predictions collected: {val_concept_probs.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: CONCEPT ACTIVATION PROBABILITY STATISTICS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            CONCEPT ACTIVATION PROBABILITY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    probs = val_concept_probs[:, i]\n",
    "\n",
    "    print(f\"\\nConcept: {concept_name}\")\n",
    "    print(f\"  Mean probability:      {np.mean(probs):.4f}\")\n",
    "    print(f\"  Std deviation:         {np.std(probs):.4f}\")\n",
    "    print(f\"  Min probability:       {np.min(probs):.4f}\")\n",
    "    print(f\"  Max probability:       {np.max(probs):.4f}\")\n",
    "    print(f\"  Median probability:    {np.median(probs):.4f}\")\n",
    "    print(f\"  25th percentile:       {np.percentile(probs, 25):.4f}\")\n",
    "    print(f\"  75th percentile:       {np.percentile(probs, 75):.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: SAMPLE CONCEPT PROBABILITIES (FIRST 5)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            SAMPLE CONCEPT PROBABILITIES (FIRST 5)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_samples_to_show = min(5, val_concept_probs.shape[0])\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    probs = val_concept_probs[:n_samples_to_show, i]\n",
    "    probs_str = \", \".join([f\"{p:.3f}\" for p in probs])\n",
    "    print(f\"{concept_name:<30}: {probs_str}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: OVERALL STATISTICS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            OVERALL CONCEPT PROBABILITY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nGlobal Statistics (all concepts, all samples):\")\n",
    "print(f\"  Shape: {val_concept_probs.shape} (samples × concepts)\")\n",
    "print(f\"  Min probability:     {val_concept_probs.min():.4f}\")\n",
    "print(f\"  Max probability:     {val_concept_probs.max():.4f}\")\n",
    "print(f\"  Mean probability:    {val_concept_probs.mean():.4f}\")\n",
    "print(f\"  Median probability:  {np.median(val_concept_probs):.4f}\")\n",
    "print(f\"  Std deviation:       {val_concept_probs.std():.4f}\")\n",
    "\n",
    "# Per-sample statistics (average predictions per person)\n",
    "per_sample_avg = val_concept_probs.mean(axis=1)  # Average across 21 concepts for each person\n",
    "print(f\"\\nPer-Sample Average Concept Probability:\")\n",
    "print(f\"  Min (across samples):  {per_sample_avg.min():.4f}\")\n",
    "print(f\"  Max (across samples):  {per_sample_avg.max():.4f}\")\n",
    "print(f\"  Mean:                  {per_sample_avg.mean():.4f}\")\n",
    "print(f\"  Median:                {np.median(per_sample_avg):.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: CONCEPT PREDICTION ACCURACY (vs Ground Truth)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            CONCEPT PREDICTION ACCURACY (Threshold = 0.5)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Threshold concept probabilities at 0.5\n",
    "c_pred_val = (val_concept_probs >= 0.5).astype(int)\n",
    "c_true_val = C_val  # Ground truth concepts\n",
    "\n",
    "# Calculate per-concept metrics\n",
    "concept_metrics_val = []\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    acc = accuracy_score(c_true_val[:, i], c_pred_val[:, i])\n",
    "\n",
    "    # Handle cases where precision/recall might be undefined\n",
    "    if c_pred_val[:, i].sum() == 0:  # No positive predictions\n",
    "        prec = 0.0\n",
    "        rec = recall_score(c_true_val[:, i], c_pred_val[:, i], zero_division=0)\n",
    "        f1 = 0.0\n",
    "    elif c_true_val[:, i].sum() == 0:  # No positive ground truth\n",
    "        prec = precision_score(c_true_val[:, i], c_pred_val[:, i], zero_division=0)\n",
    "        rec = 0.0\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        prec = precision_score(c_true_val[:, i], c_pred_val[:, i], zero_division=0)\n",
    "        rec = recall_score(c_true_val[:, i], c_pred_val[:, i], zero_division=0)\n",
    "        f1 = f1_score(c_true_val[:, i], c_pred_val[:, i], zero_division=0)\n",
    "\n",
    "    concept_metrics_val.append({\n",
    "        \"concept\": concept_name,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_acc_val = np.mean([m[\"accuracy\"] for m in concept_metrics_val])\n",
    "avg_prec_val = np.mean([m[\"precision\"] for m in concept_metrics_val])\n",
    "avg_rec_val = np.mean([m[\"recall\"] for m in concept_metrics_val])\n",
    "avg_f1_val = np.mean([m[\"f1\"] for m in concept_metrics_val])\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nAverage Metrics Across All 21 Concepts:\")\n",
    "print(f\"  Average Accuracy:  {avg_acc_val:.4f}\")\n",
    "print(f\"  Average Precision: {avg_prec_val:.4f}\")\n",
    "print(f\"  Average Recall:    {avg_rec_val:.4f}\")\n",
    "print(f\"  Average F1:        {avg_f1_val:.4f}\")\n",
    "\n",
    "# Print full table\n",
    "print(f\"\\nPer-Concept Performance (sorted by F1 score):\")\n",
    "print(f\"{'Concept':<30} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sorted_metrics_val = sorted(concept_metrics_val, key=lambda x: x[\"f1\"], reverse=True)\n",
    "\n",
    "for m in sorted_metrics_val:\n",
    "    print(f\"{m['concept']:<30} {m['accuracy']:<10.4f} {m['precision']:<10.4f} {m['recall']:<10.4f} {m['f1']:<10.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✓ Validation concept evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test set\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "y_true_list = []\n",
    "y_prob_list = []\n",
    "concept_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in test_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "        \n",
    "        c_logits, y_logits = custom_cem(x_batch)\n",
    "        c_probs = torch.sigmoid(c_logits).cpu().numpy()\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "        \n",
    "        y_true_list.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_prob_list.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "        concept_probs_list.extend(c_probs.tolist())\n",
    "\n",
    "y_true = np.array(y_true_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "\n",
    "concept_probs = np.array(concept_probs_list)\n",
    "\n",
    "print(\"✓ Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best threshold for final predictions\n",
    "y_pred = (y_prob >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Results Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_val_prob[y_val_true == 0], bins=20, alpha=0.5, label='Negative')\n",
    "plt.hist(y_val_prob[y_val_true == 1], bins=20, alpha=0.5, label='Positive')\n",
    "plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'Threshold = {best_threshold:.2f}')\n",
    "plt.legend()\n",
    "plt.title(\"Predicted Probabilities Distribution (Validation Set)\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_val_true, y_val_prob)\n",
    "plt.plot(thresholds, precision[:-1], label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], label='Recall')\n",
    "plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'Best Threshold = {best_threshold:.2f}')\n",
    "plt.legend()\n",
    "plt.title(\"Precision and Recall vs. Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "f1_binary = f1_score(y_true, y_pred, pos_label=1)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "precision_binary = precision_score(y_true, y_pred, pos_label=1)\n",
    "recall_binary = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDecision Threshold: {best_threshold:.2f}\")\n",
    "\n",
    "# Enhanced Confusion Matrix Display\n",
    "print(f\"\\n{'CONFUSION MATRIX':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'':>20} │ {'Predicted Negative':^12} │ {'Predicted Positive':^12}\")\n",
    "print(\"─\"*50)\n",
    "print(f\"{'Actual Negative':>20} │ {f'TN = {tn}':^12} │ {f'FP = {fp}':^12}\")\n",
    "print(f\"{'Actual Positive':>20} │ {f'FN = {fn}':^12} │ {f'TP = {tp}':^12}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n  True Positives:  {tp:>3}/{int(np.sum(y_true)):<3} ({100*tp/np.sum(y_true):>5.1f}% of depression cases caught)\")\n",
    "print(f\"  False Negatives: {fn:>3}/{int(np.sum(y_true)):<3} ({100*fn/np.sum(y_true):>5.1f}% of depression cases MISSED)\")\n",
    "print(f\"  True Negatives:  {tn:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*tn/(len(y_true)-np.sum(y_true)):>5.1f}% of healthy correctly identified)\")\n",
    "print(f\"  False Positives: {fp:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*fp/(len(y_true)-np.sum(y_true)):>5.1f}% false alarms)\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:                  {acc:.4f}\")\n",
    "print(f\"  Balanced Accuracy:         {balanced_acc:.4f}\")\n",
    "print(f\"  ROC-AUC:                   {roc_auc:.4f}\")\n",
    "print(f\"  Matthews Correlation:      {mcc:.4f}\")\n",
    "print(f\"\\n  F1 Score (Binary):         {f1_binary:.4f}\")\n",
    "print(f\"  F1 Score (Macro):          {f1_macro:.4f}\")\n",
    "print(f\"  Precision (Binary):        {precision_binary:.4f}\")\n",
    "print(f\"  Recall (Binary):           {recall_binary:.4f}\")\n",
    "\n",
    "print(\"\\n\" + classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results\nmetrics_dict = {\n    \"model_type\": \"cem_larger2022\",\n    \"data_source\": \"larger2022_max_alternative_attention_pipeline\",\n    \"threshold\": float(best_threshold),\n    \"n_samples\": int(len(y_true)),\n    \"n_positive\": int(np.sum(y_true)),\n    \"n_negative\": int(len(y_true) - np.sum(y_true)),\n    \"accuracy\": float(acc),\n    \"balanced_accuracy\": float(balanced_acc),\n    \"roc_auc\": float(roc_auc),\n    \"mcc\": float(mcc),\n    \"f1_binary\": float(f1_binary),\n    \"f1_macro\": float(f1_macro),\n    \"precision_binary\": float(precision_binary),\n    \"recall_binary\": float(recall_binary),\n    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)}\n}\n\nos.makedirs(os.path.join(OUTPUT_DIR, \"results\"), exist_ok=True)\nwith open(os.path.join(OUTPUT_DIR, \"results/test_metrics.json\"), 'w') as f:\n    json.dump(metrics_dict, f, indent=4)\n\n# Save predictions\npredictions_df = pd.DataFrame({\n    'subject_id': test_subject_ids,\n    'y_true': y_true,\n    'y_pred': y_pred,\n    'y_prob': y_prob\n})\n\nfor i, concept_name in enumerate(CONCEPT_NAMES):\n    predictions_df[concept_name] = concept_probs[:, i]\n\npredictions_df.to_csv(os.path.join(OUTPUT_DIR, \"results/test_predictions.csv\"), index=False)\n\nprint(f\"✓ Results saved to {OUTPUT_DIR}/results/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"          CEM LARGER2022 TRAINING COMPLETE\")\nprint(\"=\"*70)\nprint(f\"\\nGenerated files:\")\nprint(f\"  Model checkpoint: {OUTPUT_DIR}/models/\")\nprint(f\"  Metrics JSON:     {OUTPUT_DIR}/results/test_metrics.json\")\nprint(f\"  Predictions CSV:  {OUTPUT_DIR}/results/test_predictions.csv\")\nprint(f\"\\nData source: larger2022_max_alternative_attention_pipeline\")\nprint(f\"  - Uses MAX-based concept similarity (specialist posts)\")\nprint(f\"  - Larger MPNet embeddings (768-dim)\")\nprint(f\"  - Includes 2022 data (~1,400 subjects added to training)\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38concept_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}