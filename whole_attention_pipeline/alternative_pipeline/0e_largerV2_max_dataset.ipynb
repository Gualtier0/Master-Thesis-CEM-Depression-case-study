{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Alternative Attention Dataset Preparation - Larger Model (MPNet)\n",
    "\n",
    "**Purpose:** Prepare dataset using MAX of concept similarities with larger SBERT model.\n",
    "\n",
    "**Key Differences**:\n",
    "- Original (0_prepare): `all-MiniLM-L6-v2` (384-dim) with SUM\n",
    "- 0c_prepare: `all-MiniLM-L6-v2` (384-dim) with MAX\n",
    "- This notebook (03): `all-mpnet-base-v2` (768-dim) with MAX\n",
    "\n",
    "This uses a **larger, more powerful SBERT model** for better semantic representations:\n",
    "- Model: `all-mpnet-base-v2` (768 dimensions)\n",
    "- Better quality embeddings than MiniLM\n",
    "- Higher computational cost but improved performance\n",
    "\n",
    "**Runtime:** ~50-60 minutes (slower due to larger model)\n",
    "\n",
    "This notebook:\n",
    "1. Loads training and test data from XML files\n",
    "2. Uses **MPNet SBERT model** to retrieve top-50 concept-relevant posts (max-based scoring)\n",
    "3. Pools post embeddings using max-based attention weights\n",
    "4. Saves everything to `data/processed/largerV2_max_alternative_attention_pipeline/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "# Detect device (MPS/CUDA/CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU (will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configured\n",
      "  Project root: /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study\n",
      "  Data save dir: /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/largerV2_max_alternative_attention_pipeline\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATA_RAW = os.path.join(PROJECT_ROOT, \"data/raw\")\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "\n",
    "# Training data paths\n",
    "POS_DIR = os.path.join(DATA_RAW, \"train/positive_examples_anonymous_chunks\")\n",
    "NEG_DIR = os.path.join(DATA_RAW, \"train/negative_examples_anonymous_chunks\")\n",
    "\n",
    "# Test data paths\n",
    "TEST_DIR = os.path.join(DATA_RAW, \"test\")\n",
    "TEST_LABELS = os.path.join(TEST_DIR, \"test_golden_truth.txt\")\n",
    "\n",
    "# Concept labels\n",
    "CONCEPTS_FILE = os.path.join(DATA_PROCESSED, \"merged_questionnaires.csv\")\n",
    "\n",
    "# Output directory - CHANGED FOR LARGER V2 MAX ALTERNATIVE PIPELINE\n",
    "SAVE_DIR = os.path.join(DATA_PROCESSED, \"largerV2_max_alternative_attention_pipeline\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data save dir: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined 21 BDI-II concepts\n"
     ]
    }
   ],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters configured:\n",
      "  k_posts: 50\n",
      "  sbert_model: all-mpnet-base-v2\n",
      "  embedding_dim: 768\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    \"k_posts\": 50,              # Top-k posts per subject\n",
    "    \"sbert_model\": \"all-mpnet-base-v2\",\n",
    "    \"embedding_dim\": 768,\n",
    "}\n",
    "# =========================\n",
    "# DEBUG / SANITY CHECK CONFIG\n",
    "# =========================\n",
    "DEBUG = True\n",
    "DEBUG_N_SUBJECTS = 3          # how many subjects to inspect\n",
    "DEBUG_TOP_N_POSTS = 5         # how many top posts to print\n",
    "DEBUG_PRINT_CONCEPTS = True   # print per-concept similarity stats\n",
    "\n",
    "print(\"✓ Hyperparameters configured:\")\n",
    "for k, v in HYPERPARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memory configuration:\n",
      "  post_batch_size: 32\n",
      "  subject_cache_interval: 10\n",
      "  use_no_grad: True\n",
      "  move_to_cpu_immediately: True\n"
     ]
    }
   ],
   "source": [
    "# Memory Management Configuration\n",
    "MEMORY_CONFIG = {\n",
    "    \"post_batch_size\": 32,        # Encode N posts at a time\n",
    "    \"subject_cache_interval\": 10,  # Clear GPU cache every N subjects\n",
    "    \"use_no_grad\": True,           # Disable gradient tracking\n",
    "    \"move_to_cpu_immediately\": True # Move results to CPU after computation\n",
    "}\n",
    "\n",
    "print(\"✓ Memory configuration:\")\n",
    "for k, v in MEMORY_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU cache clearing utility defined\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache and run garbage collection.\"\"\"\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    elif DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✓ GPU cache clearing utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Training Data\n",
    "\n",
    "Extract 486 training subjects with posts and concept labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for XML parsing\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing null chars and extra whitespace.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\u0000\", \"\")\n",
    "    text = WHITESPACE_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_posts_from_xml(xml_path, min_chars=10):\n",
    "    \"\"\"Extract posts from a single XML file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Failed to parse {xml_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    posts = []\n",
    "    for writing in root.findall(\"WRITING\"):\n",
    "        title = writing.findtext(\"TITLE\") or \"\"\n",
    "        text = writing.findtext(\"TEXT\") or \"\"\n",
    "        \n",
    "        combined = normalize_text(f\"{title} {text}\".strip())\n",
    "        if len(combined) >= min_chars:\n",
    "            posts.append(combined)\n",
    "    \n",
    "    return posts\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "  Processing positive examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing positive examples: 100%|██████████| 830/830 [00:00<00:00, 1273.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 29868 posts from positive subjects\n",
      "  Processing negative examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing negative examples: 100%|██████████| 4031/4031 [00:05<00:00, 802.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded training data in 6.0s\n",
      "  Total posts: 286,740\n",
      "  Unique subjects: 486\n",
      "  Label distribution:\n",
      "label\n",
      "0    403\n",
      "1     83\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Parse training XML files\n",
    "print(\"Loading training data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_data = []\n",
    "\n",
    "# Process positive examples\n",
    "print(\"  Processing positive examples...\")\n",
    "pos_files = glob.glob(os.path.join(POS_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in tqdm(pos_files, desc=\"Processing positive examples\"):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 1,  # Positive (depression)\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "print(f\"  Loaded {sum(d['label'] == 1 for d in train_data)} posts from positive subjects\")\n",
    "\n",
    "# Process negative examples\n",
    "print(\"  Processing negative examples...\")\n",
    "neg_files = glob.glob(os.path.join(NEG_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in tqdm(neg_files, desc=\"Processing negative examples\"):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 0,  # Negative (control)\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "train_posts_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"\\n✓ Loaded training data in {time.time()-start_time:.1f}s\")\n",
    "print(f\"  Total posts: {len(train_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {train_posts_df['subject_id'].nunique()}\")\n",
    "print(f\"  Label distribution:\")\n",
    "print(train_posts_df.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading concept labels...\n",
      "✓ Loaded concept labels for 486 subjects\n"
     ]
    }
   ],
   "source": [
    "# Load concept labels from questionnaires\n",
    "print(\"Loading concept labels...\")\n",
    "\n",
    "concepts_df = pd.read_csv(CONCEPTS_FILE)\n",
    "concepts_df[\"subject_id\"] = concepts_df[\"Subject\"].str.replace(\"train_\", \"\", regex=True)\n",
    "\n",
    "# Binarize concept values\n",
    "concept_cols = [col for col in concepts_df.columns if col in CONCEPT_NAMES]\n",
    "for col in concept_cols:\n",
    "    concepts_df[col] = (concepts_df[col] > 0).astype(int)\n",
    "\n",
    "print(f\"✓ Loaded concept labels for {len(concepts_df)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Load Test Data\n",
    "\n",
    "Load all 401 test subjects from test folder (will be used entirely as test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test data...\n",
      "  Temp directory: /tmp/test_chunks_zvwab6nk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted chunk 3/10\n",
      "  Extracted chunk 6/10\n",
      "  Extracted chunk 9/10\n",
      "✓ Test data extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract test ZIP files to temporary directory\n",
    "print(\"Extracting test data...\")\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"test_chunks_\")\n",
    "print(f\"  Temp directory: {temp_dir}\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    zip_path = os.path.join(TEST_DIR, f\"chunk {i}.zip\")\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(temp_dir, f\"chunk_{i}\"))\n",
    "        if i % 3 == 0:\n",
    "            print(f\"  Extracted chunk {i}/10\")\n",
    "\n",
    "print(\"✓ Test data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test labels for 401 subjects\n",
      "  Label distribution:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load test labels\n",
    "test_labels_df = pd.read_csv(TEST_LABELS, sep='\\t', header=None, names=['subject_id', 'label'])\n",
    "test_labels_df['subject_id'] = test_labels_df['subject_id'].str.strip()\n",
    "\n",
    "print(f\"✓ Loaded test labels for {len(test_labels_df)} subjects\")\n",
    "print(f\"  Label distribution:\")\n",
    "print(test_labels_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test posts...\n",
      "  Found 4010 XML files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test posts\n",
      "  Total posts: 229,746\n",
      "  Unique subjects: 401\n"
     ]
    }
   ],
   "source": [
    "# Parse test XML files\n",
    "print(\"Loading test posts...\")\n",
    "test_data = []\n",
    "\n",
    "test_xml_files = glob.glob(os.path.join(temp_dir, \"**\", \"*.xml\"), recursive=True)\n",
    "print(f\"  Found {len(test_xml_files)} XML files\")\n",
    "\n",
    "for xml_file in test_xml_files:\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"(test_subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        label_row = test_labels_df[test_labels_df['subject_id'] == subject_id]\n",
    "        if len(label_row) > 0:\n",
    "            label = label_row.iloc[0]['label']\n",
    "            posts = extract_posts_from_xml(xml_file)\n",
    "            for post in posts:\n",
    "                test_data.append({\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"label\": label,\n",
    "                    \"text\": post\n",
    "                })\n",
    "\n",
    "test_posts_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"✓ Loaded test posts\")\n",
    "print(f\"  Total posts: {len(test_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {test_posts_df['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training data into train (80%) and validation (20%)...\n",
      "✓ Split complete\n",
      "  Training: 388 subjects (80% of original train)\n",
      "  Validation: 98 subjects (20% of original train)\n",
      "  Test: 401 subjects (100% of test folder)\n",
      "\n",
      "  Training label distribution:\n",
      "label\n",
      "0    322\n",
      "1     66\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "  Validation label distribution:\n",
      "label\n",
      "0    81\n",
      "1    17\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "  Test label distribution:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split TRAINING data into train and validation (80/20)\n",
    "print(\"Splitting training data into train (80%) and validation (20%)...\")\n",
    "\n",
    "train_subjects = train_posts_df.groupby('subject_id')['label'].first().reset_index()\n",
    "\n",
    "train_subjects_final, val_subjects = train_test_split(\n",
    "    train_subjects['subject_id'],\n",
    "    test_size=0.2,\n",
    "    stratify=train_subjects['label'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Create new train dataframe with only 80% of subjects\n",
    "train_posts_df_final = train_posts_df[train_posts_df['subject_id'].isin(train_subjects_final)].copy()\n",
    "\n",
    "# Create validation dataframe from remaining 20% of training subjects\n",
    "val_posts_df = train_posts_df[train_posts_df['subject_id'].isin(val_subjects)].copy()\n",
    "\n",
    "# Keep ALL test data as test set (no split)\n",
    "test_posts_df_final = test_posts_df.copy()\n",
    "\n",
    "print(f\"✓ Split complete\")\n",
    "print(f\"  Training: {train_posts_df_final['subject_id'].nunique()} subjects (80% of original train)\")\n",
    "print(f\"  Validation: {val_posts_df['subject_id'].nunique()} subjects (20% of original train)\")\n",
    "print(f\"  Test: {test_posts_df_final['subject_id'].nunique()} subjects (100% of test folder)\")\n",
    "\n",
    "# Show label distributions\n",
    "print(f\"\\n  Training label distribution:\")\n",
    "print(train_posts_df_final.groupby('label')['subject_id'].nunique())\n",
    "print(f\"\\n  Validation label distribution:\")\n",
    "print(val_posts_df.groupby('label')['subject_id'].nunique())\n",
    "print(f\"\\n  Test label distribution:\")\n",
    "print(test_posts_df_final.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: SBERT Setup & Concept Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SBERT model loaded on cuda\n",
      "  Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Load SBERT model\n",
    "print(f\"Loading SBERT model: {HYPERPARAMS['sbert_model']}\")\n",
    "sbert_model = SentenceTransformer(HYPERPARAMS['sbert_model'])\n",
    "sbert_model = sbert_model.to(DEVICE)\n",
    "\n",
    "print(f\"✓ SBERT model loaded on {DEVICE}\")\n",
    "print(f\"  Embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for 21 concepts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Concept embeddings created\n",
      "  Shape: torch.Size([21, 768])\n"
     ]
    }
   ],
   "source": [
    "# Create concept embeddings\n",
    "print(f\"Creating embeddings for {N_CONCEPTS} concepts...\")\n",
    "concept_embeddings = sbert_model.encode(\n",
    "    CONCEPT_NAMES,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "print(f\"✓ Concept embeddings created\")\n",
    "print(f\"  Shape: {concept_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batched post retrieval function defined (MAX-based)\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k_posts_max(subject_id, posts_df, concept_embs, sbert, k=50, batch_size=32, debug=False):\n",
    "    \"\"\"\n",
    "    Retrieve top-k posts for a subject based on MAX of concept similarities.\n",
    "    OPTIMIZED: Uses batching to prevent memory exhaustion.\n",
    "    \n",
    "    For each post, takes MAX similarity across all 21 concepts.\n",
    "    Selects posts that are highly relevant to at least ONE concept.\n",
    "    \"\"\"\n",
    "    subj_posts = posts_df[posts_df['subject_id'] == subject_id]['text'].tolist()\n",
    "\n",
    "    if len(subj_posts) == 0:\n",
    "        return []\n",
    "\n",
    "    if len(subj_posts) <= k:\n",
    "        if len(subj_posts) < k:\n",
    "            extra_needed = k - len(subj_posts)\n",
    "            padding = list(np.random.choice(subj_posts, size=extra_needed, replace=True))\n",
    "            return subj_posts + padding\n",
    "        else:\n",
    "            return subj_posts\n",
    "\n",
    "    # Batch encoding to prevent memory issues\n",
    "    max_sim_scores = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for i in range(0, len(subj_posts), batch_size):\n",
    "            batch_posts = subj_posts[i:i + batch_size]\n",
    "\n",
    "            # Encode batch\n",
    "            batch_embeddings = sbert.encode(\n",
    "                batch_posts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "\n",
    "            # Compute similarities for this batch\n",
    "            cos_scores = util.cos_sim(batch_embeddings, concept_embs)  # [batch, 21]\n",
    "            # KEY: Take MAX instead of SUM\n",
    "            batch_max_scores = cos_scores.max(dim=1)[0].cpu().numpy()  # [0] gets values, not indices\n",
    "\n",
    "            max_sim_scores.extend(batch_max_scores)\n",
    "\n",
    "            # Clear references\n",
    "            del batch_embeddings, cos_scores, batch_max_scores\n",
    "\n",
    "    max_sim_scores = np.array(max_sim_scores)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"[DEBUG] Subject: {subject_id}\")\n",
    "        print(f\"[DEBUG] Total posts: {len(subj_posts)}\")\n",
    "        print(\"[DEBUG] Max similarity stats:\")\n",
    "        print(f\"  min={max_sim_scores.min():.4f} \"\n",
    "              f\"max={max_sim_scores.max():.4f} \"\n",
    "              f\"mean={max_sim_scores.mean():.4f} \"\n",
    "              f\"std={max_sim_scores.std():.4f}\")\n",
    "\n",
    "        top_idx_sorted = np.argsort(-max_sim_scores)\n",
    "        print(f\"\\n[DEBUG] Top-{DEBUG_TOP_N_POSTS} retrieved posts:\")\n",
    "        for rank, i in enumerate(top_idx_sorted[:DEBUG_TOP_N_POSTS]):\n",
    "            print(f\"\\n  Rank {rank+1}\")\n",
    "            print(f\"  Score: {max_sim_scores[i]:.4f}\")\n",
    "            print(f\"  Text: {subj_posts[i][:300]}\")\n",
    "\n",
    "    # Select top-k posts\n",
    "    top_k_indices = np.argpartition(-max_sim_scores, range(min(k, len(subj_posts))))[:k]\n",
    "\n",
    "    return [subj_posts[i] for i in top_k_indices]\n",
    "\n",
    "print(\"✓ Batched post retrieval function defined (MAX-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top-50 posts (MAX-based scoring with batching)...\n",
      "⏰ This will be faster and more memory-efficient\n",
      "  Processing training subjects (80% of train data)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects:   0%|          | 0/388 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects:   1%|          | 2/388 [00:00<02:39,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject4675\n",
      "[DEBUG] Total posts: 63\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0689 max=0.4852 mean=0.2573 std=0.1003\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4852\n",
      "  Text: I have an anxiety disorder, so my being upset could be for many reasons, or no reason at all.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.4513\n",
      "  Text: Ex-boyfriend and current best friend. He has depression just as bad as mine and tried to kill himself. When he came back, he put us on a \"break\". Everything got so overwhelming without him and I tried to take my own life (not just because of him, I have a lot going on without him even around) then, \n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4504\n",
      "  Text: My experience has been me just being overly sexual. Every little thing was sexual in my mind, and I could literally have sex with just about anybody as like as they were at least somewhat aesthetically pleasing. I only slept with 3 people though (a lot for a teenage girlI still have this problem but\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4325\n",
      "  Text: May I just say, you *can not* hurt people the way you are. I don't condone it, and never will condone mistreatment of others. I think you need to get a hold on this problem of your's, which may be hard. Play violent video games, date someone who is a masochist, I don't know. Anything other than what\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.4153\n",
      "  Text: It makes you feel numbish... We just wanted to feel numb. It sounds stupid but, if you knew our emotional states, it would make sense to you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects:   1%|          | 3/388 [00:01<03:49,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject888\n",
      "[DEBUG] Total posts: 108\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0107 max=0.5184 mean=0.1552 std=0.0906\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.5184\n",
      "  Text: Does anyone else just genuinely hate him/her self?\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5000\n",
      "  Text: A bag of weed. P.S. Why are you crying?\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4478\n",
      "  Text: No, in fact the libido side of things has generally remained untouched. Then again, I've always had a pretty good libido (I was assigned Male at birth, in case you're wondering).\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.3302\n",
      "  Text: i’m gonna hurl\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.3287\n",
      "  Text: Slow news day, I guess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train subjects: 100%|██████████| 388/388 [21:07<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Processing validation subjects (20% of train data)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val subjects:   1%|          | 1/98 [00:00<00:33,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject7133\n",
      "[DEBUG] Total posts: 132\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0039 max=0.4581 mean=0.1747 std=0.0938\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4581\n",
      "  Text: I'm usually exhausted/more emotional. But I've noticed moodiness too.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.4177\n",
      "  Text: I do. Practically every night I sleep in this position, I find it strangely comfortable.\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.3953\n",
      "  Text: Mostly just unsettling...\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.3933\n",
      "  Text: • To battle the emptiness. • To punish myself • Sometimes no real reason, just a need to do it.\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.3788\n",
      "  Text: \"you're not a bad person for the ways you try to kill your sadness\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val subjects:   2%|▏         | 2/98 [00:03<03:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject933\n",
      "[DEBUG] Total posts: 529\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=-0.0012 max=0.5853 mean=0.2249 std=0.0906\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.5853\n",
      "  Text: That I was suicidal\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5445\n",
      "  Text: Managing Emotions\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.5445\n",
      "  Text: Managing Emotions\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.5126\n",
      "  Text: I don't even want to say how bad my intrusive thoughts are about sex\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.4904\n",
      "  Text: Being judged for crying in public\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val subjects:   3%|▎         | 3/98 [00:12<08:35,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: subject5080\n",
      "[DEBUG] Total posts: 1167\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=-0.0046 max=0.5731 mean=0.1787 std=0.0936\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.5731\n",
      "  Text: Low libido here and having a boy.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5471\n",
      "  Text: No matter what I eat I am always ravenous I have done LCHF/keto/Atkins and the 70% fat intake didn't suppress my appetite. I've tried fiber, water, gum, protein, everything suggested on here that didn't require a trip to my doctor. I'm pretty sure I have some form of disordered eating. Literally the\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.5293\n",
      "  Text: [Rant] Appetite is still ravenous I search \"appetite\" in /r/keto and all I get is everyone barely able to meet their calorie goals because their appetite is gone. I have PCOS, which my doctor tells me really sucks for appetite as it's usually more active in ladies with PCOS than those without. My do\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.5250\n",
      "  Text: There are plenty of times I don't eat when I'm hungry, but the constant fixation on food is what is really getting to me.\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.5144\n",
      "  Text: I had this exact routine about 6 years ago while I was on vacation from school. The only thing that helped me was fasting as notoriousrdc said and then getting into a routine and sticking to it. Also, **your bed is only for sleep and sex**. Otherwise it will take you forever to actually fall asleep \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val subjects: 100%|██████████| 98/98 [05:54<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Processing test subjects (100% of test folder)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test subjects:   0%|          | 1/401 [00:06<44:28,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: test_subject501\n",
      "[DEBUG] Total posts: 1316\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0063 max=0.4332 mean=0.1606 std=0.0664\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4332\n",
      "  Text: Well do you have a better choice?\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.4270\n",
      "  Text: Well I actually just finished watching Into the Wild. I cried my fucking heart out okay? I'm not going to explain why, just watch the film and you'll find out. That's twice I've cried today :'(\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4093\n",
      "  Text: When they are pretentious.\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.3975\n",
      "  Text: ----the amount of people that are excited for The Hateful Eight.\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.3940\n",
      "  Text: Roid Rage R2-D2 (X-post from /r/funny)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test subjects:   0%|          | 2/401 [00:11<36:29,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: test_subject9306\n",
      "[DEBUG] Total posts: 667\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=-0.0013 max=0.5476 mean=0.1954 std=0.0836\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.5476\n",
      "  Text: Confident people can have self-doubt. Self-introspection is often the difference between confidence and arrogance.\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.5468\n",
      "  Text: I can't do this - Confidence in your own abilities and *knowing your limits and inabilities* - the difference between ego and self confidence.\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.5156\n",
      "  Text: how can I love someone who doesn't love themselves Ok, what is seriously wrong with loving someone who has low self esteem? I see this constantly.\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4950\n",
      "  Text: everyone hates having me around them As someone who used to think the same thing, I can assure you that they don't. I have atrocious self-esteem (it's getting better, and I'll tell you why), and frequently would go out with my friends and return home feeling like shit because of all the stupid thing\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.4866\n",
      "  Text: In all seriousness, who has such self confidence that they don't compare themselves to anyone else, or are ever jealous of somebody's looks or abilities? It's not possible, and a little bit of self introspection and humility can be healthy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test subjects:   1%|          | 3/401 [00:20<47:40,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG] Subject: test_subject7756\n",
      "[DEBUG] Total posts: 1851\n",
      "[DEBUG] Max similarity stats:\n",
      "  min=0.0051 max=0.4755 mean=0.1553 std=0.0696\n",
      "\n",
      "[DEBUG] Top-5 retrieved posts:\n",
      "\n",
      "  Rank 1\n",
      "  Score: 0.4755\n",
      "  Text: Napping on the job could boost productivity: study\n",
      "\n",
      "  Rank 2\n",
      "  Score: 0.4701\n",
      "  Text: The Week Cynicism Rested\n",
      "\n",
      "  Rank 3\n",
      "  Score: 0.4451\n",
      "  Text: A complex relationship between genes, hormones and social factors can lead to eating disorders in women. Findings show that that during the menstrual cycle, ovarian hormones act like a master conductor they turn genetic risk on and off in the body.\n",
      "\n",
      "  Rank 4\n",
      "  Score: 0.4451\n",
      "  Text: A complex relationship between genes, hormones and social factors can lead to eating disorders in women. Findings show that that during the menstrual cycle, ovarian hormones act like a master conductor they turn genetic risk on and off in the body.\n",
      "\n",
      "  Rank 5\n",
      "  Score: 0.4144\n",
      "  Text: aaaaaaaaaaaaaaaaaand now i'm crying all over again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test subjects: 100%|██████████| 401/401 [23:20<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Post retrieval complete in 3022.8s (50.4 min)\n",
      "  Memory-optimized processing: 887 subjects\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top-k posts for all subjects\n",
    "print(f\"Retrieving top-{HYPERPARAMS['k_posts']} posts (MAX-based scoring with batching)...\")\n",
    "print(\"⏰ This will be faster and more memory-efficient\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Training subjects (80% of original training data)\n",
    "print(\"  Processing training subjects (80% of train data)...\")\n",
    "train_selected = {}\n",
    "train_subjects = train_posts_df_final['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(train_subjects, desc=\"Train subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        train_posts_df_final,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    train_selected[subject_id] = selected\n",
    "\n",
    "    # Clear GPU cache periodically\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Validation subjects (20% of original training data)\n",
    "print(\"\\n  Processing validation subjects (20% of train data)...\")\n",
    "val_selected = {}\n",
    "val_subjects = val_posts_df['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(val_subjects, desc=\"Val subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        val_posts_df,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    val_selected[subject_id] = selected\n",
    "\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Test subjects (100% of test folder)\n",
    "print(\"\\n  Processing test subjects (100% of test folder)...\")\n",
    "test_selected = {}\n",
    "test_subjects = test_posts_df_final['subject_id'].unique()\n",
    "\n",
    "for idx, subject_id in enumerate(tqdm(test_subjects, desc=\"Test subjects\")):\n",
    "    selected = retrieve_top_k_posts_max(\n",
    "        subject_id,\n",
    "        test_posts_df_final,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts'],\n",
    "        batch_size=MEMORY_CONFIG['post_batch_size'],\n",
    "        debug=(DEBUG and idx < DEBUG_N_SUBJECTS)\n",
    "    )\n",
    "    test_selected[subject_id] = selected\n",
    "\n",
    "    if (idx + 1) % MEMORY_CONFIG['subject_cache_interval'] == 0:\n",
    "        clear_gpu_cache()\n",
    "\n",
    "# Final cache clear\n",
    "clear_gpu_cache()\n",
    "\n",
    "print(f\"\\n✓ Post retrieval complete in {time.time()-start_time:.1f}s ({(time.time()-start_time)/60:.1f} min)\")\n",
    "print(f\"  Memory-optimized processing: {len(train_subjects) + len(val_subjects) + len(test_subjects)} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memory-optimized attention pooling function defined (MAX-based)\n"
     ]
    }
   ],
   "source": [
    "def encode_and_attention_pool_max(selected_posts_dict, sbert, concept_embs,\n",
    "                                   normalize=True, debug=False):\n",
    "    \"\"\"\n",
    "    Encode posts and pool using MAX of concept similarities for attention.\n",
    "    OPTIMIZED: Includes memory management for stability.\n",
    "    \n",
    "    For each post:\n",
    "    1. Compute similarities to all 21 concepts\n",
    "    2. Take MAX similarity as the post's relevance score\n",
    "    3. Use softmax(max_scores / temperature) for attention weights\n",
    "    4. Weighted sum pooling to create final embedding\n",
    "    \"\"\"\n",
    "    subject_ids = list(selected_posts_dict.keys())\n",
    "    pooled_embeddings = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for idx, subject_id in enumerate(subject_ids):\n",
    "            posts = selected_posts_dict[subject_id]\n",
    "\n",
    "            # Handle empty posts\n",
    "            if len(posts) == 0:\n",
    "                print(f\"WARNING: No posts for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(768))\n",
    "                continue\n",
    "\n",
    "            # Filter out empty posts\n",
    "            posts = [p for p in posts if p.strip()]\n",
    "            if len(posts) == 0:\n",
    "                print(f\"WARNING: All posts empty for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(768))\n",
    "                continue\n",
    "\n",
    "            # Encode posts\n",
    "            post_embs = sbert.encode(\n",
    "                posts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "\n",
    "            if post_embs.shape[0] == 0 or post_embs.shape[1] == 0:\n",
    "                print(f\"WARNING: Empty embeddings for subject {subject_id}, using zero embedding\")\n",
    "                pooled_embeddings.append(np.zeros(768))\n",
    "                continue\n",
    "\n",
    "            # Compute similarity to concepts\n",
    "            cos_scores = util.cos_sim(post_embs, concept_embs)\n",
    "\n",
    "            # KEY: Take MAX instead of SUM\n",
    "            post_scores = cos_scores.max(dim=1)[0]  # [0] gets values, not indices\n",
    "\n",
    "            # Remove negative similarities\n",
    "            post_scores = torch.clamp(post_scores, min=0.0)\n",
    "\n",
    "            # Attention weights\n",
    "            TEMPERATURE = 0.2  \n",
    "            attn_weights = torch.softmax(post_scores / TEMPERATURE, dim=0)\n",
    "\n",
    "            if debug and idx < DEBUG_N_SUBJECTS:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(f\"[DEBUG][ATTENTION] Subject: {subject_id}\")\n",
    "                attn_np = attn_weights.cpu().numpy()\n",
    "                print(\"[DEBUG][ATTENTION] Weight stats:\")\n",
    "                print(f\"  min={attn_np.min():.6f} \"\n",
    "                      f\"max={attn_np.max():.6f} \"\n",
    "                      f\"mean={attn_np.mean():.6f} \"\n",
    "                      f\"entropy={-np.sum(attn_np * np.log(attn_np + 1e-12)):.4f}\")\n",
    "\n",
    "                top_attn_idx = np.argsort(-attn_np)[:DEBUG_TOP_N_POSTS]\n",
    "                print(f\"\\n[DEBUG][ATTENTION] Top-{DEBUG_TOP_N_POSTS} attended posts:\")\n",
    "                for rank, i in enumerate(top_attn_idx):\n",
    "                    print(f\"\\n  Rank {rank+1}\")\n",
    "                    print(f\"  Attention: {attn_np[i]:.6f}\")\n",
    "                    print(f\"  Text: {posts[i][:300]}\")\n",
    "\n",
    "            # Weighted sum pooling\n",
    "            pooled = torch.sum(attn_weights.unsqueeze(1) * post_embs, dim=0)\n",
    "            pooled_embeddings.append(pooled.cpu().numpy())\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del post_embs, cos_scores, attn_weights, pooled\n",
    "\n",
    "    return np.vstack(pooled_embeddings), subject_ids\n",
    "\n",
    "print(\"✓ Memory-optimized attention pooling function defined (MAX-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding and pooling embeddings (MAX-based attention, memory-optimized)...\n",
      "  Training set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject1839\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.011876 max=0.042429 mean=0.020000 entropy=3.8472\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.042429\n",
      "  Text: I did. I was always an incredibly morose, aloof child and was diagnosed with depression and on meds since I was 10. I'm 25 now and it really makes me sad to think that I've been this way most of my life... Even before actual diagnosis. I feel you on that. It makes it hard sometimes to imagine a trul\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.034359\n",
      "  Text: Ooh I wish it was that. Backpacking around Europe would probably be way more fun than being depressed haha\n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.031973\n",
      "  Text: It really does, thanks. It's just nice to know that I'm not alone and that other people understand what I'm going through. I've been trying to start up my creative side again and do some writing and drawing projects, but I am terrible with self-motivation and I tend to get incredibly frustrated when\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.031973\n",
      "  Text: It really does, thanks. It's just nice to know that I'm not alone and that other people understand what I'm going through. I've been trying to start up my creative side again and do some writing and drawing projects, but I am terrible with self-motivation and I tend to get incredibly frustrated when\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.031973\n",
      "  Text: It really does, thanks. It's just nice to know that I'm not alone and that other people understand what I'm going through. I've been trying to start up my creative side again and do some writing and drawing projects, but I am terrible with self-motivation and I tend to get incredibly frustrated when\n",
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject4675\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.009890 max=0.048220 mean=0.020000 entropy=3.8187\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.048220\n",
      "  Text: I have an anxiety disorder, so my being upset could be for many reasons, or no reason at all.\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.040697\n",
      "  Text: Ex-boyfriend and current best friend. He has depression just as bad as mine and tried to kill himself. When he came back, he put us on a \"break\". Everything got so overwhelming without him and I tried to take my own life (not just because of him, I have a lot going on without him even around) then, \n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.040526\n",
      "  Text: My experience has been me just being overly sexual. Every little thing was sexual in my mind, and I could literally have sex with just about anybody as like as they were at least somewhat aesthetically pleasing. I only slept with 3 people though (a lot for a teenage girlI still have this problem but\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.037047\n",
      "  Text: May I just say, you *can not* hurt people the way you are. I don't condone it, and never will condone mistreatment of others. I think you need to get a hold on this problem of your's, which may be hard. Play violent video games, date someone who is a masochist, I don't know. Anything other than what\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.033998\n",
      "  Text: It makes you feel numbish... We just wanted to feel numb. It sounds stupid but, if you knew our emotional states, it would make sense to you.\n",
      "\n",
      "============================================================\n",
      "[DEBUG][ATTENTION] Subject: subject888\n",
      "[DEBUG][ATTENTION] Weight stats:\n",
      "  min=0.012189 max=0.076985 mean=0.020000 entropy=3.7708\n",
      "\n",
      "[DEBUG][ATTENTION] Top-5 attended posts:\n",
      "\n",
      "  Rank 1\n",
      "  Attention: 0.076985\n",
      "  Text: Does anyone else just genuinely hate him/her self?\n",
      "\n",
      "  Rank 2\n",
      "  Attention: 0.070198\n",
      "  Text: A bag of weed. P.S. Why are you crying?\n",
      "\n",
      "  Rank 3\n",
      "  Attention: 0.054096\n",
      "  Text: No, in fact the libido side of things has generally remained untouched. Then again, I've always had a pretty good libido (I was assigned Male at birth, in case you're wondering).\n",
      "\n",
      "  Rank 4\n",
      "  Attention: 0.030033\n",
      "  Text: i’m gonna hurl\n",
      "\n",
      "  Rank 5\n",
      "  Attention: 0.029815\n",
      "  Text: Slow news day, I guess\n",
      "    X_train shape: (388, 768)\n",
      "  Validation set...\n",
      "    X_val shape: (98, 768)\n",
      "  Test set...\n",
      "    X_test shape: (401, 768)\n",
      "\n",
      "✓ Encoding complete in 220.5s (3.7 min)\n"
     ]
    }
   ],
   "source": [
    "# Encode and pool for all splits\n",
    "print(\"Encoding and pooling embeddings (MAX-based attention, memory-optimized)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"  Training set...\")\n",
    "X_train, train_subject_ids = encode_and_attention_pool_max(\n",
    "    train_selected,\n",
    "    sbert_model,\n",
    "    concept_embeddings,\n",
    "    normalize=True,\n",
    "    debug=DEBUG\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_train shape: {X_train.shape}\")\n",
    "\n",
    "print(\"  Validation set...\")\n",
    "X_val, val_subject_ids = encode_and_attention_pool_max(\n",
    "    val_selected, sbert_model, concept_embeddings, normalize=True\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_val shape: {X_val.shape}\")\n",
    "\n",
    "print(\"  Test set...\")\n",
    "X_test, test_subject_ids = encode_and_attention_pool_max(\n",
    "    test_selected, sbert_model, concept_embeddings, normalize=True\n",
    ")\n",
    "clear_gpu_cache()\n",
    "print(f\"    X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\n✓ Encoding complete in {time.time()-start_time:.1f}s ({(time.time()-start_time)/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Build Concept Matrices and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building concept matrices and labels...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Matrices built\n",
      "  Train: X=(388, 768), C=(388, 21), y=(388,)\n",
      "  Val:   X=(98, 768), C=(98, 21), y=(98,)\n",
      "  Test:  X=(401, 768), C=(401, 21), y=(401,)\n",
      "\n",
      "  Training label distribution: [322  66]\n",
      "  Validation label distribution: [81 17]\n",
      "  Test label distribution: [349  52]\n"
     ]
    }
   ],
   "source": [
    "# Build concept matrices and label vectors\n",
    "print(\"Building concept matrices and labels...\")\n",
    "\n",
    "# Training: get concepts from questionnaires (80% of training data)\n",
    "C_train = []\n",
    "y_train = []\n",
    "for subject_id in train_subject_ids:\n",
    "    label = train_posts_df_final[train_posts_df_final['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_train.append(label)\n",
    "    \n",
    "    concept_row = concepts_df[concepts_df['subject_id'] == subject_id]\n",
    "    if len(concept_row) > 0:\n",
    "        concepts = concept_row[concept_cols].values[0]\n",
    "    else:\n",
    "        concepts = np.zeros(N_CONCEPTS)\n",
    "    C_train.append(concepts)\n",
    "\n",
    "C_train = np.array(C_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "# Validation: get concepts from questionnaires (20% of training data)\n",
    "C_val = []\n",
    "y_val = []\n",
    "for subject_id in val_subject_ids:\n",
    "    label = val_posts_df[val_posts_df['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_val.append(label)\n",
    "    \n",
    "    concept_row = concepts_df[concepts_df['subject_id'] == subject_id]\n",
    "    if len(concept_row) > 0:\n",
    "        concepts = concept_row[concept_cols].values[0]\n",
    "    else:\n",
    "        concepts = np.zeros(N_CONCEPTS)\n",
    "    C_val.append(concepts)\n",
    "\n",
    "C_val = np.array(C_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "\n",
    "# Test: zeros for concepts (no ground truth available)\n",
    "C_test = np.zeros((len(test_subject_ids), N_CONCEPTS), dtype=np.float32)\n",
    "y_test = []\n",
    "for subject_id in test_subject_ids:\n",
    "    label = test_posts_df_final[test_posts_df_final['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_test.append(label)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "print(\"✓ Matrices built\")\n",
    "print(f\"  Train: X={X_train.shape}, C={C_train.shape}, y={y_train.shape}\")\n",
    "print(f\"  Val:   X={X_val.shape}, C={C_val.shape}, y={y_val.shape}\")\n",
    "print(f\"  Test:  X={X_test.shape}, C={C_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\n  Training label distribution: {np.bincount(y_train.astype(int))}\")\n",
    "print(f\"  Validation label distribution: {np.bincount(y_val.astype(int))}\")\n",
    "print(f\"  Test label distribution: {np.bincount(y_test.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance:\n",
      "  Negative samples: 322\n",
      "  Positive samples: 66\n",
      "  Ratio: 1:4.88\n",
      "  Computed pos_weight: 4.8788\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "n_negative = int(np.sum(y_train == 0))\n",
    "n_positive = int(np.sum(y_train == 1))\n",
    "pos_weight = n_negative / n_positive\n",
    "\n",
    "print(f\"Class imbalance:\")\n",
    "print(f\"  Negative samples: {n_negative}\")\n",
    "print(f\"  Positive samples: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")\n",
    "print(f\"  Computed pos_weight: {pos_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Save All Datasets\n",
    "\n",
    "Save everything for fast loading by training pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets...\n",
      "✓ Datasets saved to /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/largerV2_max_alternative_attention_pipeline\n",
      "  train_data.npz: 388 samples\n",
      "  val_data.npz:   98 samples\n",
      "  test_data.npz:  401 samples\n",
      "  class_weights.json\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets to disk\n",
    "print(\"Saving datasets...\")\n",
    "\n",
    "# Save numpy arrays\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"train_data.npz\"),\n",
    "    X=X_train,\n",
    "    C=C_train,\n",
    "    y=y_train,\n",
    "    subject_ids=np.array(train_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"val_data.npz\"),\n",
    "    X=X_val,\n",
    "    C=C_val,\n",
    "    y=y_val,\n",
    "    subject_ids=np.array(val_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"test_data.npz\"),\n",
    "    X=X_test,\n",
    "    C=C_test,\n",
    "    y=y_test,\n",
    "    subject_ids=np.array(test_subject_ids)\n",
    ")\n",
    "\n",
    "# Save class weights info\n",
    "class_info = {\n",
    "    \"n_positive\": n_positive,\n",
    "    \"n_negative\": n_negative,\n",
    "    \"pos_weight\": float(pos_weight)\n",
    "}\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"class_weights.json\"), 'w') as f:\n",
    "    json.dump(class_info, f, indent=4)\n",
    "\n",
    "print(f\"✓ Datasets saved to {SAVE_DIR}\")\n",
    "print(f\"  train_data.npz: {X_train.shape[0]} samples\")\n",
    "print(f\"  val_data.npz:   {X_val.shape[0]} samples\")\n",
    "print(f\"  test_data.npz:  {X_test.shape[0]} samples\")\n",
    "print(f\"  class_weights.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned up temporary directory: /tmp/test_chunks_zvwab6nk\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary directory\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"✓ Cleaned up temporary directory: {temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Failed to clean up temporary directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "      LARGER V2 MAX ALTERNATIVE DATASET PREPARATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Saved files:\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/largerV2_max_alternative_attention_pipeline/train_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/largerV2_max_alternative_attention_pipeline/val_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/largerV2_max_alternative_attention_pipeline/test_data.npz\n",
      "  /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/largerV2_max_alternative_attention_pipeline/class_weights.json\n",
      "\n",
      "Data split strategy:\n",
      "  - Training: 80% of train folder (~389 subjects)\n",
      "  - Validation: 20% of train folder (~97 subjects)\n",
      "  - Test: 100% of test folder (401 subjects)\n",
      "\n",
      "Key difference from original:\n",
      "  - Uses MAX of concept similarities with MPNet model (768-dim)\n",
      "  - Larger, more powerful SBERT model: all-mpnet-base-v2\n",
      "  - Higher quality embeddings (768-dim vs 384-dim)\n",
      "\n",
      "Use this data with CEM/CBM training notebooks!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"      LARGER V2 MAX ALTERNATIVE DATASET PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSaved files:\")\n",
    "print(f\"  {SAVE_DIR}/train_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/val_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/test_data.npz\")\n",
    "print(f\"  {SAVE_DIR}/class_weights.json\")\n",
    "print(\"\\nData split strategy:\")\n",
    "print(\"  - Training: 80% of train folder (~389 subjects)\")\n",
    "print(\"  - Validation: 20% of train folder (~97 subjects)\")\n",
    "print(\"  - Test: 100% of test folder (401 subjects)\")\n",
    "print(\"\\nKey difference from original:\")\n",
    "print(\"  - Uses MAX of concept similarities with MPNet model (768-dim)\")\n",
    "print(\"  - Larger, more powerful SBERT model: all-mpnet-base-v2\")\n",
    "print(\"  - Higher quality embeddings (768-dim vs 384-dim)\")\n",
    "print(\"\\nUse this data with CEM/CBM training notebooks!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
