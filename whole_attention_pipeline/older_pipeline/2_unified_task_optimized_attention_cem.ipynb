{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-Optimized Attention CEM - Unified Pipeline\n",
    "\n",
    "**Purpose:** End-to-end pipeline with learnable attention optimized for task loss.\n",
    "\n",
    "**Key Innovation:** Attention weights learned via backpropagation from depression classification loss instead of static concept-based similarity.\n",
    "\n",
    "**Runtime:** ~1 hour (data prep: 40min, training: 15-20min)\n",
    "\n",
    "This notebook:\n",
    "1. Loads and retrieves top-k concept-relevant posts\n",
    "2. Encodes posts WITHOUT pooling (stores individual embeddings)\n",
    "3. Trains CEM with learnable attention module\n",
    "4. Analyzes attention patterns\n",
    "5. Evaluates on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using MacBook GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Detect device (MPS/CUDA/CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU (will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configured\n",
      "  Project root: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study\n",
      "  Data save dir: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/attention_task_optimized\n",
      "  Output dir: outputs_task_optimized_attention\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_RAW = os.path.join(PROJECT_ROOT, \"data/raw\")\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "\n",
    "# Training data paths\n",
    "POS_DIR = os.path.join(DATA_RAW, \"train/positive_examples_anonymous_chunks\")\n",
    "NEG_DIR = os.path.join(DATA_RAW, \"train/negative_examples_anonymous_chunks\")\n",
    "\n",
    "# Test data paths\n",
    "TEST_DIR = os.path.join(DATA_RAW, \"test\")\n",
    "TEST_LABELS = os.path.join(TEST_DIR, \"test_golden_truth.txt\")\n",
    "\n",
    "# Concept labels\n",
    "CONCEPTS_FILE = os.path.join(DATA_PROCESSED, \"merged_questionnaires.csv\")\n",
    "\n",
    "# Output directory\n",
    "SAVE_DIR = os.path.join(DATA_PROCESSED, \"attention_task_optimized\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR = \"outputs_task_optimized_attention\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data save dir: {SAVE_DIR}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined 21 BDI-II concepts\n"
     ]
    }
   ],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters configured:\n",
      "  k_posts: 50\n",
      "  sbert_model: all-MiniLM-L6-v2\n",
      "  post_dim: 384\n",
      "  n_concepts: 21\n",
      "  n_tasks: 1\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    # Data preparation\n",
    "    \"k_posts\": 50,              # Top-k posts per subject\n",
    "    \"sbert_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"post_dim\": 384,            # SBERT embedding dimension\n",
    "    \n",
    "    # Model architecture\n",
    "    \"n_concepts\": 21,\n",
    "    \"n_tasks\": 1,\n",
    "    \"emb_size\": 128,\n",
    "    \"attention_hidden\": 128,    # Attention layer hidden dim\n",
    "    \n",
    "    # CEM-specific\n",
    "    \"shared_prob_gen\": True,\n",
    "    \"intervention_prob\": 0.25,\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size_train\": 32,\n",
    "    \"batch_size_eval\": 64,\n",
    "    \"max_epochs\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 4e-05,\n",
    "    \n",
    "    # Loss\n",
    "    \"concept_loss_weight\": 1.0,\n",
    "    \"task_loss_weight\": 1.0,\n",
    "    \n",
    "    # LDAM Loss\n",
    "    \"use_ldam_loss\": True,\n",
    "    \"ldam_max_margin\": 0.6,\n",
    "    \"ldam_scale\": 40,\n",
    "    \n",
    "    # Weighted Sampler\n",
    "    \"use_weighted_sampler\": True,\n",
    "}\n",
    "\n",
    "print(\"✓ Hyperparameters configured:\")\n",
    "for k, v in list(HYPERPARAMS.items())[:5]:\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for XML parsing\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing null chars and extra whitespace.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\x00\", \"\")\n",
    "    text = WHITESPACE_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_posts_from_xml(xml_path, min_chars=10):\n",
    "    \"\"\"Extract posts from a single XML file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Failed to parse {xml_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    posts = []\n",
    "    for writing in root.findall(\"WRITING\"):\n",
    "        title = writing.findtext(\"TITLE\") or \"\"\n",
    "        text = writing.findtext(\"TEXT\") or \"\"\n",
    "        \n",
    "        combined = normalize_text(f\"{title} {text}\".strip())\n",
    "        if len(combined) >= min_chars:\n",
    "            posts.append(combined)\n",
    "    \n",
    "    return posts\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "  Processing positive examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing positive examples: 100%|██████████| 830/830 [00:00<00:00, 2537.29it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 29868 posts from positive subjects\n",
      "  Processing negative examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing negative examples: 100%|██████████| 4031/4031 [00:02<00:00, 1733.06it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded training data in 2.8s\n",
      "  Total posts: 286,740\n",
      "  Unique subjects: 486\n"
     ]
    }
   ],
   "source": [
    "# Parse training XML files\n",
    "print(\"Loading training data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_data = []\n",
    "\n",
    "# Process positive examples\n",
    "print(\"  Processing positive examples...\")\n",
    "pos_files = glob.glob(os.path.join(POS_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in tqdm(pos_files, desc=\"Processing positive examples\"):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 1,\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "print(f\"  Loaded {sum(d['label'] == 1 for d in train_data)} posts from positive subjects\")\n",
    "\n",
    "# Process negative examples\n",
    "print(\"  Processing negative examples...\")\n",
    "neg_files = glob.glob(os.path.join(NEG_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in tqdm(neg_files, desc=\"Processing negative examples\"):\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 0,\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "train_posts_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"✓ Loaded training data in {time.time()-start_time:.1f}s\")\n",
    "print(f\"  Total posts: {len(train_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {train_posts_df['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading concept labels...\n",
      "✓ Loaded concept labels for 486 subjects\n"
     ]
    }
   ],
   "source": [
    "# Load concept labels from questionnaires\n",
    "print(\"Loading concept labels...\")\n",
    "\n",
    "concepts_df = pd.read_csv(CONCEPTS_FILE)\n",
    "concepts_df[\"subject_id\"] = concepts_df[\"Subject\"].str.replace(\"train_\", \"\", regex=True)\n",
    "\n",
    "# Binarize concept values\n",
    "concept_cols = [col for col in concepts_df.columns if col in CONCEPT_NAMES]\n",
    "for col in concept_cols:\n",
    "    concepts_df[col] = (concepts_df[col] > 0).astype(int)\n",
    "\n",
    "print(f\"✓ Loaded concept labels for {len(concepts_df)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test data...\n",
      "  Temp directory: /var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000gn/T/test_chunks_5701uwv8\n",
      "  Extracted chunk 3/10\n",
      "  Extracted chunk 3/10\n",
      "  Extracted chunk 6/10\n",
      "  Extracted chunk 6/10\n",
      "  Extracted chunk 9/10\n",
      "  Extracted chunk 9/10\n",
      "✓ Test data extracted\n",
      "✓ Test data extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract test ZIP files to temporary directory\n",
    "print(\"Extracting test data...\")\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"test_chunks_\")\n",
    "print(f\"  Temp directory: {temp_dir}\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    zip_path = os.path.join(TEST_DIR, f\"chunk {i}.zip\")\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(temp_dir, f\"chunk_{i}\"))\n",
    "        if i % 3 == 0:\n",
    "            print(f\"  Extracted chunk {i}/10\")\n",
    "\n",
    "print(\"✓ Test data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test labels for 401 subjects\n"
     ]
    }
   ],
   "source": [
    "# Load test labels\n",
    "test_labels_df = pd.read_csv(TEST_LABELS, sep='\t', header=None, names=['subject_id', 'label'])\n",
    "test_labels_df['subject_id'] = test_labels_df['subject_id'].str.strip()\n",
    "\n",
    "print(f\"✓ Loaded test labels for {len(test_labels_df)} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test posts...\n",
      "  Found 4010 XML files\n",
      "✓ Loaded test posts\n",
      "  Total posts: 229,746\n",
      "  Unique subjects: 401\n",
      "✓ Loaded test posts\n",
      "  Total posts: 229,746\n",
      "  Unique subjects: 401\n"
     ]
    }
   ],
   "source": [
    "# Parse test XML files\n",
    "print(\"Loading test posts...\")\n",
    "test_data = []\n",
    "\n",
    "test_xml_files = glob.glob(os.path.join(temp_dir, \"**\", \"*.xml\"), recursive=True)\n",
    "print(f\"  Found {len(test_xml_files)} XML files\")\n",
    "\n",
    "for xml_file in test_xml_files:\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"(test_subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        label_row = test_labels_df[test_labels_df['subject_id'] == subject_id]\n",
    "        if len(label_row) > 0:\n",
    "            label = label_row.iloc[0]['label']\n",
    "            posts = extract_posts_from_xml(xml_file)\n",
    "            for post in posts:\n",
    "                test_data.append({\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"label\": label,\n",
    "                    \"text\": post\n",
    "                })\n",
    "\n",
    "test_posts_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"✓ Loaded test posts\")\n",
    "print(f\"  Total posts: {len(test_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {test_posts_df['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting test data into validation and test...\n",
      "✓ Split complete\n",
      "  Validation: 200 subjects\n",
      "  Test: 201 subjects\n"
     ]
    }
   ],
   "source": [
    "# Split test data into validation and test sets (stratified 50/50)\n",
    "print(\"Splitting test data into validation and test...\")\n",
    "\n",
    "test_subjects = test_posts_df.groupby('subject_id')['label'].first().reset_index()\n",
    "\n",
    "val_subjects, test_subjects_final = train_test_split(\n",
    "    test_subjects['subject_id'],\n",
    "    test_size=0.5,\n",
    "    stratify=test_subjects['label'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_posts_df = test_posts_df[test_posts_df['subject_id'].isin(val_subjects)].copy()\n",
    "test_posts_df_final = test_posts_df[test_posts_df['subject_id'].isin(test_subjects_final)].copy()\n",
    "\n",
    "print(f\"✓ Split complete\")\n",
    "print(f\"  Validation: {val_posts_df['subject_id'].nunique()} subjects\")\n",
    "print(f\"  Test: {test_posts_df_final['subject_id'].nunique()} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: SBERT Setup & Concept Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model: all-MiniLM-L6-v2\n",
      "✓ SBERT model loaded on mps\n",
      "  Embedding dimension: 384\n",
      "✓ SBERT model loaded on mps\n",
      "  Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Load SBERT model\n",
    "print(f\"Loading SBERT model: {HYPERPARAMS['sbert_model']}\")\n",
    "sbert_model = SentenceTransformer(HYPERPARAMS['sbert_model'])\n",
    "sbert_model = sbert_model.to(DEVICE)\n",
    "\n",
    "print(f\"✓ SBERT model loaded on {DEVICE}\")\n",
    "print(f\"  Embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for 21 concepts...\n",
      "✓ Concept embeddings created\n",
      "  Shape: torch.Size([21, 384])\n",
      "✓ Concept embeddings created\n",
      "  Shape: torch.Size([21, 384])\n"
     ]
    }
   ],
   "source": [
    "# Create concept embeddings\n",
    "print(f\"Creating embeddings for {N_CONCEPTS} concepts...\")\n",
    "concept_embeddings = sbert_model.encode(\n",
    "    CONCEPT_NAMES,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "print(f\"✓ Concept embeddings created\")\n",
    "print(f\"  Shape: {concept_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Post Retrieval (Top-k per Subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Post retrieval function defined\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k_posts(subject_id, posts_df, concept_embs, sbert, k=50):\n",
    "    \"\"\"Retrieve top-k posts for a subject based on concept similarity.\"\"\"\n",
    "    subj_posts = posts_df[posts_df['subject_id'] == subject_id]['text'].tolist()\n",
    "    \n",
    "    if len(subj_posts) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Pad if needed\n",
    "    if len(subj_posts) <= k:\n",
    "        if len(subj_posts) < k:\n",
    "            extra_needed = k - len(subj_posts)\n",
    "            padding = list(np.random.choice(subj_posts, size=extra_needed, replace=True))\n",
    "            return subj_posts + padding\n",
    "        else:\n",
    "            return subj_posts\n",
    "    \n",
    "    # Encode and rank\n",
    "    post_embeddings = sbert.encode(\n",
    "        subj_posts,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    cos_scores = util.cos_sim(post_embeddings, concept_embs)\n",
    "    max_sim_scores = cos_scores.max(dim=1).values.cpu().numpy()\n",
    "    top_k_indices = np.argpartition(-max_sim_scores, range(min(k, len(subj_posts))))[:k]\n",
    "    \n",
    "    return [subj_posts[i] for i in top_k_indices]\n",
    "\n",
    "print(\"✓ Post retrieval function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top-50 posts for all subjects...\n",
      "⏰ This will take ~40 minutes\n",
      "  Processing training subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training subjects: 100%|██████████| 486/486 [17:04<00:00,  2.11s/it]\n",
      "Processing training subjects: 100%|██████████| 486/486 [17:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing validation subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation subjects: 100%|██████████| 200/200 [09:09<00:00,  2.75s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing test subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test subjects: 100%|██████████| 201/201 [09:11<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Post retrieval complete in 2126.0s (35.4 min)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top-k posts for all subjects\n",
    "print(f\"Retrieving top-{HYPERPARAMS['k_posts']} posts for all subjects...\")\n",
    "print(\"⏰ This will take ~40 minutes\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Training subjects\n",
    "print(\"  Processing training subjects...\")\n",
    "train_selected = {}\n",
    "train_subjects = train_posts_df['subject_id'].unique()\n",
    "for subject_id in tqdm(train_subjects, desc=\"Processing training subjects\"):\n",
    "    selected = retrieve_top_k_posts(\n",
    "        subject_id, train_posts_df, concept_embeddings, sbert_model, k=HYPERPARAMS['k_posts']\n",
    "    )\n",
    "    train_selected[subject_id] = selected\n",
    "\n",
    "# Validation subjects\n",
    "print(\"  Processing validation subjects...\")\n",
    "val_selected = {}\n",
    "val_subjects = val_posts_df['subject_id'].unique()\n",
    "for subject_id in tqdm(val_subjects, desc=\"Processing validation subjects\"):\n",
    "    selected = retrieve_top_k_posts(\n",
    "        subject_id, val_posts_df, concept_embeddings, sbert_model, k=HYPERPARAMS['k_posts']\n",
    "    )\n",
    "    val_selected[subject_id] = selected\n",
    "\n",
    "# Test subjects\n",
    "print(\"  Processing test subjects...\")\n",
    "test_selected = {}\n",
    "test_subjects = test_posts_df_final['subject_id'].unique()\n",
    "for subject_id in tqdm(test_subjects, desc=\"Processing test subjects\"):\n",
    "    selected = retrieve_top_k_posts(\n",
    "        subject_id, test_posts_df_final, concept_embeddings, sbert_model, k=HYPERPARAMS['k_posts']\n",
    "    )\n",
    "    test_selected[subject_id] = selected\n",
    "\n",
    "print(f\"✓ Post retrieval complete in {time.time()-start_time:.1f}s ({(time.time()-start_time)/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Encode Posts (WITHOUT Pooling)\n",
    "\n",
    "**KEY CHANGE:** Store individual post embeddings instead of pre-pooling them. This allows attention to be learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoding function defined (NO pooling)\n"
     ]
    }
   ],
   "source": [
    "def encode_posts_no_pooling(selected_posts_dict, sbert, max_posts=50):\n",
    "    \"\"\"\n",
    "    Encode selected posts WITHOUT pooling - keep individual embeddings.\n",
    "    \n",
    "    This is the KEY CHANGE from concept-based attention:\n",
    "    - OLD: Encode → Pool with concept-based weights → Save [n, 384]\n",
    "    - NEW: Encode → Save individual posts → [n, k, 384]\n",
    "    \n",
    "    Args:\n",
    "        selected_posts_dict: {subject_id: [post1, post2, ...]}\n",
    "        sbert: Sentence-BERT model\n",
    "        max_posts: Number of posts per subject (pad/truncate)\n",
    "    \n",
    "    Returns:\n",
    "        post_embeddings: [n_subjects, max_posts, 384]\n",
    "        subject_ids: List of subject IDs\n",
    "    \"\"\"\n",
    "    subject_ids = list(selected_posts_dict.keys())\n",
    "    all_post_embeddings = []\n",
    "\n",
    "    for subject_id in tqdm(subject_ids, desc=\"Encoding posts\"):\n",
    "        posts = selected_posts_dict[subject_id]\n",
    "\n",
    "        # Encode all posts for this subject\n",
    "        post_embs = sbert.encode(\n",
    "            posts,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=False\n",
    "        )  # shape: [k, 384]\n",
    "\n",
    "        # Pad or truncate to exactly max_posts\n",
    "        if len(post_embs) < max_posts:\n",
    "            # Pad with zeros\n",
    "            padding = torch.zeros(max_posts - len(post_embs), post_embs.shape[1])\n",
    "            post_embs = torch.cat([post_embs, padding], dim=0)\n",
    "        elif len(post_embs) > max_posts:\n",
    "            # Truncate (shouldn't happen if retrieval is correct)\n",
    "            post_embs = post_embs[:max_posts]\n",
    "\n",
    "        all_post_embeddings.append(post_embs.cpu().numpy())\n",
    "\n",
    "    return np.stack(all_post_embeddings), subject_ids\n",
    "\n",
    "print(\"✓ Encoding function defined (NO pooling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding posts WITHOUT pooling...\n",
      "⏰ This will take ~8-10 minutes\n",
      "  Encoding training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding posts: 100%|██████████| 486/486 [03:25<00:00,  2.37it/s]\n",
      "Encoding posts: 100%|██████████| 486/486 [03:25<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    X_train shape: (486, 50, 384)\n",
      "  Encoding validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding posts: 100%|██████████| 200/200 [01:07<00:00,  2.97it/s]\n",
      "Encoding posts: 100%|██████████| 200/200 [01:07<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    X_val shape: (200, 50, 384)\n",
      "  Encoding test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding posts: 100%|██████████| 201/201 [01:08<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    X_test shape: (201, 50, 384)\n",
      "✓ Encoding complete in 340.9s (5.7 min)\n",
      "✓ Shape verification passed: [n, 50, 384]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode posts for all splits (keep individual embeddings)\n",
    "print(\"Encoding posts WITHOUT pooling...\")\n",
    "print(\"⏰ This will take ~8-10 minutes\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"  Encoding training set...\")\n",
    "X_train, train_subject_ids = encode_posts_no_pooling(\n",
    "    train_selected,\n",
    "    sbert_model,\n",
    "    max_posts=HYPERPARAMS['k_posts']\n",
    ")\n",
    "print(f\"    X_train shape: {X_train.shape}\")  # Should be [n_train, 50, 384]\n",
    "\n",
    "print(\"  Encoding validation set...\")\n",
    "X_val, val_subject_ids = encode_posts_no_pooling(\n",
    "    val_selected,\n",
    "    sbert_model,\n",
    "    max_posts=HYPERPARAMS['k_posts']\n",
    ")\n",
    "print(f\"    X_val shape: {X_val.shape}\")\n",
    "\n",
    "print(\"  Encoding test set...\")\n",
    "X_test, test_subject_ids = encode_posts_no_pooling(\n",
    "    test_selected,\n",
    "    sbert_model,\n",
    "    max_posts=HYPERPARAMS['k_posts']\n",
    ")\n",
    "print(f\"    X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"✓ Encoding complete in {time.time()-start_time:.1f}s ({(time.time()-start_time)/60:.1f} min)\")\n",
    "\n",
    "# Verify shapes\n",
    "assert X_train.shape[1] == HYPERPARAMS['k_posts'], \"Train shape mismatch!\"\n",
    "assert X_train.shape[2] == HYPERPARAMS['post_dim'], \"Embedding dim mismatch!\"\n",
    "print(f\"✓ Shape verification passed: [n, {HYPERPARAMS['k_posts']}, {HYPERPARAMS['post_dim']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6: Build Concept Matrices and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building concept matrices and labels...\n",
      "✓ Matrices built\n",
      "  Train: X=(486, 50, 384), C=(486, 21), y=(486,)\n",
      "  Val:   X=(200, 50, 384), C=(200, 21), y=(200,)\n",
      "  Test:  X=(201, 50, 384), C=(201, 21), y=(201,)\n",
      "Training label distribution: [403  83]\n",
      "✓ Matrices built\n",
      "  Train: X=(486, 50, 384), C=(486, 21), y=(486,)\n",
      "  Val:   X=(200, 50, 384), C=(200, 21), y=(200,)\n",
      "  Test:  X=(201, 50, 384), C=(201, 21), y=(201,)\n",
      "Training label distribution: [403  83]\n"
     ]
    }
   ],
   "source": [
    "# Build concept matrices and label vectors\n",
    "print(\"Building concept matrices and labels...\")\n",
    "\n",
    "# Training: get concepts from questionnaires\n",
    "C_train = []\n",
    "y_train = []\n",
    "for subject_id in train_subject_ids:\n",
    "    label = train_posts_df[train_posts_df['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_train.append(label)\n",
    "    \n",
    "    concept_row = concepts_df[concepts_df['subject_id'] == subject_id]\n",
    "    if len(concept_row) > 0:\n",
    "        concepts = concept_row[concept_cols].values[0]\n",
    "    else:\n",
    "        concepts = np.zeros(N_CONCEPTS)\n",
    "    C_train.append(concepts)\n",
    "\n",
    "C_train = np.array(C_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "# Validation: zeros for concepts (no ground truth)\n",
    "C_val = np.zeros((len(val_subject_ids), N_CONCEPTS), dtype=np.float32)\n",
    "y_val = []\n",
    "for subject_id in val_subject_ids:\n",
    "    label = val_posts_df[val_posts_df['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_val.append(label)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "\n",
    "# Test: zeros for concepts\n",
    "C_test = np.zeros((len(test_subject_ids), N_CONCEPTS), dtype=np.float32)\n",
    "y_test = []\n",
    "for subject_id in test_subject_ids:\n",
    "    label = test_posts_df_final[test_posts_df_final['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_test.append(label)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "print(\"✓ Matrices built\")\n",
    "print(f\"  Train: X={X_train.shape}, C={C_train.shape}, y={y_train.shape}\")\n",
    "print(f\"  Val:   X={X_val.shape}, C={C_val.shape}, y={y_val.shape}\")\n",
    "print(f\"  Test:  X={X_test.shape}, C={C_test.shape}, y={y_test.shape}\")\n",
    "print(f\"Training label distribution: {np.bincount(y_train.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 7: Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance:\n",
      "  Negative samples: 403\n",
      "  Positive samples: 83\n",
      "  Ratio: 1:4.86\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "n_negative = int(np.sum(y_train == 0))\n",
    "n_positive = int(np.sum(y_train == 1))\n",
    "pos_weight = n_negative / n_positive\n",
    "\n",
    "# Update hyperparameters\n",
    "HYPERPARAMS['n_positive'] = n_positive\n",
    "HYPERPARAMS['n_negative'] = n_negative\n",
    "\n",
    "print(f\"Class imbalance:\")\n",
    "print(f\"  Negative samples: {n_negative}\")\n",
    "print(f\"  Positive samples: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8: Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets...\n",
      "✓ Datasets saved to /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/attention_task_optimized\n",
      "  train_data.npz: (486, 50, 384)\n",
      "  val_data.npz:   (200, 50, 384)\n",
      "  test_data.npz:  (201, 50, 384)\n",
      "  class_weights.json\n",
      "NEW DATA SHAPE: [486, 50, 384]\n",
      "  OLD DATA SHAPE would have been: [486, 384] (pre-pooled)\n",
      "✓ Datasets saved to /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/attention_task_optimized\n",
      "  train_data.npz: (486, 50, 384)\n",
      "  val_data.npz:   (200, 50, 384)\n",
      "  test_data.npz:  (201, 50, 384)\n",
      "  class_weights.json\n",
      "NEW DATA SHAPE: [486, 50, 384]\n",
      "  OLD DATA SHAPE would have been: [486, 384] (pre-pooled)\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets to disk\n",
    "print(\"Saving datasets...\")\n",
    "\n",
    "# Save numpy arrays (NEW SHAPE: [n, k, 384])\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"train_data.npz\"),\n",
    "    X=X_train,  # [n_train, 50, 384] - individual posts!\n",
    "    C=C_train,\n",
    "    y=y_train,\n",
    "    subject_ids=np.array(train_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"val_data.npz\"),\n",
    "    X=X_val,\n",
    "    C=C_val,\n",
    "    y=y_val,\n",
    "    subject_ids=np.array(val_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"test_data.npz\"),\n",
    "    X=X_test,\n",
    "    C=C_test,\n",
    "    y=y_test,\n",
    "    subject_ids=np.array(test_subject_ids)\n",
    ")\n",
    "\n",
    "# Save class weights info\n",
    "class_info = {\n",
    "    \"n_positive\": n_positive,\n",
    "    \"n_negative\": n_negative,\n",
    "    \"pos_weight\": float(pos_weight)\n",
    "}\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"class_weights.json\"), 'w') as f:\n",
    "    json.dump(class_info, f, indent=4)\n",
    "\n",
    "print(f\"✓ Datasets saved to {SAVE_DIR}\")\n",
    "print(f\"  train_data.npz: {X_train.shape}\")\n",
    "print(f\"  val_data.npz:   {X_val.shape}\")\n",
    "print(f\"  test_data.npz:  {X_test.shape}\")\n",
    "print(f\"  class_weights.json\")\n",
    "print(f\"NEW DATA SHAPE: [{X_train.shape[0]}, {X_train.shape[1]}, {X_train.shape[2]}]\")\n",
    "print(f\"  OLD DATA SHAPE would have been: [{X_train.shape[0]}, {X_train.shape[2]}] (pre-pooled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned up temporary directory\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary directory\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"✓ Cleaned up temporary directory\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Failed to clean up temporary directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "          DATA PREPARATION COMPLETE\n",
      "======================================================================\n",
      "Saved data with shape: [n_samples, 50, 384]\n",
      "  Memory per split: ~35.6 MB\n",
      "Next: Train model with task-optimized attention\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\" + \"=\"*70)\n",
    "print(\"          DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Saved data with shape: [n_samples, {HYPERPARAMS['k_posts']}, {HYPERPARAMS['post_dim']}]\")\n",
    "print(f\"  Memory per split: ~{X_train.nbytes / 1024**2:.1f} MB\")\n",
    "print(\"Next: Train model with task-optimized attention\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: MODEL TRAINING WITH TASK-OPTIMIZED ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9: PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Datasets created\n",
      "  Train: 486 samples\n",
      "  Val:   200 samples\n",
      "  Test:  201 samples\n"
     ]
    }
   ],
   "source": [
    "class CEMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for CEM with individual post embeddings.\n",
    "    \n",
    "    X shape: [n_samples, k_posts, post_dim] - NEW!\n",
    "    C shape: [n_samples, n_concepts]\n",
    "    y shape: [n_samples, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, X, C, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # [n, 50, 384]\n",
    "        self.C = torch.tensor(C, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.C[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CEMDataset(X_train, C_train, y_train)\n",
    "val_dataset = CEMDataset(X_val, C_val, y_val)\n",
    "test_dataset = CEMDataset(X_test, C_test, y_test)\n",
    "\n",
    "print(f\"✓ Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val:   {len(val_dataset)} samples\")\n",
    "print(f\"  Test:  {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using WeightedRandomSampler\n",
      "✓ All DataLoaders created\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "if HYPERPARAMS['use_weighted_sampler']:\n",
    "    class_sample_counts = np.bincount(y_train.astype(int))\n",
    "    weights = 1. / class_sample_counts\n",
    "    sample_weights = weights[y_train.astype(int)]\n",
    "    \n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Using WeightedRandomSampler\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], sampler=train_sampler)\n",
    "else:\n",
    "    print(\"✓ Using standard DataLoader (shuffle=True)\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "\n",
    "print(\"✓ All DataLoaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 10: Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LDAM Loss defined\n"
     ]
    }
   ],
   "source": [
    "class LDAMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Label-Distribution-Aware Margin (LDAM) Loss for long-tailed recognition.\n",
    "    \n",
    "    Creates class-dependent margins to make decision boundaries harder for minority classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_positive, n_negative, max_margin=0.5, scale=30):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        self.max_margin = max_margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Compute class frequencies\n",
    "        total = n_positive + n_negative\n",
    "        freq_pos = n_positive / total\n",
    "        freq_neg = n_negative / total\n",
    "        \n",
    "        # Compute margins: minority class gets larger margin\n",
    "        margin_pos = max_margin * (freq_pos ** (-0.25))\n",
    "        margin_neg = max_margin * (freq_neg ** (-0.25))\n",
    "        \n",
    "        self.register_buffer('margin_pos', torch.tensor(margin_pos))\n",
    "        self.register_buffer('margin_neg', torch.tensor(margin_neg))\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1).float()\n",
    "        \n",
    "        # Apply class-dependent margins\n",
    "        margin = targets * self.margin_pos + (1 - targets) * (-self.margin_neg)\n",
    "        adjusted_logits = (logits - margin) * self.scale\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(adjusted_logits, targets, reduction='mean')\n",
    "\n",
    "print(\"✓ LDAM Loss defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 11: Task-Optimized Attention CEM Model\n",
    "\n",
    "**KEY INNOVATION:** Learnable attention layer that computes post importance scores, optimized via task loss gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TaskOptimizedAttentionCEM model defined\n",
      "  Key innovation: Learnable attention layer optimized via task loss\n"
     ]
    }
   ],
   "source": [
    "class TaskOptimizedAttentionCEM(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    CEM with learnable attention that optimizes post selection for task loss.\n",
    "\n",
    "    Architecture:\n",
    "        1. Attention Layer: Learns which posts matter for task (NEW!)\n",
    "        2. Attention Pooling: Weighted average of posts\n",
    "        3. Concept Extractor: Extracts pre-concept features\n",
    "        4. Concept Layers: 21 dual-embedding concepts\n",
    "        5. Task Classifier: Final depression prediction\n",
    "        \n",
    "    KEY DIFFERENCE from original:\n",
    "        - Input: [batch, k_posts, post_dim] instead of [batch, post_dim]\n",
    "        - Has learnable attention_layer\n",
    "        - Attention weights optimized via task loss gradients\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_concepts=21,\n",
    "        emb_size=128,\n",
    "        k_posts=50,\n",
    "        post_dim=384,\n",
    "        attention_hidden=128,\n",
    "        shared_prob_gen=True,\n",
    "        intervention_prob=0.25,\n",
    "        concept_loss_weight=1.0,\n",
    "        task_loss_weight=1.0,\n",
    "        learning_rate=0.01,\n",
    "        weight_decay=4e-05,\n",
    "        use_ldam_loss=True,\n",
    "        n_positive=83,\n",
    "        n_negative=403,\n",
    "        ldam_max_margin=0.5,\n",
    "        ldam_scale=30,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STAGE 0: ATTENTION MODULE (NEW!)\n",
    "        # =====================================================================\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(post_dim, attention_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(attention_hidden, 1)  # Score for each post\n",
    "        )\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STAGE 1: CONCEPT EXTRACTOR (receives pooled embedding)\n",
    "        # =====================================================================\n",
    "        self.concept_extractor = nn.Sequential(\n",
    "            nn.Linear(post_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 256)\n",
    "        )\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STAGE 2: CONCEPT CONTEXT GENERATORS (21 concepts, dual embeddings)\n",
    "        # =====================================================================\n",
    "        self.context_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, emb_size * 2),  # Dual embeddings\n",
    "                nn.LeakyReLU()\n",
    "            ) for _ in range(n_concepts)\n",
    "        ])\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STAGE 3: CONCEPT PROBABILITY GENERATORS\n",
    "        # =====================================================================\n",
    "        if shared_prob_gen:\n",
    "            self.prob_generator = nn.Linear(emb_size * 2, 1)\n",
    "        else:\n",
    "            self.prob_generators = nn.ModuleList([\n",
    "                nn.Linear(emb_size * 2, 1) for _ in range(n_concepts)\n",
    "            ])\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STAGE 4: TASK CLASSIFIER (C2Y model)\n",
    "        # =====================================================================\n",
    "        self.task_classifier = nn.Sequential(\n",
    "            nn.Linear(n_concepts * emb_size, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        # =====================================================================\n",
    "        # LOSSES\n",
    "        # =====================================================================\n",
    "        self.concept_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        if use_ldam_loss:\n",
    "            self.task_loss_fn = LDAMLoss(n_positive, n_negative, ldam_max_margin, ldam_scale)\n",
    "        else:\n",
    "            self.task_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, post_embeddings, c_true=None, train=False):\n",
    "        \"\"\"\n",
    "        Forward pass with learnable attention.\n",
    "        \n",
    "        Args:\n",
    "            post_embeddings: [batch, k_posts, post_dim] - individual posts\n",
    "            c_true: [batch, n_concepts] - concept labels (for interventions)\n",
    "            train: bool - training mode\n",
    "        \n",
    "        Returns:\n",
    "            c_logits: [batch, 21] - concept predictions\n",
    "            y_logits: [batch, 1] - task prediction\n",
    "            attn_weights: [batch, k_posts] - attention weights (for analysis)\n",
    "        \"\"\"\n",
    "        batch_size = post_embeddings.shape[0]\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 1: Compute attention weights (LEARNABLE!)\n",
    "        # ====================================================================\n",
    "        attn_scores = self.attention_layer(post_embeddings)  # [batch, k, 1]\n",
    "        attn_weights = torch.softmax(attn_scores.squeeze(-1), dim=1)  # [batch, k]\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 2: Attention-weighted pooling\n",
    "        # ====================================================================\n",
    "        pooled = torch.sum(\n",
    "            attn_weights.unsqueeze(-1) * post_embeddings,  # [batch, k, 1] * [batch, k, 384]\n",
    "            dim=1\n",
    "        )  # [batch, 384]\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 3: Concept extraction (existing CEM pipeline)\n",
    "        # ====================================================================\n",
    "        pre_c = self.concept_extractor(pooled)  # [batch, 256]\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 4: Concept context and probability generation\n",
    "        # ====================================================================\n",
    "        contexts = []\n",
    "        c_logits_list = []\n",
    "        \n",
    "        for i, context_layer in enumerate(self.context_layers):\n",
    "            # Select probability generator\n",
    "            if self.hparams.shared_prob_gen:\n",
    "                prob_gen = self.prob_generator\n",
    "            else:\n",
    "                prob_gen = self.prob_generators[i]\n",
    "            \n",
    "            # Generate context\n",
    "            context = context_layer(pre_c)  # [batch, 2*emb_size]\n",
    "            \n",
    "            # Generate concept probability (logit)\n",
    "            logit = prob_gen(context)  # [batch, 1]\n",
    "            c_logits_list.append(logit)\n",
    "            \n",
    "            contexts.append(context.unsqueeze(1))  # [batch, 1, 2*emb_size]\n",
    "        \n",
    "        c_logits = torch.cat(c_logits_list, dim=-1)  # [batch, 21]\n",
    "        contexts = torch.cat(contexts, dim=1)  # [batch, 21, 2*emb_size]\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 5: Concept interventions (during training)\n",
    "        # ====================================================================\n",
    "        c_probs = torch.sigmoid(c_logits)  # [batch, 21]\n",
    "        \n",
    "        if train and (self.hparams.intervention_prob > 0) and (c_true is not None):\n",
    "            intervention_mask = torch.bernoulli(\n",
    "                torch.ones_like(c_probs) * self.hparams.intervention_prob\n",
    "            )\n",
    "            c_probs = c_probs * (1 - intervention_mask) + c_true * intervention_mask\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 6: Dual embeddings and task prediction\n",
    "        # ====================================================================\n",
    "        c_pred_embs = (\n",
    "            contexts[:, :, :self.hparams.emb_size] * c_probs.unsqueeze(-1) +\n",
    "            contexts[:, :, self.hparams.emb_size:] * (1 - c_probs.unsqueeze(-1))\n",
    "        )\n",
    "        c_pred_embs = c_pred_embs.view(batch_size, -1)  # [batch, 21*emb_size]\n",
    "        \n",
    "        y_logits = self.task_classifier(c_pred_embs)  # [batch, 1]\n",
    "        \n",
    "        return c_logits, y_logits, attn_weights\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        post_embs, y, c_true = batch\n",
    "        c_logits, y_logits, _ = self.forward(post_embs, c_true=c_true, train=True)\n",
    "        \n",
    "        # Task loss\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = (self.hparams.task_loss_weight * task_loss + \n",
    "                self.hparams.concept_loss_weight * concept_loss)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('train_concept_loss', concept_loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        post_embs, y, c_true = batch\n",
    "        c_logits, y_logits, _ = self.forward(post_embs, c_true=c_true, train=False)\n",
    "        \n",
    "        # Task loss\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = (self.hparams.task_loss_weight * task_loss +\n",
    "                self.hparams.concept_loss_weight * concept_loss)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_task_loss', task_loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "print(\"✓ TaskOptimizedAttentionCEM model defined\")\n",
    "print(\"  Key innovation: Learnable attention layer optimized via task loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 12: Model Initialization & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model initialized\n",
      "  Attention hidden dim: 128\n",
      "  Concept embedding size: 128\n",
      "  Using LDAM Loss: True\n"
     ]
    }
   ],
   "source": [
    "# Initialize TaskOptimizedAttentionCEM model\n",
    "model = TaskOptimizedAttentionCEM(\n",
    "    n_concepts=HYPERPARAMS['n_concepts'],\n",
    "    emb_size=HYPERPARAMS['emb_size'],\n",
    "    k_posts=HYPERPARAMS['k_posts'],\n",
    "    post_dim=HYPERPARAMS['post_dim'],\n",
    "    attention_hidden=HYPERPARAMS['attention_hidden'],\n",
    "    shared_prob_gen=HYPERPARAMS['shared_prob_gen'],\n",
    "    intervention_prob=HYPERPARAMS['intervention_prob'],\n",
    "    concept_loss_weight=HYPERPARAMS['concept_loss_weight'],\n",
    "    task_loss_weight=HYPERPARAMS['task_loss_weight'],\n",
    "    learning_rate=HYPERPARAMS['learning_rate'],\n",
    "    weight_decay=HYPERPARAMS['weight_decay'],\n",
    "    use_ldam_loss=HYPERPARAMS['use_ldam_loss'],\n",
    "    n_positive=HYPERPARAMS['n_positive'],\n",
    "    n_negative=HYPERPARAMS['n_negative'],\n",
    "    ldam_max_margin=HYPERPARAMS['ldam_max_margin'],\n",
    "    ldam_scale=HYPERPARAMS['ldam_scale']\n",
    ")\n",
    "\n",
    "print(\"✓ Model initialized\")\n",
    "print(f\"  Attention hidden dim: {HYPERPARAMS['attention_hidden']}\")\n",
    "print(f\"  Concept embedding size: {HYPERPARAMS['emb_size']}\")\n",
    "print(f\"  Using LDAM Loss: {HYPERPARAMS['use_ldam_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer configured\n"
     ]
    }
   ],
   "source": [
    "# Setup trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(OUTPUT_DIR, \"models\"),\n",
    "    filename=\"task-attention-cem-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=HYPERPARAMS['max_epochs'],\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    logger=CSVLogger(save_dir=os.path.join(OUTPUT_DIR, \"logs\"), name=\"task_attention_cem\"),\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "⏰ This will take ~15-20 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory outputs_task_optimized_attention/models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name              | Type              | Params\n",
      "--------------------------------------------------------\n",
      "0 | attention_layer   | Sequential        | 49.4 K\n",
      "1 | concept_extractor | Sequential        | 164 K \n",
      "2 | context_layers    | ModuleList        | 1.4 M \n",
      "3 | prob_generator    | Linear            | 257   \n",
      "4 | task_classifier   | Sequential        | 344 K \n",
      "5 | concept_loss_fn   | BCEWithLogitsLoss | 0     \n",
      "6 | task_loss_fn      | LDAMLoss          | 0     \n",
      "--------------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.760     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93863d539c741e5971eef2a002cebe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49001f4840004a9091e8ffe029f182a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e647bcaef264c26a278c7a0c0744a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62621f51cf374da295a749266d05ed92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b072e694124909bdc5518c96beac43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a81165598a1471395a8a0f0026c2edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edc858d16a445efab9224b63102454e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442d223bd9e14f8d97432b79949deaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b037c5402edf4341b79d521e8c25c81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fc6e6df9ed48cea3e8e3b161bd0296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d46f029ebce43b3b9758b19bbce478c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fa34fda2d24817a6e4db8934c49baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68cc7d5b94341cdb5dc386f850a9710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667bebafab214a4283eebf5b2a13c33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec015d82a7d2447aad9cd4336a386861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2c1cc7aeb3496499977130986d4024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8b62dd26e644de8e75b3cdc28ce274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c4d8cf98de48daa2b10805c37f9bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0398e0c74d4b6aa940b48e298b8de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494bdc1cd6cd43ef8e9a0480b92f491b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa8775873e7443ca25cc2b995ba90cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01aff1c8f424177b53e8d353822f979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1474615b50724d90b6099a8f42657c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e719c44846f4a7dbf1063eba41d6026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dd2878a13e4857994674583429961f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbbbf4ea04a474686f04b5cef39c9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c649ef7335254167bc76d78f7188dffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3197d2a170463fbabf0542df9dd07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17bcc2972cd4cd5acf520d5e2ae3867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e9b837da3e48c4a477ca6443f5ff24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ccb525b4e24e18b4f9cad8d166bd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee73162768541de9b26bc70c240c2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4a40b4577447bc87f53c78a3599f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146bf1eb111846078548be126f658481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9359768c7584483852de7776a71ad06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9528bc49af24b41a5017f1e85269ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4542390807634bdc95ec03414b541f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bd56b29f4b4f75b98a9c800e63cbc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e894e6d961451d9da82e11d98d03f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771edb706609457585a0ee7b7c95bc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095da7c0bb2449229fd9d60b7ccf06c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa322907d644d499285d58fa68df43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8143318bdb97407bafbec89931fba23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1871bbacd16e40f19d2141c165beac92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4910526d484a425abc78a971df614bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae7e6c0a6a849e993dfc340c450b42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ebadcbc0c84a028e471b72f5bdf004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0908f38b42024f8fb7252dbb969c8399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab4ce6e07eb475eb59065c7238b0254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7d69b93f13427ab01044c13ce2d330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292880b66a0442fd81a4bf82c19e5351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c842aeaf7847c09896504a9a2d8bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89d5ca78f5443bcb26d999c14a8b083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2679e3fd0309446f87ce0a5f1bee15a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3f285865e04d359dac1520c2389c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e20e3f687674f2b86513c2e0d44ffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b572940a798e4b2bb6676410e2a2793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeebf42705749b9b2c1af1b6e14c432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd2705bfd114fe1945d33d032079f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391721fac3f1455f938ebf6c619dab23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9eaab7f489f4ad399c6dfa5e4c55bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260ad8a8ba024edf8c2d07903f1e410f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7409b8de1b4931b133ca03851f9a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d0a0b400444394b73c46e406889262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4944b4c275484e9e9e5fafb16f3733cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59930b6fe88433bba93970e00345451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7abea191a54464facef5a2ad2e17120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dec21eac8a4c3fb2ab0ddc10a812d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e396b39cff44332ba37b494d4cc5833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367b9b24ffc44341a62300a845b36213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5825120a15426684f84c0586a5519b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db9eb4d562a45bd8b2fdedad364a5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e404efc6b90400b82e1e54e4330f7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3febf76306334f0d842e5325123051c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56c668b722c4b029609293d01e0ae5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42badda42f8640a8b23019744ded6dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cf81aae7ac4aceaf41090b97bb5cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26555c7d89984f389e7f8eac6db60882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5173689e02724ca4963a1293fea1c690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b27bccd4414d14ab56147118487a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af51552c8f95436ab1893a3927c59d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b70cce9ec604f7da022419e95284fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f75fba12e34495d97bc33b405cced08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b840cbd384b43abad758e141dee659e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192ab5757c8045aca2251b1889152fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d74035ca534ed78fc1a774dba3c146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76646b3e400244b3a2e5c9da0d9956f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7401ce48f3b40b8b20fa9cad9c8c8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79335069990143e8b5a10b1fc6a6619f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4187535d354c0fa32d591b5a893431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9ab986583e4c159b1e7a82b948c91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca288a4d03f48be939b3c750522e443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49204c28f57743eda9567b8c92337c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1878db1d18984cfa8ddb2e7603453834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf27cf0f28114195b5279249b76a2028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2123339ec982450ea758f9077029a49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a07ac223df246f3bb9526be42e4ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526e887bf6f74adca6c93c9bb26e20d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ff7bad953c43469e0bb2f44417890d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aeb528edb3642f1bfc942fd3ef9feba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3f7f602c1443dbae683cf4714dc301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f445a4e7b144509a1e0c1355295acc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "print(\"⏰ This will take ~15-20 minutes\")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 13: Validation Threshold Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting decision threshold on validation set...\n",
      "✓ Selected threshold: 0.50\n",
      "  Target recall ≥ 0.8, achieved precision = 0.000\n",
      "✓ Selected threshold: 0.50\n",
      "  Target recall ≥ 0.8, achieved precision = 0.000\n"
     ]
    }
   ],
   "source": [
    "# Select decision threshold on validation set\n",
    "print(\"Selecting decision threshold on validation set...\")\n",
    "\n",
    "model.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "model = model.to(device_obj)\n",
    "\n",
    "y_val_true = []\n",
    "y_val_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for post_embs, y_batch, c_batch in val_loader:\n",
    "        post_embs = post_embs.to(device_obj)\n",
    "        \n",
    "        _, y_logits, _ = model(post_embs)\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "        \n",
    "        y_val_true.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_val_prob.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "\n",
    "y_val_true = np.array(y_val_true)\n",
    "y_val_prob = np.array(y_val_prob)\n",
    "\n",
    "# Find best threshold for 80% recall\n",
    "best_threshold = 0.5\n",
    "best_precision = 0.0\n",
    "target_recall = 0.80\n",
    "\n",
    "for threshold in np.linspace(0.01, 0.50, 50):\n",
    "    y_pred_temp = (y_val_prob >= threshold).astype(int)\n",
    "    \n",
    "    if np.sum(y_pred_temp) == 0:\n",
    "        continue\n",
    "    \n",
    "    recall = recall_score(y_val_true, y_pred_temp)\n",
    "    precision = precision_score(y_val_true, y_pred_temp)\n",
    "    \n",
    "    if recall >= target_recall and precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"✓ Selected threshold: {best_threshold:.2f}\")\n",
    "print(f\"  Target recall ≥ {target_recall}, achieved precision = {best_precision:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 14: Attention Analysis\n",
    "\n",
    "**NEW CAPABILITY:** Analyze which posts the model focuses on for its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "           ATTENTION WEIGHT ANALYSIS\n",
      "======================================================================\n",
      "Attention weights shape: (200, 50)\n",
      "Attention statistics:\n",
      "  Mean weight: 0.0200\n",
      "  Std weight:  0.1156\n",
      "  Max weight:  1.0000\n",
      "  Min weight:  0.0000\n",
      "Attention entropy:\n",
      " Mean: 0.5934 (max = 3.9120 for uniform)\n",
      " Std:  0.5254\n",
      " Concentration: 84.8%\n",
      "Attention by prediction quality:\n",
      "  Correct predictions:   entropy = 0.5673\n",
      "  Incorrect predictions: entropy = 0.8052\n",
      "✓ Saved attention analysis to outputs_task_optimized_attention/attention_analysis.npz\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze attention weights on validation set\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"           ATTENTION WEIGHT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device_obj)\n",
    "\n",
    "all_attn_weights = []\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for post_embs, y, c in val_loader:\n",
    "        post_embs = post_embs.to(device_obj)\n",
    "        \n",
    "        c_logits, y_logits, attn_weights = model(post_embs)\n",
    "        \n",
    "        all_attn_weights.append(attn_weights.cpu().numpy())\n",
    "        all_y_true.append(y.cpu().numpy())\n",
    "        all_y_pred.append(torch.sigmoid(y_logits).cpu().numpy())\n",
    "\n",
    "attn_weights_val = np.vstack(all_attn_weights)  # [n_val, k_posts]\n",
    "y_true_val = np.concatenate(all_y_true)\n",
    "y_pred_val = np.concatenate(all_y_pred)\n",
    "\n",
    "print(f\"Attention weights shape: {attn_weights_val.shape}\")\n",
    "print(f\"Attention statistics:\")\n",
    "print(f\"  Mean weight: {attn_weights_val.mean():.4f}\")\n",
    "print(f\"  Std weight:  {attn_weights_val.std():.4f}\")\n",
    "print(f\"  Max weight:  {attn_weights_val.max():.4f}\")\n",
    "print(f\"  Min weight:  {attn_weights_val.min():.4f}\")\n",
    "\n",
    "# Compute entropy of attention distribution\n",
    "def attention_entropy(weights):\n",
    "    \"\"\"Compute entropy of attention distribution\"\"\"\n",
    "    entropy = -np.sum(weights * np.log(weights + 1e-10), axis=1)\n",
    "    return entropy\n",
    "\n",
    "entropy_val = attention_entropy(attn_weights_val)\n",
    "max_entropy = np.log(HYPERPARAMS['k_posts'])\n",
    "\n",
    "print(f\"Attention entropy:\")\n",
    "print(f\" Mean: {entropy_val.mean():.4f} (max = {max_entropy:.4f} for uniform)\")\n",
    "print(f\" Std:  {entropy_val.std():.4f}\")\n",
    "print(f\" Concentration: {(max_entropy - entropy_val.mean()) / max_entropy * 100:.1f}%\")\n",
    "\n",
    "# Compare attention for correct vs incorrect predictions\n",
    "y_pred_binary = (y_pred_val > 0.5).astype(int).flatten()\n",
    "y_true_binary = y_true_val.astype(int).flatten()\n",
    "correct_mask = (y_pred_binary == y_true_binary)\n",
    "\n",
    "print(f\"Attention by prediction quality:\")\n",
    "print(f\"  Correct predictions:   entropy = {entropy_val[correct_mask].mean():.4f}\")\n",
    "print(f\"  Incorrect predictions: entropy = {entropy_val[~correct_mask].mean():.4f}\")\n",
    "\n",
    "# Save attention weights for further analysis\n",
    "np.savez(\n",
    "    os.path.join(OUTPUT_DIR, 'attention_analysis.npz'),\n",
    "    attn_weights=attn_weights_val,\n",
    "    y_true=y_true_val,\n",
    "    y_pred=y_pred_val,\n",
    "    entropy=entropy_val,\n",
    "    correct_mask=correct_mask\n",
    ")\n",
    "\n",
    "print(f\"✓ Saved attention analysis to {OUTPUT_DIR}/attention_analysis.npz\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 15: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                  TEST SET EVALUATION\n",
      "======================================================================\n",
      "Running inference on test set...\n",
      "✓ Inference complete\n",
      "✓ Inference complete\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test set\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"                  TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device_obj)\n",
    "\n",
    "y_true_list = []\n",
    "y_prob_list = []\n",
    "concept_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for post_embs, y_batch, c_batch in test_loader:\n",
    "        post_embs = post_embs.to(device_obj)\n",
    "        \n",
    "        c_logits, y_logits, _ = model(post_embs)\n",
    "        c_probs = torch.sigmoid(c_logits).cpu().numpy()\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "        \n",
    "        y_true_list.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_prob_list.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "        concept_probs_list.extend(c_probs.tolist())\n",
    "\n",
    "y_true = np.array(y_true_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "concept_probs = np.array(concept_probs_list)\n",
    "\n",
    "# Apply threshold\n",
    "y_pred = (y_prob >= best_threshold).astype(int)\n",
    "\n",
    "print(\"✓ Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Threshold: 0.50\n",
      "                 CONFUSION MATRIX                 \n",
      "==================================================\n",
      "                     │ Predicted Neg │ Predicted Pos\n",
      "──────────────────────────────────────────────────\n",
      "     Actual Negative │   TN = 170   │    FP = 5   \n",
      "     Actual Positive │   FN = 16    │   TP = 10   \n",
      "==================================================\n",
      "TP: 10/26 (38.5% caught)\n",
      "  FN: 16/26 (61.5% missed)\n",
      "Performance Metrics:\n",
      "  MCC:                0.4547\n",
      "  F1 Score:           0.4878\n",
      "  Recall:             0.3846\n",
      "  Precision:          0.6667\n",
      "  ROC-AUC:            0.7857\n",
      "  Accuracy:           0.8955\n",
      "  Balanced Accuracy:  0.6780\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.97      0.94       175\n",
      "    Positive       0.67      0.38      0.49        26\n",
      "\n",
      "    accuracy                           0.90       201\n",
      "   macro avg       0.79      0.68      0.71       201\n",
      "weighted avg       0.88      0.90      0.88       201\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute all metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "f1_binary = f1_score(y_true, y_pred, pos_label=1)\n",
    "precision_binary = precision_score(y_true, y_pred, pos_label=1)\n",
    "recall_binary = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# Print results\n",
    "print(f\"Decision Threshold: {best_threshold:.2f}\")\n",
    "\n",
    "print(f\"{'CONFUSION MATRIX':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'':>20} │ {'Predicted Neg':^12} │ {'Predicted Pos':^12}\")\n",
    "print(\"─\"*50)\n",
    "print(f\"{'Actual Negative':>20} │ {f'TN = {tn}':^12} │ {f'FP = {fp}':^12}\")\n",
    "print(f\"{'Actual Positive':>20} │ {f'FN = {fn}':^12} │ {f'TP = {tp}':^12}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "n_pos = int(np.sum(y_true))\n",
    "n_neg = int(len(y_true) - n_pos)\n",
    "\n",
    "print(f\"TP: {tp}/{n_pos} ({100*tp/n_pos if n_pos > 0 else 0:.1f}% caught)\")\n",
    "print(f\"  FN: {fn}/{n_pos} ({100*fn/n_pos if n_pos > 0 else 0:.1f}% missed)\")\n",
    "\n",
    "print(f\"Performance Metrics:\")\n",
    "print(f\"  MCC:                {mcc:.4f}\")\n",
    "print(f\"  F1 Score:           {f1_binary:.4f}\")\n",
    "print(f\"  Recall:             {recall_binary:.4f}\")\n",
    "print(f\"  Precision:          {precision_binary:.4f}\")\n",
    "print(f\"  ROC-AUC:            {roc_auc:.4f}\")\n",
    "print(f\"  Accuracy:           {acc:.4f}\")\n",
    "print(f\"  Balanced Accuracy:  {balanced_acc:.4f}\")\n",
    "\n",
    "print(\"\" + classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to outputs_task_optimized_attention/results/\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "metrics_dict = {\n",
    "    \"model_type\": \"task_optimized_attention_cem\",\n",
    "    \"threshold\": float(best_threshold),\n",
    "    \"n_samples\": int(len(y_true)),\n",
    "    \"n_positive\": int(np.sum(y_true)),\n",
    "    \"n_negative\": int(len(y_true) - np.sum(y_true)),\n",
    "    \"accuracy\": float(acc),\n",
    "    \"balanced_accuracy\": float(balanced_acc),\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "    \"mcc\": float(mcc),\n",
    "    \"f1_binary\": float(f1_binary),\n",
    "    \"precision_binary\": float(precision_binary),\n",
    "    \"recall_binary\": float(recall_binary),\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)}\n",
    "}\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"results\"), exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"results/test_metrics.json\"), 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'subject_id': test_subject_ids,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'y_prob': y_prob\n",
    "})\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    predictions_df[concept_name] = concept_probs[:, i]\n",
    "\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_DIR, \"results/test_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"✓ Results saved to {OUTPUT_DIR}/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "        TASK-OPTIMIZED ATTENTION CEM - COMPLETE\n",
      "======================================================================\n",
      "Generated files:\n",
      "  Data:      /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/attention_task_optimized/\n",
      "  Model:     outputs_task_optimized_attention/models/\n",
      "  Metrics:   outputs_task_optimized_attention/results/test_metrics.json\n",
      "  Predictions: outputs_task_optimized_attention/results/test_predictions.csv\n",
      "  Attention: outputs_task_optimized_attention/attention_analysis.npz\n",
      "Key Results:\n",
      "  Test MCC:       0.4547\n",
      "  Test F1:        0.4878\n",
      "  Test Recall:    0.3846\n",
      "  Attention concentration: 84.8%\n",
      "✅ Unified pipeline with task-optimized attention complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\" + \"=\"*70)\n",
    "print(\"        TASK-OPTIMIZED ATTENTION CEM - COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Generated files:\")\n",
    "print(f\"  Data:      {SAVE_DIR}/\")\n",
    "print(f\"  Model:     {OUTPUT_DIR}/models/\")\n",
    "print(f\"  Metrics:   {OUTPUT_DIR}/results/test_metrics.json\")\n",
    "print(f\"  Predictions: {OUTPUT_DIR}/results/test_predictions.csv\")\n",
    "print(f\"  Attention: {OUTPUT_DIR}/attention_analysis.npz\")\n",
    "\n",
    "print(f\"Key Results:\")\n",
    "print(f\"  Test MCC:       {mcc:.4f}\")\n",
    "print(f\"  Test F1:        {f1_binary:.4f}\")\n",
    "print(f\"  Test Recall:    {recall_binary:.4f}\")\n",
    "print(f\"  Attention concentration: {(max_entropy - entropy_val.mean()) / max_entropy * 100:.1f}%\")\n",
    "\n",
    "print(\"✅ Unified pipeline with task-optimized attention complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38concept_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
