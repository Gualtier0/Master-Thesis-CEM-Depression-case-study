{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4779fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293c65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "retrieved_path = \"../data/processed/retrieved_noise_dataset.csv\"   # your retrieved posts\n",
    "concepts_path  = \"../data/processed/merged_questionnaires.csv\"     # questionnaire\n",
    "out_path       = \"../data/processed/concat_subject_texts.csv\"\n",
    "# Load data\n",
    "retrieved_df = pd.read_csv(retrieved_path)\n",
    "symptoms_df  = pd.read_csv(concepts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f3a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Clean and normalize IDs ---\n",
    "retrieved_df[\"subject_id\"] = retrieved_df[\"subject_id\"].astype(str)\n",
    "symptoms_df[\"subject_id\"]  = (\n",
    "    symptoms_df[\"Subject\"].astype(str).str.replace(\"^train_\", \"\", regex=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ae2ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged retrieved posts: (9720, 25)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Merge diagnosis & concepts ---\n",
    "concept_cols = [\n",
    "    c for c in symptoms_df.columns if c not in [\"Subject\", \"Diagnosis\", \"subject_id\"]\n",
    "]\n",
    "symptoms_df = symptoms_df[[\"subject_id\", \"Diagnosis\"] + concept_cols]\n",
    "\n",
    "retrieved_df = retrieved_df.merge(\n",
    "    symptoms_df, on=\"subject_id\", how=\"inner\"\n",
    ")\n",
    "print(f\"Merged retrieved posts: {retrieved_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd2311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad172dd8693a4e5ca14bbafb40a375c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b14c437b4b4657af97e814885cff19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba67c9ff10a436bb3a56ef954831f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bfeaacc57542db865c095af2f3567d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 3. Prepare tokenizer (for token-length-aware truncation) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "SEP = tokenizer.sep_token if tokenizer.sep_token else \"[SEP]\"\n",
    "MAX_TOKENS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "146cb14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created concatenated dataset: (486, 24)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Concatenate posts per subject ---\n",
    "def concat_posts(posts, max_tokens=MAX_TOKENS):\n",
    "    \"\"\"\n",
    "    Concatenate a list of post texts into one string separated by [SEP],\n",
    "    truncating when total token length exceeds model limit.\n",
    "    \"\"\"\n",
    "    concatenated = \"\"\n",
    "    total_tokens = 0\n",
    "    for text in posts:\n",
    "        # Predict token count for this post (+1 for SEP)\n",
    "        tokens = len(tokenizer.tokenize(text)) + 1\n",
    "        if total_tokens + tokens > max_tokens - 2:\n",
    "            break\n",
    "        concatenated += text.strip() + f\" {SEP} \"\n",
    "        total_tokens += tokens\n",
    "    return concatenated.strip()\n",
    "\n",
    "grouped = []\n",
    "for sid, group in retrieved_df.groupby(\"subject_id\", sort=False):\n",
    "    # Keep retrieval order (your DF already has top-15 + 5 random in order)\n",
    "    posts = group[\"text\"].astype(str).tolist()\n",
    "    merged_text = concat_posts(posts)\n",
    "    label = group[\"Diagnosis\"].iloc[0]\n",
    "    concepts = group[concept_cols].iloc[0].to_dict()\n",
    "    grouped.append({\"subject_id\": sid, \"text\": merged_text, \"label\": label, **concepts})\n",
    "\n",
    "concat_df = pd.DataFrame(grouped)\n",
    "print(\"Created concatenated dataset:\", concat_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3707129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved concatenated texts to ../data/processed/concat_subject_texts.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Save ---\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "concat_df.to_csv(out_path, index=False)\n",
    "print(f\"Saved concatenated texts to {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38concept_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
