{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CEM Model - Fast Pipeline\n",
    "\n",
    "**Runtime:** ~10-15 minutes (just training)\n",
    "\n",
    "This notebook:\n",
    "1. Loads preprocessed data from `data/processed/whole_pipeline/`\n",
    "2. Trains a Concept Embedding Model (CEM)\n",
    "3. Evaluates and saves results\n",
    "\n",
    "**Prerequisites:** Run `0_prepare_dataset.ipynb` first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from patched_model import PatchedConceptEmbeddingModel\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "DATASET_DIR = os.path.join(DATA_PROCESSED, \"whole_pipeline\")\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameters\nHYPERPARAMS = {\n    # Model architecture\n    \"embedding_dim\": 384,\n    \"n_concepts\": 21,\n    \"n_tasks\": 1,\n    \"emb_size\": 128,\n    \n    # Training\n    \"batch_size_train\": 32,\n    \"batch_size_eval\": 64,\n    \"max_epochs\": 100,\n    \"learning_rate\": 0.01,\n    \"weight_decay\": 4e-05,\n    \n    # Loss weights\n    \"concept_loss_weight\": 1.0,\n    \"training_intervention_prob\": 0.25,\n    \n    # ===== LOSS FUNCTION SELECTION (Enable ONE) =====\n    # LDAM Loss (RECOMMENDED - Best for severe class imbalance)\n    \"use_ldam_loss\": True,       # Enable LDAM Loss\n    \"use_focal_loss\": False,      # Disable Focal Loss\n    \n    # LDAM Loss parameters (only used if use_ldam_loss=True)\n    \"n_positive\": None,           # Will be set after loading data\n    \"n_negative\": None,           # Will be set after loading data\n    \"ldam_max_margin\": 0.5,       # Try: 0.3, 0.5, 0.7, 1.0\n    \"ldam_scale\": 30,             # Try: 20, 30, 40, 50\n    \n    # Focal Loss parameters (only used if use_focal_loss=True)\n    \"focal_loss_alpha\": 0.17,    # Proportion of positive class\n    \"focal_loss_gamma\": 3.0,     # Focusing parameter (2.0-4.0)\n    \n    # Weighted Sampler (batch-level oversampling)\n    \"use_weighted_sampler\": True,  # Enable WeightedRandomSampler\n}\n\nprint(\"✓ Hyperparameters configured\")\nif HYPERPARAMS['use_ldam_loss']:\n    print(f\"  Using LDAM LOSS (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\nelif HYPERPARAMS['use_focal_loss']:\n    print(f\"  Using FOCAL LOSS (alpha={HYPERPARAMS['focal_loss_alpha']}, gamma={HYPERPARAMS['focal_loss_gamma']})\")\nelse:\n    print(f\"  Using standard BCE loss with class weights\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading preprocessed datasets...\")\n",
    "\n",
    "train_data = np.load(os.path.join(DATASET_DIR, \"train_data.npz\"))\n",
    "X_train = train_data['X']\n",
    "C_train = train_data['C']\n",
    "y_train = train_data['y']\n",
    "train_subject_ids = train_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded training data: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "val_data = np.load(os.path.join(DATASET_DIR, \"val_data.npz\"))\n",
    "X_val = val_data['X']\n",
    "C_val = val_data['C']\n",
    "y_val = val_data['y']\n",
    "val_subject_ids = val_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded validation data: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.load(os.path.join(DATASET_DIR, \"test_data.npz\"))\n",
    "X_test = test_data['X']\n",
    "C_test = test_data['C']\n",
    "y_test = test_data['y']\n",
    "test_subject_ids = test_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load class weights\nwith open(os.path.join(DATASET_DIR, \"class_weights.json\"), 'r') as f:\n    class_info = json.load(f)\n\nn_positive = class_info['n_positive']\nn_negative = class_info['n_negative']\npos_weight = class_info['pos_weight']\n\n# Update HYPERPARAMS with actual class counts for LDAM\nHYPERPARAMS['n_positive'] = n_positive\nHYPERPARAMS['n_negative'] = n_negative\n\npos_weight_tensor = torch.tensor([pos_weight], dtype=torch.float32)\n\nprint(f\"✓ Loaded class weights:\")\nprint(f\"  Negative: {n_negative}, Positive: {n_positive}\")\nprint(f\"  Ratio: 1:{pos_weight:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: PyTorch Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import WeightedRandomSampler\n\nclass CEMDataset(Dataset):\n    def __init__(self, X, C, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.C = torch.tensor(C, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx], self.C[idx]\n\n# Create datasets\ntrain_dataset = CEMDataset(X_train, C_train, y_train)\nval_dataset = CEMDataset(X_val, C_val, y_val)\ntest_dataset = CEMDataset(X_test, C_test, y_test)\n\n# Create WeightedRandomSampler for batch-level oversampling (if enabled)\nif HYPERPARAMS['use_weighted_sampler']:\n    # Compute class sample counts\n    class_sample_counts = np.bincount(y_train.astype(int))  # [n_negative, n_positive]\n    weights = 1. / class_sample_counts\n    sample_weights = weights[y_train.astype(int)]\n    \n    # Create sampler\n    train_sampler = WeightedRandomSampler(\n        weights=sample_weights,\n        num_samples=len(sample_weights),\n        replacement=True  # Allow positive samples to appear multiple times\n    )\n    \n    print(f\"✓ WeightedRandomSampler created:\")\n    print(f\"  Negative weight: {weights[0]:.4f}\")\n    print(f\"  Positive weight: {weights[1]:.4f}\")\n    print(f\"  Expected positive ratio per batch: ~{weights[1]/(weights[0]+weights[1]):.1%}\")\n    \n    # Create train loader with sampler (shuffle=False when using sampler)\n    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], sampler=train_sampler)\nelse:\n    # Standard train loader with shuffle\n    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], shuffle=True)\n    print(\"✓ Using standard DataLoader (shuffle=True)\")\n\n# Validation and test loaders (no sampling)\nval_loader = DataLoader(val_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n\nprint(\"✓ All DataLoaders created\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: CEM Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def c_extractor_arch(output_dim):\n    return nn.Sequential(\n        nn.Linear(HYPERPARAMS['embedding_dim'], 256),\n        nn.ReLU(),\n        nn.Dropout(0.3),\n        nn.Linear(256, output_dim or 256)\n    )\n\n# Initialize CEM model with LDAM Loss support\ncem_model = PatchedConceptEmbeddingModel(\n    n_concepts=HYPERPARAMS['n_concepts'],\n    n_tasks=HYPERPARAMS['n_tasks'],\n    input_dim=HYPERPARAMS['embedding_dim'],\n    emb_size=HYPERPARAMS['emb_size'],\n    concept_loss_weight=HYPERPARAMS['concept_loss_weight'],\n    training_intervention_prob=HYPERPARAMS['training_intervention_prob'],\n    c_extractor_arch=c_extractor_arch,\n    learning_rate=HYPERPARAMS['learning_rate'],\n    weight_decay=HYPERPARAMS['weight_decay'],\n    c2y_model=None,\n    task_class_weights=None if (HYPERPARAMS['use_focal_loss'] or HYPERPARAMS['use_ldam_loss']) else pos_weight_tensor,\n    # Focal Loss params\n    use_focal_loss=HYPERPARAMS['use_focal_loss'],\n    focal_loss_alpha=HYPERPARAMS['focal_loss_alpha'],\n    focal_loss_gamma=HYPERPARAMS['focal_loss_gamma'],\n    # LDAM Loss params\n    use_ldam_loss=HYPERPARAMS['use_ldam_loss'],\n    n_positive=HYPERPARAMS['n_positive'],\n    n_negative=HYPERPARAMS['n_negative'],\n    ldam_max_margin=HYPERPARAMS['ldam_max_margin'],\n    ldam_scale=HYPERPARAMS['ldam_scale']\n)\n\nprint(\"✓ CEM model initialized\")\nif HYPERPARAMS['use_ldam_loss']:\n    print(f\"  Using LDAM Loss (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\n    print(f\"  Class counts: {HYPERPARAMS['n_positive']} positive, {HYPERPARAMS['n_negative']} negative\")\nelif HYPERPARAMS['use_focal_loss']:\n    print(f\"  Using Focal Loss (alpha={HYPERPARAMS['focal_loss_alpha']}, gamma={HYPERPARAMS['focal_loss_gamma']})\")\nelse:\n    print(f\"  Using BCE Loss with pos_weight={pos_weight:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(OUTPUT_DIR, \"models\"),\n",
    "    filename=\"cem-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=HYPERPARAMS['max_epochs'],\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    logger=CSVLogger(save_dir=os.path.join(OUTPUT_DIR, \"logs\"), name=\"cem_pipeline\"),\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\\n\")\n",
    "trainer.fit(cem_model, train_loader, val_loader)\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test set\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "cem_model.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "cem_model = cem_model.to(device_obj)\n",
    "\n",
    "y_true_list = []\n",
    "y_prob_list = []\n",
    "concept_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in test_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "        \n",
    "        c_logits, _, y_logits = cem_model(x_batch)\n",
    "        c_probs = torch.sigmoid(c_logits).cpu().numpy()\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "        \n",
    "        y_true_list.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_prob_list.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "        concept_probs_list.extend(c_probs.tolist())\n",
    "\n",
    "y_true = np.array(y_true_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "concept_probs = np.array(concept_probs_list)\n",
    "\n",
    "print(\"✓ Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds to find best for minority class\n",
    "print(\"\\nTesting different decision thresholds...\")\n",
    "print(f\"{'Threshold':<12} {'Recall':<10} {'Precision':<10} {'F1':<10}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    y_pred_temp = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    if np.sum(y_pred_temp) == 0:\n",
    "        continue\n",
    "    \n",
    "    recall = recall_score(y_true, y_pred_temp)\n",
    "    precision = precision_score(y_true, y_pred_temp)\n",
    "    f1 = f1_score(y_true, y_pred_temp)\n",
    "    \n",
    "    print(f\"{threshold:<12.1f} {recall:<10.4f} {precision:<10.4f} {f1:<10.4f}\")\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n✓ Best threshold: {best_threshold:.2f} (F1={best_f1:.4f})\")\n",
    "\n",
    "# Use best threshold for final predictions\n",
    "y_pred = (y_prob >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Results Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute all metrics\ncm = confusion_matrix(y_true, y_pred)\ntn, fp, fn, tp = cm.ravel()\n\nacc = accuracy_score(y_true, y_pred)\nbalanced_acc = balanced_accuracy_score(y_true, y_pred)\nroc_auc = roc_auc_score(y_true, y_prob)\nmcc = matthews_corrcoef(y_true, y_pred)\nf1_binary = f1_score(y_true, y_pred, pos_label=1)\nf1_macro = f1_score(y_true, y_pred, average='macro')\nprecision_binary = precision_score(y_true, y_pred, pos_label=1)\nrecall_binary = recall_score(y_true, y_pred, pos_label=1)\n\n# Print results\nprint(\"\\n\" + \"=\"*70)\nprint(\"                    TEST SET EVALUATION\")\nprint(\"=\"*70)\nprint(f\"\\nDecision Threshold: {best_threshold:.2f}\")\n\n# Enhanced Confusion Matrix Display\nprint(f\"\\n{'CONFUSION MATRIX':^50}\")\nprint(\"=\"*50)\nprint(f\"{'':>20} │ {'Predicted Negative':^12} │ {'Predicted Positive':^12}\")\nprint(\"─\"*50)\nprint(f\"{'Actual Negative':>20} │ {f'TN = {tn}':^12} │ {f'FP = {fp}':^12}\")\nprint(f\"{'Actual Positive':>20} │ {f'FN = {fn}':^12} │ {f'TP = {tp}':^12}\")\nprint(\"=\"*50)\nprint(f\"\\n  True Positives:  {tp:>3}/{int(np.sum(y_true)):<3} ({100*tp/np.sum(y_true):>5.1f}% of depression cases caught)\")\nprint(f\"  False Negatives: {fn:>3}/{int(np.sum(y_true)):<3} ({100*fn/np.sum(y_true):>5.1f}% of depression cases MISSED)\")\nprint(f\"  True Negatives:  {tn:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*tn/(len(y_true)-np.sum(y_true)):>5.1f}% of healthy correctly identified)\")\nprint(f\"  False Positives: {fp:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*fp/(len(y_true)-np.sum(y_true)):>5.1f}% false alarms)\")\n\nprint(f\"\\nPerformance Metrics:\")\nprint(f\"  Accuracy:                  {acc:.4f}\")\nprint(f\"  Balanced Accuracy:         {balanced_acc:.4f}\")\nprint(f\"  ROC-AUC:                   {roc_auc:.4f}\")\nprint(f\"  Matthews Correlation:      {mcc:.4f}\")\nprint(f\"\\n  F1 Score (Binary):         {f1_binary:.4f}\")\nprint(f\"  F1 Score (Macro):          {f1_macro:.4f}\")\nprint(f\"  Precision (Binary):        {precision_binary:.4f}\")\nprint(f\"  Recall (Binary):           {recall_binary:.4f}\")\n\nprint(\"\\n\" + classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "metrics_dict = {\n",
    "    \"threshold\": float(best_threshold),\n",
    "    \"n_samples\": int(len(y_true)),\n",
    "    \"n_positive\": int(np.sum(y_true)),\n",
    "    \"n_negative\": int(len(y_true) - np.sum(y_true)),\n",
    "    \"accuracy\": float(acc),\n",
    "    \"balanced_accuracy\": float(balanced_acc),\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "    \"mcc\": float(mcc),\n",
    "    \"f1_binary\": float(f1_binary),\n",
    "    \"f1_macro\": float(f1_macro),\n",
    "    \"precision_binary\": float(precision_binary),\n",
    "    \"recall_binary\": float(recall_binary),\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)}\n",
    "}\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"results\"), exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"results/test_metrics.json\"), 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'subject_id': test_subject_ids,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'y_prob': y_prob\n",
    "})\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    predictions_df[concept_name] = concept_probs[:, i]\n",
    "\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_DIR, \"results/test_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"✓ Results saved to {OUTPUT_DIR}/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"              CEM TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  Model checkpoint: {OUTPUT_DIR}/models/\")\n",
    "print(f\"  Metrics JSON:     {OUTPUT_DIR}/results/test_metrics.json\")\n",
    "print(f\"  Predictions CSV:  {OUTPUT_DIR}/results/test_predictions.csv\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}