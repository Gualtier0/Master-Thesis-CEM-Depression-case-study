{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete CEM Pipeline - End-to-End Depression Detection\n",
    "\n",
    "This notebook implements a complete pipeline for:\n",
    "1. Loading and processing training/test data\n",
    "2. Retrieving top-20 concept-relevant posts per subject\n",
    "3. Averaging post embeddings into single vectors\n",
    "4. Training a Concept Embedding Model (CEM)\n",
    "5. Evaluating with detailed metrics and concept probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.special import expit  # sigmoid\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from patched_model import PatchedConceptEmbeddingModel\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using MacBook GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Detect device (MPS/CUDA/CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU (will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configured\n",
      "  Project root: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study\n",
      "  Positive dir: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/raw/train/positive_examples_anonymous_chunks\n",
      "  Negative dir: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/raw/train/negative_examples_anonymous_chunks\n",
      "  Output dir: outputs\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_RAW = os.path.join(PROJECT_ROOT, \"data/raw\")\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "# Training data paths (in train/ subdirectory)\n",
    "POS_DIR = os.path.join(DATA_RAW, \"train/positive_examples_anonymous_chunks\")\n",
    "NEG_DIR = os.path.join(DATA_RAW, \"train/negative_examples_anonymous_chunks\")\n",
    "\n",
    "# Test data paths\n",
    "TEST_DIR = os.path.join(DATA_RAW, \"test\")\n",
    "TEST_LABELS = os.path.join(TEST_DIR, \"test_golden_truth.txt\")\n",
    "\n",
    "# Concept labels\n",
    "CONCEPTS_FILE = os.path.join(DATA_PROCESSED, \"merged_questionnaires.csv\")\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  Positive dir: {POS_DIR}\")\n",
    "print(f\"  Negative dir: {NEG_DIR}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined 21 BDI-II concepts\n"
     ]
    }
   ],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters configured:\n",
      "  k_posts: 20\n",
      "  sbert_model: all-MiniLM-L6-v2\n",
      "  embedding_dim: 384\n",
      "  n_concepts: 21\n",
      "  n_tasks: 1\n",
      "  emb_size: 128\n",
      "  batch_size_train: 32\n",
      "  batch_size_eval: 64\n",
      "  max_epochs: 100\n",
      "  learning_rate: 0.01\n",
      "  weight_decay: 4e-05\n",
      "  concept_loss_weight: 1.0\n",
      "  training_intervention_prob: 0.25\n",
      "  use_focal_loss: False\n",
      "  focal_loss_alpha: 0.17\n",
      "  focal_loss_gamma: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    # Data\n",
    "    \"k_posts\": 20,              # Top-k posts per subject\n",
    "    \"sbert_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"embedding_dim\": 384,       # SBERT embedding dimension\n",
    "    \n",
    "    # Model architecture\n",
    "    \"n_concepts\": 21,\n",
    "    \"n_tasks\": 1,\n",
    "    \"emb_size\": 128,\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size_train\": 32,\n",
    "    \"batch_size_eval\": 64,\n",
    "    \"max_epochs\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 4e-05,\n",
    "    \n",
    "    # Loss weights\n",
    "    \"concept_loss_weight\": 1.0,\n",
    "    \"training_intervention_prob\": 0.25,\n",
    "    \n",
    "    # Focal Loss (set use_focal_loss=True to enable)\n",
    "    \"use_focal_loss\": False,     # Set to True to use Focal Loss instead of BCE\n",
    "    \"focal_loss_alpha\": 0.17,    # Proportion of positive class (83/486 ≈ 0.17)\n",
    "    \"focal_loss_gamma\": 2.0,     # Focusing parameter (higher = more focus on hard examples)\n",
    "}\n",
    "\n",
    "print(\"✓ Hyperparameters configured:\")\n",
    "for k, v in HYPERPARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Training Data\n",
    "\n",
    "Extract 486 training subjects with posts and concept labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for XML parsing\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing null chars and extra whitespace.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\u0000\", \"\")\n",
    "    text = WHITESPACE_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_posts_from_xml(xml_path, min_chars=10):\n",
    "    \"\"\"Extract posts from a single XML file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Failed to parse {xml_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    posts = []\n",
    "    for writing in root.findall(\"WRITING\"):\n",
    "        title = writing.findtext(\"TITLE\") or \"\"\n",
    "        text = writing.findtext(\"TEXT\") or \"\"\n",
    "        \n",
    "        combined = normalize_text(f\"{title} {text}\".strip())\n",
    "        if len(combined) >= min_chars:\n",
    "            posts.append(combined)\n",
    "    \n",
    "    return posts\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "  Processing positive examples...\n",
      "  Loaded 29868 posts from positive subjects\n",
      "  Processing negative examples...\n",
      "\n",
      "✓ Loaded training data in 2.8s\n",
      "  Total posts: 286,740\n",
      "  Unique subjects: 486\n",
      "  Label distribution:\n",
      "label\n",
      "0    403\n",
      "1     83\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Parse training XML files\n",
    "print(\"Loading training data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_data = []\n",
    "\n",
    "# Process positive examples\n",
    "print(\"  Processing positive examples...\")\n",
    "pos_files = glob.glob(os.path.join(POS_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in pos_files:\n",
    "    filename = os.path.basename(xml_file)\n",
    "    # Extract subject_id (e.g., train_subject1095_1.xml -> subject1095)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 1,  # Positive (depression)\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "print(f\"  Loaded {len([d for d in train_data if d['label']==1])} posts from positive subjects\")\n",
    "\n",
    "# Process negative examples\n",
    "print(\"  Processing negative examples...\")\n",
    "neg_files = glob.glob(os.path.join(NEG_DIR, \"**\", \"*.xml\"), recursive=True)\n",
    "for xml_file in neg_files:\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"train_(subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        posts = extract_posts_from_xml(xml_file)\n",
    "        for post in posts:\n",
    "            train_data.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"label\": 0,  # Negative (control)\n",
    "                \"text\": post\n",
    "            })\n",
    "\n",
    "train_posts_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"\\n✓ Loaded training data in {time.time()-start_time:.1f}s\")\n",
    "print(f\"  Total posts: {len(train_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {train_posts_df['subject_id'].nunique()}\")\n",
    "print(f\"  Label distribution:\")\n",
    "print(train_posts_df.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading concept labels...\n",
      "✓ Loaded concept labels for 486 subjects\n",
      "  Concept columns: 21\n",
      "  First few subjects:\n",
      "    subject_id  Diagnosis  Sadness  Pessimism  Past failure\n",
      "0  subject4550          0        0          0             0\n",
      "1  subject4181          0        0          0             0\n",
      "2  subject8202          0        0          0             0\n",
      "3  subject6783          0        0          0             0\n",
      "4  subject1642          0        0          0             0\n"
     ]
    }
   ],
   "source": [
    "# Load concept labels from questionnaires\n",
    "print(\"Loading concept labels...\")\n",
    "\n",
    "concepts_df = pd.read_csv(CONCEPTS_FILE)\n",
    "# Clean subject_id (remove \"train_\" prefix)\n",
    "concepts_df[\"subject_id\"] = concepts_df[\"Subject\"].str.replace(\"train_\", \"\", regex=True)\n",
    "\n",
    "# Binarize concept values (any value > 0 becomes 1)\n",
    "concept_cols = [col for col in concepts_df.columns if col in CONCEPT_NAMES]\n",
    "for col in concept_cols:\n",
    "    concepts_df[col] = (concepts_df[col] > 0).astype(int)\n",
    "\n",
    "print(f\"✓ Loaded concept labels for {len(concepts_df)} subjects\")\n",
    "print(f\"  Concept columns: {len(concept_cols)}\")\n",
    "print(f\"  First few subjects:\")\n",
    "print(concepts_df[[\"subject_id\", \"Diagnosis\"] + concept_cols[:3]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Load Test Data\n",
    "\n",
    "Extract 401 test subjects and split into validation (200) and test (201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test data...\n",
      "  Temp directory: /var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000gn/T/test_chunks_804t3r5a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted chunk 3/10\n",
      "  Extracted chunk 6/10\n",
      "  Extracted chunk 9/10\n",
      "✓ Test data extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract test ZIP files to temporary directory\n",
    "print(\"Extracting test data...\")\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"test_chunks_\")\n",
    "print(f\"  Temp directory: {temp_dir}\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    zip_path = os.path.join(TEST_DIR, f\"chunk {i}.zip\")\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(temp_dir, f\"chunk_{i}\"))\n",
    "        if i % 3 == 0:\n",
    "            print(f\"  Extracted chunk {i}/10\")\n",
    "    else:\n",
    "        print(f\"  WARNING: {zip_path} not found\")\n",
    "\n",
    "print(\"✓ Test data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test labels for 401 subjects\n",
      "  Label distribution:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load test labels\n",
    "test_labels_df = pd.read_csv(TEST_LABELS, sep='\\t', header=None, names=['subject_id', 'label'])\n",
    "test_labels_df['subject_id'] = test_labels_df['subject_id'].str.strip()\n",
    "\n",
    "print(f\"✓ Loaded test labels for {len(test_labels_df)} subjects\")\n",
    "print(f\"  Label distribution:\")\n",
    "print(test_labels_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test posts...\n",
      "  Found 4010 XML files\n",
      "✓ Loaded test posts\n",
      "  Total posts: 229,746\n",
      "  Unique subjects: 401\n"
     ]
    }
   ],
   "source": [
    "# Parse test XML files\n",
    "print(\"Loading test posts...\")\n",
    "test_data = []\n",
    "\n",
    "test_xml_files = glob.glob(os.path.join(temp_dir, \"**\", \"*.xml\"), recursive=True)\n",
    "print(f\"  Found {len(test_xml_files)} XML files\")\n",
    "\n",
    "for xml_file in test_xml_files:\n",
    "    filename = os.path.basename(xml_file)\n",
    "    match = re.match(r\"(test_subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        # Get label from test_labels_df\n",
    "        label_row = test_labels_df[test_labels_df['subject_id'] == subject_id]\n",
    "        if len(label_row) > 0:\n",
    "            label = label_row.iloc[0]['label']\n",
    "            posts = extract_posts_from_xml(xml_file)\n",
    "            for post in posts:\n",
    "                test_data.append({\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"label\": label,\n",
    "                    \"text\": post\n",
    "                })\n",
    "\n",
    "test_posts_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"✓ Loaded test posts\")\n",
    "print(f\"  Total posts: {len(test_posts_df):,}\")\n",
    "print(f\"  Unique subjects: {test_posts_df['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting test data into validation and test...\n",
      "✓ Split complete\n",
      "  Validation: 200 subjects, 116544 posts\n",
      "  Test: 201 subjects, 113202 posts\n",
      "\n",
      "  Validation label distribution:\n",
      "label\n",
      "0    174\n",
      "1     26\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "  Test label distribution:\n",
      "label\n",
      "0    175\n",
      "1     26\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split test data into validation and test sets (stratified 50/50)\n",
    "print(\"Splitting test data into validation and test...\")\n",
    "\n",
    "# Get unique subjects with labels\n",
    "test_subjects = test_posts_df.groupby('subject_id')['label'].first().reset_index()\n",
    "\n",
    "# Stratified split\n",
    "val_subjects, test_subjects_final = train_test_split(\n",
    "    test_subjects['subject_id'],\n",
    "    test_size=0.5,\n",
    "    stratify=test_subjects['label'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Create validation and test dataframes\n",
    "val_posts_df = test_posts_df[test_posts_df['subject_id'].isin(val_subjects)].copy()\n",
    "test_posts_df_final = test_posts_df[test_posts_df['subject_id'].isin(test_subjects_final)].copy()\n",
    "\n",
    "print(f\"✓ Split complete\")\n",
    "print(f\"  Validation: {val_posts_df['subject_id'].nunique()} subjects, {len(val_posts_df)} posts\")\n",
    "print(f\"  Test: {test_posts_df_final['subject_id'].nunique()} subjects, {len(test_posts_df_final)} posts\")\n",
    "print(f\"\\n  Validation label distribution:\")\n",
    "print(val_posts_df.groupby('label')['subject_id'].nunique())\n",
    "print(f\"\\n  Test label distribution:\")\n",
    "print(test_posts_df_final.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: SBERT Setup & Concept Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model: all-MiniLM-L6-v2\n",
      "✓ SBERT model loaded on mps\n",
      "  Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Load SBERT model\n",
    "print(f\"Loading SBERT model: {HYPERPARAMS['sbert_model']}\")\n",
    "sbert_model = SentenceTransformer(HYPERPARAMS['sbert_model'])\n",
    "sbert_model = sbert_model.to(DEVICE)\n",
    "\n",
    "print(f\"✓ SBERT model loaded on {DEVICE}\")\n",
    "print(f\"  Embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for 21 concepts...\n",
      "✓ Concept embeddings created\n",
      "  Shape: torch.Size([21, 384])\n"
     ]
    }
   ],
   "source": [
    "# Create concept embeddings\n",
    "print(f\"Creating embeddings for {N_CONCEPTS} concepts...\")\n",
    "concept_embeddings = sbert_model.encode(\n",
    "    CONCEPT_NAMES,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "print(f\"✓ Concept embeddings created\")\n",
    "print(f\"  Shape: {concept_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Post Retrieval (Top-20 per Subject)\n",
    "\n",
    "Select the 20 most concept-relevant posts for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Post retrieval function defined\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k_posts(subject_id, posts_df, concept_embs, sbert, k=20):\n",
    "    \"\"\"\n",
    "    Retrieve top-k posts for a subject based on concept similarity.\n",
    "    \n",
    "    Returns:\n",
    "        List of k selected post texts\n",
    "    \"\"\"\n",
    "    # Get subject's posts\n",
    "    subj_posts = posts_df[posts_df['subject_id'] == subject_id]['text'].tolist()\n",
    "    \n",
    "    if len(subj_posts) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Handle subjects with fewer than k posts\n",
    "    if len(subj_posts) <= k:\n",
    "        if len(subj_posts) < k:\n",
    "            # Pad with random duplicates\n",
    "            extra_needed = k - len(subj_posts)\n",
    "            padding = list(np.random.choice(subj_posts, size=extra_needed, replace=True))\n",
    "            return subj_posts + padding\n",
    "        else:\n",
    "            return subj_posts\n",
    "    \n",
    "    # Encode all subject's posts\n",
    "    post_embeddings = sbert.encode(\n",
    "        subj_posts,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Compute cosine similarity: (num_posts, num_concepts)\n",
    "    cos_scores = util.cos_sim(post_embeddings, concept_embs)\n",
    "    \n",
    "    # For each post, take max similarity across all concepts\n",
    "    max_sim_scores = cos_scores.max(dim=1).values.cpu().numpy()\n",
    "    \n",
    "    # Select top-k posts\n",
    "    top_k_indices = np.argpartition(-max_sim_scores, range(min(k, len(subj_posts))))[:k]\n",
    "    \n",
    "    selected_posts = [subj_posts[i] for i in top_k_indices]\n",
    "    \n",
    "    return selected_posts\n",
    "\n",
    "print(\"✓ Post retrieval function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top-20 posts for all subjects...\n",
      "  Processing training subjects...\n",
      "    Processed 100/486 subjects\n",
      "    Processed 200/486 subjects\n",
      "    Processed 300/486 subjects\n",
      "    Processed 400/486 subjects\n",
      "  Processing validation subjects...\n",
      "    Processed 50/200 subjects\n",
      "    Processed 100/200 subjects\n",
      "    Processed 150/200 subjects\n",
      "    Processed 200/200 subjects\n",
      "  Processing test subjects...\n",
      "    Processed 50/201 subjects\n",
      "    Processed 100/201 subjects\n",
      "    Processed 150/201 subjects\n",
      "    Processed 200/201 subjects\n",
      "\n",
      "✓ Post retrieval complete in 2327.5s\n",
      "  Train: 486 subjects x 20 posts\n",
      "  Val: 200 subjects x 20 posts\n",
      "  Test: 201 subjects x 20 posts\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top-k posts for all subjects\n",
    "print(f\"Retrieving top-{HYPERPARAMS['k_posts']} posts for all subjects...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Training subjects\n",
    "print(\"  Processing training subjects...\")\n",
    "train_selected = {}\n",
    "train_subjects = train_posts_df['subject_id'].unique()\n",
    "for idx, subject_id in enumerate(train_subjects):\n",
    "    selected = retrieve_top_k_posts(\n",
    "        subject_id,\n",
    "        train_posts_df,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts']\n",
    "    )\n",
    "    train_selected[subject_id] = selected\n",
    "    \n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"    Processed {idx + 1}/{len(train_subjects)} subjects\")\n",
    "\n",
    "# Validation subjects\n",
    "print(\"  Processing validation subjects...\")\n",
    "val_selected = {}\n",
    "val_subjects = val_posts_df['subject_id'].unique()\n",
    "for idx, subject_id in enumerate(val_subjects):\n",
    "    selected = retrieve_top_k_posts(\n",
    "        subject_id,\n",
    "        val_posts_df,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts']\n",
    "    )\n",
    "    val_selected[subject_id] = selected\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"    Processed {idx + 1}/{len(val_subjects)} subjects\")\n",
    "\n",
    "# Test subjects\n",
    "print(\"  Processing test subjects...\")\n",
    "test_selected = {}\n",
    "test_subjects = test_posts_df_final['subject_id'].unique()\n",
    "for idx, subject_id in enumerate(test_subjects):\n",
    "    selected = retrieve_top_k_posts(\n",
    "        subject_id,\n",
    "        test_posts_df_final,\n",
    "        concept_embeddings,\n",
    "        sbert_model,\n",
    "        k=HYPERPARAMS['k_posts']\n",
    "    )\n",
    "    test_selected[subject_id] = selected\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"    Processed {idx + 1}/{len(test_subjects)} subjects\")\n",
    "\n",
    "print(f\"\\n✓ Post retrieval complete in {time.time()-start_time:.1f}s\")\n",
    "print(f\"  Train: {len(train_selected)} subjects x {HYPERPARAMS['k_posts']} posts\")\n",
    "print(f\"  Val: {len(val_selected)} subjects x {HYPERPARAMS['k_posts']} posts\")\n",
    "print(f\"  Test: {len(test_selected)} subjects x {HYPERPARAMS['k_posts']} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Embedding Aggregation (Simple Averaging)\n",
    "\n",
    "Encode the selected posts and average them into single embeddings per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoding and averaging function defined\n"
     ]
    }
   ],
   "source": [
    "def encode_and_average(selected_posts_dict, sbert, batch_size=64):\n",
    "    \"\"\"\n",
    "    Encode selected posts and average them per subject.\n",
    "    \n",
    "    Args:\n",
    "        selected_posts_dict: {subject_id: [post1, post2, ...], ...}\n",
    "    \n",
    "    Returns:\n",
    "        averaged_embeddings: (n_subjects, embedding_dim)\n",
    "        subject_ids: list of subject_ids in same order\n",
    "    \"\"\"\n",
    "    subject_ids = list(selected_posts_dict.keys())\n",
    "    averaged_embeddings = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        posts = selected_posts_dict[subject_id]\n",
    "        \n",
    "        # Encode all posts for this subject\n",
    "        post_embs = sbert.encode(\n",
    "            posts,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Average across posts (axis=0)\n",
    "        avg_emb = np.mean(post_embs, axis=0)\n",
    "        averaged_embeddings.append(avg_emb)\n",
    "    \n",
    "    return np.array(averaged_embeddings), subject_ids\n",
    "\n",
    "print(\"✓ Encoding and averaging function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding and averaging embeddings...\n",
      "  Training set...\n",
      "    X_train shape: (486, 384)\n",
      "  Validation set...\n",
      "    X_val shape: (200, 384)\n",
      "  Test set...\n",
      "    X_test shape: (201, 384)\n",
      "\n",
      "✓ Encoding complete in 502.0s\n"
     ]
    }
   ],
   "source": [
    "# Encode and average for all splits\n",
    "print(\"Encoding and averaging embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"  Training set...\")\n",
    "X_train, train_subject_ids = encode_and_average(train_selected, sbert_model)\n",
    "print(f\"    X_train shape: {X_train.shape}\")\n",
    "\n",
    "print(\"  Validation set...\")\n",
    "X_val, val_subject_ids = encode_and_average(val_selected, sbert_model)\n",
    "print(f\"    X_val shape: {X_val.shape}\")\n",
    "\n",
    "print(\"  Test set...\")\n",
    "X_test, test_subject_ids = encode_and_average(test_selected, sbert_model)\n",
    "print(f\"    X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\n✓ Encoding complete in {time.time()-start_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building concept matrices and labels...\n",
      "✓ Matrices built\n",
      "  Train: X=(486, 384), C=(486, 21), y=(486,)\n",
      "  Val:   X=(200, 384), C=(200, 21), y=(200,)\n",
      "  Test:  X=(201, 384), C=(201, 21), y=(201,)\n",
      "\n",
      "  Training label distribution: [403  83]\n",
      "  Validation label distribution: [174  26]\n",
      "  Test label distribution: [175  26]\n"
     ]
    }
   ],
   "source": [
    "# Build concept matrices and label vectors\n",
    "print(\"Building concept matrices and labels...\")\n",
    "\n",
    "# Training: get concepts from questionnaires\n",
    "C_train = []\n",
    "y_train = []\n",
    "for subject_id in train_subject_ids:\n",
    "    # Get label\n",
    "    label = train_posts_df[train_posts_df['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_train.append(label)\n",
    "    \n",
    "    # Get concepts from questionnaire\n",
    "    concept_row = concepts_df[concepts_df['subject_id'] == subject_id]\n",
    "    if len(concept_row) > 0:\n",
    "        concepts = concept_row[concept_cols].values[0]\n",
    "    else:\n",
    "        # If missing, use zeros\n",
    "        concepts = np.zeros(N_CONCEPTS)\n",
    "    C_train.append(concepts)\n",
    "\n",
    "C_train = np.array(C_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "# Validation: zeros for concepts (no ground truth)\n",
    "C_val = np.zeros((len(val_subject_ids), N_CONCEPTS), dtype=np.float32)\n",
    "y_val = []\n",
    "for subject_id in val_subject_ids:\n",
    "    label = val_posts_df[val_posts_df['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_val.append(label)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "\n",
    "# Test: zeros for concepts\n",
    "C_test = np.zeros((len(test_subject_ids), N_CONCEPTS), dtype=np.float32)\n",
    "y_test = []\n",
    "for subject_id in test_subject_ids:\n",
    "    label = test_posts_df_final[test_posts_df_final['subject_id'] == subject_id]['label'].iloc[0]\n",
    "    y_test.append(label)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "print(\"✓ Matrices built\")\n",
    "print(f\"  Train: X={X_train.shape}, C={C_train.shape}, y={y_train.shape}\")\n",
    "print(f\"  Val:   X={X_val.shape}, C={C_val.shape}, y={y_val.shape}\")\n",
    "print(f\"  Test:  X={X_test.shape}, C={C_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\n  Training label distribution: {np.bincount(y_train.astype(int))}\")\n",
    "print(f\"  Validation label distribution: {np.bincount(y_val.astype(int))}\")\n",
    "print(f\"  Test label distribution: {np.bincount(y_test.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Handling\n",
    "\n",
    "Compute class weights to address the imbalance (403 negative vs 83 positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance:\n",
      "  Negative samples: 403\n",
      "  Positive samples: 83\n",
      "  Ratio: 1:4.86\n",
      "  Computed pos_weight: 4.8554\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "n_negative = int(np.sum(y_train == 0))\n",
    "n_positive = int(np.sum(y_train == 1))\n",
    "pos_weight = n_negative / n_positive\n",
    "\n",
    "# Convert to tensor\n",
    "pos_weight_tensor = torch.tensor([pos_weight], dtype=torch.float32)\n",
    "\n",
    "print(f\"Class imbalance:\")\n",
    "print(f\"  Negative samples: {n_negative}\")\n",
    "print(f\"  Positive samples: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")\n",
    "print(f\"  Computed pos_weight: {pos_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Datasets\n",
    "\n",
    "Save the processed datasets to disk for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets...\n",
      "✓ Datasets saved to /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/whole_pipeline\n",
      "  train_data.npz: 486 samples\n",
      "  val_data.npz:   200 samples\n",
      "  test_data.npz:  201 samples\n",
      "  class_weights.json\n",
      "\n",
      "To load later:\n",
      "  train = np.load(os.path.join(DATA_PROCESSED, 'whole_pipeline/train_data.npz'))\n",
      "  X_train = train['X']\n",
      "  C_train = train['C']\n",
      "  y_train = train['y']\n",
      "  train_subject_ids = train['subject_ids']\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets to disk for later use\n",
    "SAVE_DIR = os.path.join(DATA_PROCESSED, \"whole_pipeline\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Saving datasets...\")\n",
    "\n",
    "# Save numpy arrays\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"train_data.npz\"),\n",
    "    X=X_train,\n",
    "    C=C_train,\n",
    "    y=y_train,\n",
    "    subject_ids=np.array(train_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"val_data.npz\"),\n",
    "    X=X_val,\n",
    "    C=C_val,\n",
    "    y=y_val,\n",
    "    subject_ids=np.array(val_subject_ids)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    os.path.join(SAVE_DIR, \"test_data.npz\"),\n",
    "    X=X_test,\n",
    "    C=C_test,\n",
    "    y=y_test,\n",
    "    subject_ids=np.array(test_subject_ids)\n",
    ")\n",
    "\n",
    "# Save class weights info\n",
    "class_info = {\n",
    "    \"n_positive\": n_positive,\n",
    "    \"n_negative\": n_negative,\n",
    "    \"pos_weight\": float(pos_weight)\n",
    "}\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"class_weights.json\"), 'w') as f:\n",
    "    json.dump(class_info, f, indent=4)\n",
    "\n",
    "print(f\"✓ Datasets saved to {SAVE_DIR}\")\n",
    "print(f\"  train_data.npz: {X_train.shape[0]} samples\")\n",
    "print(f\"  val_data.npz:   {X_val.shape[0]} samples\")\n",
    "print(f\"  test_data.npz:  {X_test.shape[0]} samples\")\n",
    "print(f\"  class_weights.json\")\n",
    "print(\"\\nTo load later:\")\n",
    "print(\"  train = np.load(os.path.join(DATA_PROCESSED, 'whole_pipeline/train_data.npz'))\")\n",
    "print(\"  X_train = train['X']\")\n",
    "print(\"  C_train = train['C']\")\n",
    "print(\"  y_train = train['y']\")\n",
    "print(\"  train_subject_ids = train['subject_ids']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: PyTorch Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CEMDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class CEMDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for CEM model.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, C, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.C = torch.tensor(C, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.C[idx]\n",
    "\n",
    "print(\"✓ CEMDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Datasets created\n",
      "  Train: 486 samples\n",
      "  Val: 200 samples\n",
      "  Test: 201 samples\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = CEMDataset(X_train, C_train, y_train)\n",
    "val_dataset = CEMDataset(X_val, C_val, y_val)\n",
    "test_dataset = CEMDataset(X_test, C_test, y_test)\n",
    "\n",
    "print(\"✓ Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoaders created\n",
      "  Train batches: 16\n",
      "  Val batches: 4\n",
      "  Test batches: 4\n",
      "\n",
      "  Sample batch shapes:\n",
      "    X: torch.Size([32, 384])\n",
      "    y: torch.Size([32])\n",
      "    C: torch.Size([32, 21])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=HYPERPARAMS['batch_size_train'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=HYPERPARAMS['batch_size_eval'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=HYPERPARAMS['batch_size_eval'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"✓ DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test batch\n",
    "x_batch, y_batch, c_batch = next(iter(train_loader))\n",
    "print(f\"\\n  Sample batch shapes:\")\n",
    "print(f\"    X: {x_batch.shape}\")\n",
    "print(f\"    y: {y_batch.shape}\")\n",
    "print(f\"    C: {c_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: CEM Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Concept extractor architecture defined\n"
     ]
    }
   ],
   "source": [
    "def c_extractor_arch(output_dim):\n",
    "    \"\"\"Concept extractor architecture.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(HYPERPARAMS['embedding_dim'], 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, output_dim or 256)\n",
    "    )\n",
    "\n",
    "print(\"✓ Concept extractor architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CEM model initialized\n",
      "  Using BCE Loss with pos_weight=4.8554\n",
      "PatchedConceptEmbeddingModel(\n",
      "  (pre_concept_model): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (concept_context_generators): ModuleList(\n",
      "    (0-20): 21 x Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (concept_prob_generators): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (c2y_model): Sequential(\n",
      "    (0): Linear(in_features=2688, out_features=1, bias=True)\n",
      "  )\n",
      "  (loss_concept): BCEWithLogitsLoss()\n",
      "  (loss_task): BCEWithLogitsLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize CEM model\n",
    "cem_model = PatchedConceptEmbeddingModel(\n",
    "    n_concepts=HYPERPARAMS['n_concepts'],\n",
    "    n_tasks=HYPERPARAMS['n_tasks'],\n",
    "    input_dim=HYPERPARAMS['embedding_dim'],\n",
    "    emb_size=HYPERPARAMS['emb_size'],\n",
    "    concept_loss_weight=HYPERPARAMS['concept_loss_weight'],\n",
    "    training_intervention_prob=HYPERPARAMS['training_intervention_prob'],\n",
    "    c_extractor_arch=c_extractor_arch,\n",
    "    learning_rate=HYPERPARAMS['learning_rate'],\n",
    "    weight_decay=HYPERPARAMS['weight_decay'],\n",
    "    c2y_model=None,\n",
    "    task_class_weights=pos_weight_tensor,  # Use class weights for imbalanced data\n",
    "    use_focal_loss=HYPERPARAMS['use_focal_loss'],\n",
    "    focal_loss_alpha=HYPERPARAMS['focal_loss_alpha'],\n",
    "    focal_loss_gamma=HYPERPARAMS['focal_loss_gamma']\n",
    ")\n",
    "\n",
    "print(\"✓ CEM model initialized\")\n",
    "if HYPERPARAMS['use_focal_loss']:\n",
    "    print(f\"  Using Focal Loss (alpha={HYPERPARAMS['focal_loss_alpha']}, gamma={HYPERPARAMS['focal_loss_gamma']})\")\n",
    "else:\n",
    "    print(f\"  Using BCE Loss with pos_weight={pos_weight:.4f}\")\n",
    "print(cem_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer configured\n",
      "  Device: mps\n",
      "  Max epochs: 100\n"
     ]
    }
   ],
   "source": [
    "# Setup trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(OUTPUT_DIR, \"models\"),\n",
    "    filename=\"cem-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=HYPERPARAMS['max_epochs'],\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    logger=CSVLogger(save_dir=os.path.join(OUTPUT_DIR, \"logs\"), name=\"cem_pipeline\"),\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Max epochs: {HYPERPARAMS['max_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory outputs/models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name                       | Type              | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | pre_concept_model          | Sequential        | 164 K \n",
      "1 | concept_context_generators | ModuleList        | 1.4 M \n",
      "2 | concept_prob_generators    | ModuleList        | 257   \n",
      "3 | c2y_model                  | Sequential        | 2.7 K \n",
      "4 | loss_concept               | BCEWithLogitsLoss | 0     \n",
      "5 | loss_task                  | BCEWithLogitsLoss | 0     \n",
      "-----------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.196     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d4118f04154a679933d487762b34bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920d324e51ff4062b4dfa8eb0d795246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2b46a1d51145adaa449ad5a0f01f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b427a84ab04b46add28750ebf21d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919aa3ee52a44d2e879e1b87ee13d57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11826a5309024614b404243b11224119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e1f6c6b38841629148e7dc7668a50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d66e80674164bf0a689944b8fe95e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90592d73090b4a58baeffaa83f2151e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e604bfb27d5e4cbdb827dc90b66d4070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf896a541884e30922d35cb9850c517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e03c8652e5415baf1fb08877072ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d14d3a633e4cd69bc1242d0c62260a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df95f79a6034017a4d6d9750d2bc055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62eb7e9edff46babb069223d6715f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e7171fe9564b548b14d8cc249ac6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f9fc683ff9469f8dd5d6cc7df38e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7a49a282d94e88a17d1b00f9cc6c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0931f501de06473abb34c8b102c986aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab54bd1f81946c6bc2e85b2b4df5b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37ccca5b5af444ab99d76315e7fb7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38430f6b8cb74cd1a70eaa2f1ab9fd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616d47f54d314ad7a290ab988971fffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08bed08947443019e78cc3238e86bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8c48491c2548a18e66d4ba2600ce22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0426a8161c4d9098b63740186763b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d15e73189b4e53816ccf3a9db4649e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed33ef6183a4eba8def16b7a601483e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab90292a893f494193ccf3ca9a1615fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6357c6442a4f43a489f91ca4b1ace8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d677ba3878f47048153413df39d2003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18bbebe2d8b41d28f777f9ce8204cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f986aed092497c9732e5356b859ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403f715561444fce997143b52f1a606d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097496612eca4c4a9a6403768fd931ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000f55b0f87748a0ba3842c9a968d83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9098dc8d9294bb3ab1b13d7b01f4e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553ecc90f5b64dcd826e5d67923c60f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff79166a5fe64c7d8450c799d1d73ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0466ee5738a54fcb83dabb1e758448cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5915d8741e4a433aa2c608a507e640b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f779e5eb65453f95620e3aca31f913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e396d0d8f846b9a30ec6b3548ae0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b7d3aa59d9462ca543bbf1040dd27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdb5f2b7b784133af999849b87b6326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee10a0ff1604335998e9416d84196e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e452fb19774cdc8e626a905b49863e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90499e25413b4ab5baf08e898a10eebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c6f00013564199b453c3d291041097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ba1e71aa6b47ebb70147f3169c2a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b73010cb7684ece9173ea8917920fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db579a8215df458ab5166ba118cd9c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f22795c899c4f02ad28a1cb18779043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69e87be1705425d954f3fd174309414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fc40f5ccab437ca99edd484d03ecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f018681206354326ac9d19bb1d0f7c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864dceb0ab574771a1028d8ea44c1c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c45de3c634c415b9531b5798f78ffd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694e676cc66c43858609dcf78a8985e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba90f3a09634b0eb126aef3f8ad237c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fbe3c017a14da3a5442def85a1b4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e545811f454f4f85ef954cc2ccf19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e08994b3d244b3fb1a760db04495999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181e5445e99943248af3a8d853442af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db280be48126468bac615a89ee9e25b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f8e18e74d34b60961429a22873be8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0773c4452904dc6a46b82f4019ecff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca493d7ebdc5442dbec34a79d60da0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4263c7ca2a45ee8c56a1666314a2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d76eea8ab87467cbe2aa1b8dc4b48f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf945dba944ce993edb84767ef4f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c3790997714a218b0c2bb3c3150042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e82b49f82694282bf9a1fe30f9cb319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95ca2b2bbe64f31ba7ed3fea5250758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835b37357a1543ea9f8f300c6a9d9bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1fa66a90c3411882b1625f431361c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea906fa97014b528dc8692dfa17cdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8c83d74d18474baf65264dc3199c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c228f2c3e9c4c57b1bd7c37d7ba3973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a35a29060cc4f79a6322104b998f453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08afbe78fc7c44fab1af08ee97a5518d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4601d504994f81b0fd0938e8380d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58528b04545042558fb9609b36c70811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339ed5995193434cb769949cf5d87eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf518fe894dd4b9faba27f785ce83819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9653220f747a4e2fa87e21c171b8f43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4d53cfe64f4593885c38b7ed30138c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc38a92f1ae45099e7fc3f9d4b983a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220a065eba504866a0059fb230eff172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89159500a54540edb223a624c216a716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f370d2001c497da8f2a9bdb7d31b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7006cffd17b24c10951eb91d8fb1f21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9140001a844742a5c6b644b67d1aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25412059e33a47dd83931bed40fa467a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7aab556a3c4677b4b5c7f4df49c987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171d28bc66b14d5cae0e4ecb9124d55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2b1ba5f2864cb286cad2008ac0d374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1f2d0169804a8d9454f15c3f052cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcd6c02d18642b69e80312ee3108cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4909f6ce4ce14c2a94a571d1ee24f58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8c750ecab9427fb61e84b01cccff3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b271ef1444485bba6970d6fcab534e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer.fit(cem_model, train_loader, val_loader)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model set to evaluation mode\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "cem_model.eval()\n",
    "\n",
    "# Move model to device\n",
    "device_obj = torch.device(DEVICE)\n",
    "cem_model = cem_model.to(device_obj)\n",
    "\n",
    "print(\"✓ Model set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on test set...\n",
      "✓ Inference complete\n",
      "  Predictions shape: (201,)\n",
      "  Concept probs shape: (201, 21)\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test set\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "y_prob_list = []\n",
    "concept_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in test_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "        \n",
    "        # Forward pass\n",
    "        c_logits, _, y_logits = cem_model(x_batch)\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        c_probs = torch.sigmoid(c_logits).cpu().numpy()\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "        \n",
    "        # Threshold at 0.5 for predictions\n",
    "        y_pred = (y_probs >= 0.5).astype(int)\n",
    "        \n",
    "        # Collect results\n",
    "        y_true_list.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_pred_list.extend(y_pred.tolist() if isinstance(y_pred, np.ndarray) else [y_pred])\n",
    "        y_prob_list.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "        concept_probs_list.extend(c_probs.tolist())\n",
    "\n",
    "# Convert to arrays\n",
    "y_true = np.array(y_true_list)\n",
    "y_pred = np.array(y_pred_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "concept_probs = np.array(concept_probs_list)\n",
    "\n",
    "print(\"✓ Inference complete\")\n",
    "print(f\"  Predictions shape: {y_pred.shape}\")\n",
    "print(f\"  Concept probs shape: {concept_probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Metrics & Results Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "✓ Metrics computed\n"
     ]
    }
   ],
   "source": [
    "# Compute all metrics\n",
    "print(\"Computing metrics...\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "f1_binary = f1_score(y_true, y_pred, pos_label=1)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "precision_binary = precision_score(y_true, y_pred, pos_label=1)\n",
    "recall_binary = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "print(\"✓ Metrics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    TEST SET EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Test subjects:        201\n",
      "  Positive cases:       26 (12.9%)\n",
      "  Negative cases:       175 (87.1%)\n",
      "\n",
      "Performance Metrics:\n",
      "  Accuracy:                  0.8507\n",
      "  Balanced Accuracy:         0.6851\n",
      "  ROC-AUC:                   0.8798\n",
      "  Matthews Correlation:      0.3587\n",
      "\n",
      "  F1 Score (Binary):         0.4444\n",
      "  F1 Score (Macro):          0.6791\n",
      "  F1 Score (Micro):          0.8507\n",
      "\n",
      "  Precision (Binary):        0.4286\n",
      "  Recall (Binary):           0.4615\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg    Predicted Pos\n",
      "Actual Neg                159               16       \n",
      "Actual Pos                 14               12       \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.91      0.91       175\n",
      "    Positive       0.43      0.46      0.44        26\n",
      "\n",
      "    accuracy                           0.85       201\n",
      "   macro avg       0.67      0.69      0.68       201\n",
      "weighted avg       0.86      0.85      0.85       201\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print formatted results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"  Test subjects:        {len(y_true)}\")\n",
    "print(f\"  Positive cases:       {np.sum(y_true)} ({100*np.sum(y_true)/len(y_true):.1f}%)\")\n",
    "print(f\"  Negative cases:       {len(y_true)-np.sum(y_true)} ({100*(len(y_true)-np.sum(y_true))/len(y_true):.1f}%)\")\n",
    "print()\n",
    "print(f\"Performance Metrics:\")\n",
    "print(f\"  Accuracy:                  {acc:.4f}\")\n",
    "print(f\"  Balanced Accuracy:         {balanced_acc:.4f}\")\n",
    "print(f\"  ROC-AUC:                   {roc_auc:.4f}\")\n",
    "print(f\"  Matthews Correlation:      {mcc:.4f}\")\n",
    "print()\n",
    "print(f\"  F1 Score (Binary):         {f1_binary:.4f}\")\n",
    "print(f\"  F1 Score (Macro):          {f1_macro:.4f}\")\n",
    "print(f\"  F1 Score (Micro):          {f1_micro:.4f}\")\n",
    "print()\n",
    "print(f\"  Precision (Binary):        {precision_binary:.4f}\")\n",
    "print(f\"  Recall (Binary):           {recall_binary:.4f}\")\n",
    "print()\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"                    Predicted Neg    Predicted Pos\")\n",
    "print(f\"Actual Neg          {tn:^16d} {fp:^16d}\")\n",
    "print(f\"Actual Pos          {fn:^16d} {tp:^16d}\")\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Metrics saved to outputs/results/test_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Save metrics to JSON\n",
    "metrics_dict = {\n",
    "    \"n_samples\": int(len(y_true)),\n",
    "    \"n_positive\": int(np.sum(y_true)),\n",
    "    \"n_negative\": int(len(y_true) - np.sum(y_true)),\n",
    "    \"accuracy\": float(acc),\n",
    "    \"balanced_accuracy\": float(balanced_acc),\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "    \"mcc\": float(mcc),\n",
    "    \"f1_binary\": float(f1_binary),\n",
    "    \"f1_macro\": float(f1_macro),\n",
    "    \"f1_micro\": float(f1_micro),\n",
    "    \"precision_binary\": float(precision_binary),\n",
    "    \"recall_binary\": float(recall_binary),\n",
    "    \"confusion_matrix\": {\n",
    "        \"tn\": int(tn),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "        \"tp\": int(tp)\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"results\"), exist_ok=True)\n",
    "metrics_path = os.path.join(OUTPUT_DIR, \"results/test_metrics.json\")\n",
    "\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "print(f\"✓ Metrics saved to {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Predictions saved to outputs/results/test_predictions.csv\n",
      "\n",
      "First 10 subjects with concept probabilities:\n",
      "         subject_id  y_true  y_pred        y_prob   Sadness  Pessimism  \\\n",
      "0  test_subject4471       1       0  2.887360e-03  0.041965   0.097764   \n",
      "1  test_subject8981       0       0  2.285121e-09  0.005434   0.058009   \n",
      "2  test_subject8777       0       0  2.217279e-21  0.000088   0.014087   \n",
      "3  test_subject1372       0       0  4.164572e-10  0.004552   0.055387   \n",
      "4  test_subject1830       0       0  1.097032e-05  0.014577   0.037514   \n",
      "5  test_subject3791       0       0  3.811828e-21  0.000133   0.017802   \n",
      "6  test_subject2284       0       0  1.210110e-09  0.004736   0.049253   \n",
      "7  test_subject5689       0       0  3.861887e-12  0.002627   0.033180   \n",
      "8  test_subject7467       1       0  1.204088e-08  0.007306   0.068030   \n",
      "9  test_subject7578       0       0  4.536292e-26  0.000011   0.002672   \n",
      "\n",
      "   Past failure  Loss of pleasure  Guilty feelings  Punishment feelings  ...  \\\n",
      "0      0.118874      1.565952e-02     8.362616e-03             0.068531  ...   \n",
      "1      0.006420      5.257675e-04     1.421976e-04             0.020993  ...   \n",
      "2      0.000017      3.241864e-07     3.124160e-08             0.001555  ...   \n",
      "3      0.004853      3.145588e-04     8.232011e-05             0.017991  ...   \n",
      "4      0.037963      3.139890e-03     1.309326e-03             0.032339  ...   \n",
      "5      0.000021      5.247280e-07     4.403773e-08             0.002040  ...   \n",
      "6      0.005101      3.073774e-04     8.769404e-05             0.016044  ...   \n",
      "7      0.001885      1.184444e-04     2.080193e-05             0.012214  ...   \n",
      "8      0.009332      7.280282e-04     2.197557e-04             0.023941  ...   \n",
      "9      0.000001      1.922149e-08     7.686954e-10             0.000498  ...   \n",
      "\n",
      "   Loss of interest  Indecisiveness  Worthlessness  Loss of energy  \\\n",
      "0          0.068440        0.061388       0.107948        0.053497   \n",
      "1          0.012080        0.006237       0.018591        0.006488   \n",
      "2          0.000262        0.000054       0.000353        0.000061   \n",
      "3          0.009581        0.004651       0.015318        0.004579   \n",
      "4          0.031074        0.027697       0.031732        0.017203   \n",
      "5          0.000432        0.000063       0.000421        0.000089   \n",
      "6          0.008723        0.005167       0.013708        0.004235   \n",
      "7          0.007277        0.002513       0.006569        0.002417   \n",
      "8          0.014076        0.007793       0.023608        0.007751   \n",
      "9          0.000067        0.000010       0.000029        0.000009   \n",
      "\n",
      "   Changes in sleeping pattern  Irritability  Changes in appetite  \\\n",
      "0                     0.046579      0.015019             0.046907   \n",
      "1                     0.011022      0.002759             0.010360   \n",
      "2                     0.000445      0.000087             0.000306   \n",
      "3                     0.008429      0.002283             0.007851   \n",
      "4                     0.037897      0.007537             0.028462   \n",
      "5                     0.000654      0.000105             0.000409   \n",
      "6                     0.007889      0.002407             0.007428   \n",
      "7                     0.009717      0.001612             0.006037   \n",
      "8                     0.011428      0.003343             0.011141   \n",
      "9                     0.000635      0.000038             0.000099   \n",
      "\n",
      "   Concentration difficulty  Tiredness or fatigue  Loss of interest in sex  \n",
      "0                  0.072837          4.235375e-03                 0.026026  \n",
      "1                  0.011193          1.155117e-05                 0.002333  \n",
      "2                  0.000224          6.162624e-11                 0.000012  \n",
      "3                  0.008326          5.284596e-06                 0.001761  \n",
      "4                  0.037892          4.073197e-04                 0.007375  \n",
      "5                  0.000287          9.610145e-11                 0.000019  \n",
      "6                  0.008738          6.725642e-06                 0.001471  \n",
      "7                  0.006523          8.402810e-07                 0.000831  \n",
      "8                  0.012656          2.177067e-05                 0.003045  \n",
      "9                  0.000137          4.727359e-13                 0.000001  \n",
      "\n",
      "[10 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create predictions DataFrame with concept probabilities\n",
    "predictions_df = pd.DataFrame({\n",
    "    'subject_id': test_subject_ids,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'y_prob': y_prob\n",
    "})\n",
    "\n",
    "# Add concept probabilities\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    predictions_df[concept_name] = concept_probs[:, i]\n",
    "\n",
    "# Save to CSV\n",
    "predictions_path = os.path.join(OUTPUT_DIR, \"results/test_predictions.csv\")\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "print(f\"✓ Predictions saved to {predictions_path}\")\n",
    "print(f\"\\nFirst 10 subjects with concept probabilities:\")\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept Activation Statistics:\n",
      "======================================================================\n",
      "Concept                                   Mean        Std        Max\n",
      "----------------------------------------------------------------------\n",
      "Sadness                                 0.0356     0.0693     0.3065\n",
      "Pessimism                               0.1028     0.1734     0.7077\n",
      "Past failure                            0.0968     0.1896     0.7864\n",
      "Loss of pleasure                        0.0262     0.0619     0.3623\n",
      "Guilty feelings                         0.0196     0.0479     0.2155\n",
      "Punishment feelings                     0.0507     0.0846     0.3506\n",
      "Self-dislike                            0.1501     0.2114     0.9281\n",
      "Self-criticalness                       0.0467     0.0904     0.3340\n",
      "Suicidal thoughts or wishes             0.0138     0.0351     0.1587\n",
      "Crying                                  0.0091     0.0229     0.1382\n",
      "Agitation                               0.0066     0.0127     0.0497\n",
      "Loss of interest                        0.0526     0.0954     0.4219\n",
      "Indecisiveness                          0.0437     0.0793     0.3456\n",
      "Worthlessness                           0.0695     0.1225     0.4715\n",
      "Loss of energy                          0.0567     0.1184     0.6089\n",
      "Changes in sleeping pattern             0.0318     0.0486     0.2174\n",
      "Irritability                            0.0091     0.0145     0.0554\n",
      "Changes in appetite                     0.0285     0.0434     0.1458\n",
      "Concentration difficulty                0.0519     0.0882     0.3424\n",
      "Tiredness or fatigue                    0.0264     0.0706     0.3412\n",
      "Loss of interest in sex                 0.0211     0.0408     0.1578\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display concept activation statistics\n",
    "print(\"\\nConcept Activation Statistics:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Concept':<35} {'Mean':>10} {'Std':>10} {'Max':>10}\")\n",
    "print(\"-\"*70)\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    mean_act = np.mean(concept_probs[:, i])\n",
    "    std_act = np.std(concept_probs[:, i])\n",
    "    max_act = np.max(concept_probs[:, i])\n",
    "    print(f\"{concept_name:<35} {mean_act:>10.4f} {std_act:>10.4f} {max_act:>10.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned up temporary directory: /var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000gn/T/test_chunks_804t3r5a\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary directory\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"✓ Cleaned up temporary directory: {temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Failed to clean up temporary directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "              PIPELINE EXECUTION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Generated files:\n",
      "  Model checkpoint: outputs/models/\n",
      "  Metrics JSON:     outputs/results/test_metrics.json\n",
      "  Predictions CSV:  outputs/results/test_predictions.csv\n",
      "  Training logs:    outputs/logs/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"              PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(f\"  Model checkpoint: {OUTPUT_DIR}/models/\")\n",
    "print(f\"  Metrics JSON:     {OUTPUT_DIR}/results/test_metrics.json\")\n",
    "print(f\"  Predictions CSV:  {OUTPUT_DIR}/results/test_predictions.csv\")\n",
    "print(f\"  Training logs:    {OUTPUT_DIR}/logs/\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38concept_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
