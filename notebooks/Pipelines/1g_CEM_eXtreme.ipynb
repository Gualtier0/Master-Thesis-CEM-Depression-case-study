{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEM Model - eXtreme Alternative Pipeline (Temperature-Sharpened MAX)\n",
    "\n",
    "**Runtime:** ~20-25 minutes\n",
    "\n",
    "This notebook:\n",
    "1. Trains **CEM** using eXtreme-based concept similarity data (temperature-sharpened MAX)\n",
    "2. Uses **LDAM Loss** for severe class imbalance (66 positive, 322 negative)\n",
    "3. Tests threshold optimization to maximize minority class detection\n",
    "4. Evaluates on test set with balanced metrics\n",
    "\n",
    "**Data Source:** `extreme_alternative_attention_pipeline` (temperature-sharpened MAX)\n",
    "- Amplifies strong cosine similarities before MAX\n",
    "- Creates more extreme post selection (winner-take-all effect)\n",
    "- COSINE_TEMPERATURE = 0.5 applied in data preparation\n",
    "\n",
    "**Prerequisites:** Run `0d_eXtreme_dataset.ipynb` first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "# Detect device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configured\n",
      "  Dataset dir: /teamspace/studios/this_studio/Master-Thesis-CEM-Depression-etc-case-study/data/processed/extreme_alternative_attention_pipeline\n",
      "  Output dir: outputs_cem_extreme\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "DATASET_DIR = os.path.join(DATA_PROCESSED, \"extreme_alternative_attention_pipeline\")\n",
    "OUTPUT_DIR = \"outputs_cem_extreme\"\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined 21 BDI-II concepts\n"
     ]
    }
   ],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters configured\n",
      "  Using LDAM LOSS (margin=0.3, scale=20)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    # Model architecture\n",
    "    \"embedding_dim\": 384,\n",
    "    \"n_concepts\": 21,\n",
    "    \"n_tasks\": 1,\n",
    "    \"emb_size\": 128,\n",
    "    \n",
    "    # CEM-specific\n",
    "    \"shared_prob_gen\": True,        # Share probability generator across concepts\n",
    "    \"intervention_prob\": 0.25,      # Training intervention probability\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size_train\": 32,\n",
    "    \"batch_size_eval\": 64,\n",
    "    \"max_epochs\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 4e-05,\n",
    "    \n",
    "    # Loss\n",
    "    \"concept_loss_weight\": 1.0,\n",
    "    \n",
    "    # LDAM Loss\n",
    "    \"use_ldam_loss\": True,\n",
    "    \"n_positive\": None,               # Will be set after loading data\n",
    "    \"n_negative\": None,               # Will be set after loading data\n",
    "    \"ldam_max_margin\": 0.3,           # Try: 0.3, 0.5, 0.7, 1.0\n",
    "    \"ldam_scale\": 20,                 # Try: 20, 30, 40, 50\n",
    "    \n",
    "    # Weighted Sampler\n",
    "    \"use_weighted_sampler\": False,\n",
    "}\n",
    "\n",
    "print(\"✓ Hyperparameters configured\")\n",
    "if HYPERPARAMS['use_ldam_loss']:\n",
    "    print(f\"  Using LDAM LOSS (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\n",
    "else:\n",
    "    print(f\"  Using standard BCE loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed datasets...\n",
      "✓ Loaded training data: (388, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "print(\"Loading preprocessed datasets...\")\n",
    "\n",
    "train_data = np.load(os.path.join(DATASET_DIR, \"train_data.npz\"))\n",
    "X_train = train_data['X']\n",
    "C_train = train_data['C']\n",
    "y_train = train_data['y']\n",
    "train_subject_ids = train_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded training data: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded validation data: (98, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "val_data = np.load(os.path.join(DATASET_DIR, \"val_data.npz\"))\n",
    "X_val = val_data['X']\n",
    "C_val = val_data['C']\n",
    "y_val = val_data['y']\n",
    "val_subject_ids = val_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded validation data: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test data: (401, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = np.load(os.path.join(DATASET_DIR, \"test_data.npz\"))\n",
    "X_test = test_data['X']\n",
    "C_test = test_data['C']\n",
    "y_test = test_data['y']\n",
    "test_subject_ids = test_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded class weights:\n",
      "  Negative: 322, Positive: 66\n",
      "  Ratio: 1:4.88\n"
     ]
    }
   ],
   "source": [
    "# Load class weights\n",
    "with open(os.path.join(DATASET_DIR, \"class_weights.json\"), 'r') as f:\n",
    "    class_info = json.load(f)\n",
    "\n",
    "n_positive = class_info['n_positive']\n",
    "n_negative = class_info['n_negative']\n",
    "pos_weight = class_info['pos_weight']\n",
    "\n",
    "# Update HYPERPARAMS with actual class counts for LDAM\n",
    "HYPERPARAMS['n_positive'] = n_positive\n",
    "HYPERPARAMS['n_negative'] = n_negative\n",
    "\n",
    "print(f\"✓ Loaded class weights:\")\n",
    "print(f\"  Negative: {n_negative}, Positive: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: PyTorch Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using standard DataLoader (shuffle=True)\n",
      "✓ All DataLoaders created\n"
     ]
    }
   ],
   "source": [
    "class CEMDataset(Dataset):\n",
    "    def __init__(self, X, C, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.C = torch.tensor(C, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.C[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CEMDataset(X_train, C_train, y_train)\n",
    "val_dataset = CEMDataset(X_val, C_val, y_val)\n",
    "test_dataset = CEMDataset(X_test, C_test, y_test)\n",
    "\n",
    "# Create WeightedRandomSampler for batch-level oversampling (if enabled)\n",
    "if HYPERPARAMS['use_weighted_sampler']:\n",
    "    # Compute class sample counts\n",
    "    class_sample_counts = np.bincount(y_train.astype(int))  # [n_negative, n_positive]\n",
    "    weights = 1. / class_sample_counts\n",
    "    sample_weights = weights[y_train.astype(int)]\n",
    "    \n",
    "    # Create sampler\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True  # Allow positive samples to appear multiple times\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ WeightedRandomSampler created:\")\n",
    "    print(f\"  Negative weight: {weights[0]:.4f}\")\n",
    "    print(f\"  Positive weight: {weights[1]:.4f}\")\n",
    "    print(f\"  Expected positive ratio per batch: ~{weights[1]/(weights[0]+weights[1]):.1%}\")\n",
    "    \n",
    "    # Create train loader with sampler (shuffle=False when using sampler)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], sampler=train_sampler)\n",
    "else:\n",
    "    # Standard train loader with shuffle\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], shuffle=True)\n",
    "    print(\"✓ Using standard DataLoader (shuffle=True)\")\n",
    "\n",
    "# Validation and test loaders (no sampling)\n",
    "val_loader = DataLoader(val_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "\n",
    "print(\"✓ All DataLoaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Custom CEM Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom CEM model defined\n"
     ]
    }
   ],
   "source": [
    "# LDAM Loss (for class imbalance)\n",
    "class LDAMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Label-Distribution-Aware Margin (LDAM) Loss for long-tailed recognition.\n",
    "    \n",
    "    Creates class-dependent margins to make decision boundaries harder for minority classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_positive, n_negative, max_margin=0.5, scale=30):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        self.max_margin = max_margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Compute class frequencies\n",
    "        total = n_positive + n_negative\n",
    "        freq_pos = n_positive / total\n",
    "        freq_neg = n_negative / total\n",
    "        \n",
    "        # Compute margins: minority class gets larger margin\n",
    "        margin_pos = max_margin * (freq_pos ** (-0.25))\n",
    "        margin_neg = max_margin * (freq_neg ** (-0.25))\n",
    "        \n",
    "        self.register_buffer('margin_pos', torch.tensor(margin_pos))\n",
    "        self.register_buffer('margin_neg', torch.tensor(margin_neg))\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1).float()\n",
    "        \n",
    "        # Apply class-dependent margins\n",
    "        margin = targets * self.margin_pos + (1 - targets) * (-self.margin_neg)\n",
    "        adjusted_logits = (logits - margin) * self.scale\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(adjusted_logits, targets, reduction='mean')\n",
    "\n",
    "\n",
    "# Custom CEM Implementation\n",
    "class CustomCEM(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Custom Concept Embedding Model (CEM) implementation.\n",
    "    \n",
    "    Architecture:\n",
    "      X → concept_extractor → context_layers → prob_generator → dual_embeddings → task_classifier → y\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_concepts=21,\n",
    "        emb_size=128,\n",
    "        input_dim=384,\n",
    "        shared_prob_gen=True,\n",
    "        intervention_prob=0.25,\n",
    "        concept_loss_weight=1.0,\n",
    "        learning_rate=0.01,\n",
    "        weight_decay=4e-05,\n",
    "        use_ldam_loss=True,\n",
    "        n_positive=83,\n",
    "        n_negative=403,\n",
    "        ldam_max_margin=0.5,\n",
    "        ldam_scale=30,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.n_concepts = n_concepts\n",
    "        self.emb_size = emb_size\n",
    "        self.intervention_prob = intervention_prob\n",
    "        self.concept_loss_weight = concept_loss_weight\n",
    "        \n",
    "        # Stage 1: Concept Extractor (X → Pre-Concept Features)\n",
    "        self.concept_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 256)  # Pre-concept features\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Context Generators (Features → Dual Embeddings)\n",
    "        # Each concept gets its own context generator\n",
    "        self.context_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, emb_size * 2),  # Dual embeddings (true/false)\n",
    "                nn.LeakyReLU()\n",
    "            ) for _ in range(n_concepts)\n",
    "        ])\n",
    "        \n",
    "        # Stage 3: Probability Generator (Contexts → Concept Probabilities)\n",
    "        if shared_prob_gen:\n",
    "            # Single shared generator for all concepts\n",
    "            self.prob_generator = nn.Linear(emb_size * 2, 1)\n",
    "        else:\n",
    "            # Per-concept probability generators\n",
    "            self.prob_generators = nn.ModuleList([\n",
    "                nn.Linear(emb_size * 2, 1) for _ in range(n_concepts)\n",
    "            ])\n",
    "        \n",
    "        self.shared_prob_gen = shared_prob_gen\n",
    "        \n",
    "        # Stage 4: Task Classifier (Concept Embeddings → Task Output)\n",
    "        self.task_classifier = nn.Sequential(\n",
    "            nn.Linear(n_concepts * emb_size, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        # Loss functions\n",
    "        self.concept_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        if use_ldam_loss:\n",
    "            self.task_loss_fn = LDAMLoss(n_positive, n_negative, ldam_max_margin, ldam_scale)\n",
    "        else:\n",
    "            self.task_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, x, c_true=None, train=False):\n",
    "        # Step 1: Extract pre-concept features\n",
    "        pre_features = self.concept_extractor(x)  # (B, 256)\n",
    "        \n",
    "        # Step 2: Generate contexts and probabilities per concept\n",
    "        contexts = []\n",
    "        c_logits_list = []\n",
    "        \n",
    "        for i, context_layer in enumerate(self.context_layers):\n",
    "            context = context_layer(pre_features)  # (B, emb_size*2)\n",
    "            \n",
    "            # Get probability logit\n",
    "            if self.shared_prob_gen:\n",
    "                logit = self.prob_generator(context)  # (B, 1)\n",
    "            else:\n",
    "                logit = self.prob_generators[i](context)\n",
    "            \n",
    "            contexts.append(context)\n",
    "            c_logits_list.append(logit)\n",
    "        \n",
    "        c_logits = torch.cat(c_logits_list, dim=1)  # (B, 21)\n",
    "        c_probs = torch.sigmoid(c_logits)           # (B, 21)\n",
    "        \n",
    "        # Step 3: Apply intervention (optional during training)\n",
    "        if train and self.intervention_prob > 0 and c_true is not None:\n",
    "            intervention_mask = torch.bernoulli(\n",
    "                torch.ones_like(c_probs) * self.intervention_prob\n",
    "            )\n",
    "            c_probs = c_probs * (1 - intervention_mask) + c_true * intervention_mask\n",
    "        \n",
    "        # Step 4: Mix dual embeddings based on probabilities\n",
    "        concept_embeddings = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            # Split into true/false embeddings\n",
    "            emb_true = context[:, :self.emb_size]       # First half\n",
    "            emb_false = context[:, self.emb_size:]      # Second half\n",
    "            \n",
    "            # Weight by probability\n",
    "            prob = c_probs[:, i:i+1]  # (B, 1)\n",
    "            mixed_emb = emb_true * prob + emb_false * (1 - prob)\n",
    "            concept_embeddings.append(mixed_emb)\n",
    "        \n",
    "        # Concatenate all concept embeddings\n",
    "        c_embeddings = torch.cat(concept_embeddings, dim=1)  # (B, 21*emb_size)\n",
    "        \n",
    "        # Step 5: Task prediction\n",
    "        y_logits = self.task_classifier(c_embeddings)  # (B, 1)\n",
    "        \n",
    "        return c_logits, y_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, c_true = batch\n",
    "        c_logits, y_logits = self.forward(x, c_true=c_true, train=True)\n",
    "        \n",
    "        # Task loss (LDAM)\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss (BCE)\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = task_loss + self.concept_loss_weight * concept_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('train_concept_loss', concept_loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, c_true = batch\n",
    "        c_logits, y_logits = self.forward(x, c_true=c_true, train=False)\n",
    "        \n",
    "        # Task loss\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = task_loss + self.concept_loss_weight * concept_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('val_concept_loss', concept_loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "print(\"✓ Custom CEM model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom CEM model initialized\n",
      "  Using LDAM Loss (margin=0.3, scale=20)\n",
      "  Concept embedding size: 128\n",
      "  Intervention probability: 0.25\n",
      "  Shared probability generator: True\n",
      "  Class counts: 66 positive, 322 negative\n",
      "  Data source: eXtreme alternative attention pipeline (temp-sharpened MAX)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Custom CEM model\n",
    "custom_cem = CustomCEM(\n",
    "    n_concepts=HYPERPARAMS['n_concepts'],\n",
    "    emb_size=HYPERPARAMS['emb_size'],\n",
    "    input_dim=HYPERPARAMS['embedding_dim'],\n",
    "    shared_prob_gen=HYPERPARAMS['shared_prob_gen'],\n",
    "    intervention_prob=HYPERPARAMS['intervention_prob'],\n",
    "    concept_loss_weight=HYPERPARAMS['concept_loss_weight'],\n",
    "    learning_rate=HYPERPARAMS['learning_rate'],\n",
    "    weight_decay=HYPERPARAMS['weight_decay'],\n",
    "    use_ldam_loss=HYPERPARAMS['use_ldam_loss'],\n",
    "    n_positive=HYPERPARAMS['n_positive'],\n",
    "    n_negative=HYPERPARAMS['n_negative'],\n",
    "    ldam_max_margin=HYPERPARAMS['ldam_max_margin'],\n",
    "    ldam_scale=HYPERPARAMS['ldam_scale']\n",
    ")\n",
    "\n",
    "print(\"✓ Custom CEM model initialized\")\n",
    "print(f\"  Using LDAM Loss (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\n",
    "print(f\"  Concept embedding size: {HYPERPARAMS['emb_size']}\")\n",
    "print(f\"  Intervention probability: {HYPERPARAMS['intervention_prob']}\")\n",
    "print(f\"  Shared probability generator: {HYPERPARAMS['shared_prob_gen']}\")\n",
    "print(f\"  Class counts: {HYPERPARAMS['n_positive']} positive, {HYPERPARAMS['n_negative']} negative\")\n",
    "print(f\"  Data source: eXtreme alternative attention pipeline (temp-sharpened MAX)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer configured\n"
     ]
    }
   ],
   "source": [
    "# Setup trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(OUTPUT_DIR, \"models\"),\n",
    "    filename=\"cem-max-gold-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=HYPERPARAMS['max_epochs'],\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    logger=CSVLogger(save_dir=os.path.join(OUTPUT_DIR, \"logs\"), name=\"cem_max_gold\"),\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ concept_extractor │ Sequential        │  164 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ context_layers    │ ModuleList        │  1.4 M │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ prob_generator    │ Linear            │    257 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ task_classifier   │ Sequential        │  344 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ concept_loss_fn   │ BCEWithLogitsLoss │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ task_loss_fn      │ LDAMLoss          │      0 │ train │     0 │\n",
       "└───┴───────────────────┴───────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ concept_extractor │ Sequential        │  164 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ context_layers    │ ModuleList        │  1.4 M │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ prob_generator    │ Linear            │    257 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ task_classifier   │ Sequential        │  344 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ concept_loss_fn   │ BCEWithLogitsLoss │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ task_loss_fn      │ LDAMLoss          │      0 │ train │     0 │\n",
       "└───┴───────────────────┴───────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.9 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.9 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 7                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 77                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.9 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.9 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 7                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 77                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7657f827a874de7af2e97c575fd2f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connec\n",
       "tor.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the \n",
       "value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connec\n",
       "tor.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the \n",
       "value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connec\n",
       "tor.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the \n",
       "value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connec\n",
       "tor.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the \n",
       "value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\\n\")\n",
    "trainer.fit(custom_cem, train_loader, val_loader)\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference on validation set for threshold selection...\n",
      "✓ Validation inference complete\n"
     ]
    }
   ],
   "source": [
    "## Validation threshold optimization\n",
    "\n",
    "print(\"\\nRunning inference on validation set for threshold selection...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "y_val_true = []\n",
    "y_val_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, _ in val_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "\n",
    "        _, y_logits = custom_cem(x_batch)\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "\n",
    "        y_val_true.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_val_prob.extend(\n",
    "            y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs]\n",
    "        )\n",
    "\n",
    "y_val_true = np.array(y_val_true)\n",
    "y_val_prob = np.array(y_val_prob)\n",
    "\n",
    "print(\"✓ Validation inference complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting decision threshold on VALIDATION set...\n",
      "✓ Selected validation threshold: 0.050\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSelecting decision threshold on VALIDATION set...\")\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_metric = -1.0\n",
    "\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 91)\n",
    "\n",
    "for threshold in THRESHOLDS:\n",
    "    y_pred_temp = (y_val_prob >= threshold).astype(int)\n",
    "\n",
    "    if y_pred_temp.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    # choose ONE objective, uncomment the one you want\n",
    "\n",
    "    # For recall maximization\n",
    "    metric = recall_score(y_val_true, y_pred_temp)\n",
    "\n",
    "    # For F1\n",
    "    # metric = f1_score(y_val_true, y_pred_temp)\n",
    "\n",
    "    # For MCC\n",
    "    # metric = matthews_corrcoef(y_val_true, y_pred_temp)\n",
    "\n",
    "    if metric > best_metric:\n",
    "        best_metric = metric\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"✓ Selected validation threshold: {best_threshold:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on test set...\n",
      "✓ Inference complete\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test set\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "y_true_list = []\n",
    "y_prob_list = []\n",
    "concept_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in test_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "        \n",
    "        c_logits, y_logits = custom_cem(x_batch)\n",
    "        c_probs = torch.sigmoid(c_logits).cpu().numpy()\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "        \n",
    "        y_true_list.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_prob_list.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "        concept_probs_list.extend(c_probs.tolist())\n",
    "\n",
    "y_true = np.array(y_true_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "\n",
    "concept_probs = np.array(concept_probs_list)\n",
    "\n",
    "print(\"✓ Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best threshold for final predictions\n",
    "y_pred = (y_prob >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Results Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    TEST SET EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Decision Threshold: 0.05\n",
      "\n",
      "                 CONFUSION MATRIX                 \n",
      "==================================================\n",
      "                     │ Predicted Negative │ Predicted Positive\n",
      "──────────────────────────────────────────────────\n",
      "     Actual Negative │   TN = 306   │   FP = 43   \n",
      "     Actual Positive │   FN = 12    │   TP = 40   \n",
      "==================================================\n",
      "\n",
      "  True Positives:   40/52  ( 76.9% of depression cases caught)\n",
      "  False Negatives:  12/52  ( 23.1% of depression cases MISSED)\n",
      "  True Negatives:  306/349 ( 87.7% of healthy correctly identified)\n",
      "  False Positives:  43/349 ( 12.3% false alarms)\n",
      "\n",
      "Performance Metrics:\n",
      "  Accuracy:                  0.8628\n",
      "  Balanced Accuracy:         0.8230\n",
      "  ROC-AUC:                   0.8992\n",
      "  Matthews Correlation:      0.5357\n",
      "\n",
      "  F1 Score (Binary):         0.5926\n",
      "  F1 Score (Macro):          0.7551\n",
      "  Precision (Binary):        0.4819\n",
      "  Recall (Binary):           0.7692\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.96      0.88      0.92       349\n",
      "    Positive       0.48      0.77      0.59        52\n",
      "\n",
      "    accuracy                           0.86       401\n",
      "   macro avg       0.72      0.82      0.76       401\n",
      "weighted avg       0.90      0.86      0.88       401\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute all metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "f1_binary = f1_score(y_true, y_pred, pos_label=1)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "precision_binary = precision_score(y_true, y_pred, pos_label=1)\n",
    "recall_binary = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDecision Threshold: {best_threshold:.2f}\")\n",
    "\n",
    "# Enhanced Confusion Matrix Display\n",
    "print(f\"\\n{'CONFUSION MATRIX':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'':>20} │ {'Predicted Negative':^12} │ {'Predicted Positive':^12}\")\n",
    "print(\"─\"*50)\n",
    "print(f\"{'Actual Negative':>20} │ {f'TN = {tn}':^12} │ {f'FP = {fp}':^12}\")\n",
    "print(f\"{'Actual Positive':>20} │ {f'FN = {fn}':^12} │ {f'TP = {tp}':^12}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n  True Positives:  {tp:>3}/{int(np.sum(y_true)):<3} ({100*tp/np.sum(y_true):>5.1f}% of depression cases caught)\")\n",
    "print(f\"  False Negatives: {fn:>3}/{int(np.sum(y_true)):<3} ({100*fn/np.sum(y_true):>5.1f}% of depression cases MISSED)\")\n",
    "print(f\"  True Negatives:  {tn:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*tn/(len(y_true)-np.sum(y_true)):>5.1f}% of healthy correctly identified)\")\n",
    "print(f\"  False Positives: {fp:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*fp/(len(y_true)-np.sum(y_true)):>5.1f}% false alarms)\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:                  {acc:.4f}\")\n",
    "print(f\"  Balanced Accuracy:         {balanced_acc:.4f}\")\n",
    "print(f\"  ROC-AUC:                   {roc_auc:.4f}\")\n",
    "print(f\"  Matthews Correlation:      {mcc:.4f}\")\n",
    "print(f\"\\n  F1 Score (Binary):         {f1_binary:.4f}\")\n",
    "print(f\"  F1 Score (Macro):          {f1_macro:.4f}\")\n",
    "print(f\"  Precision (Binary):        {precision_binary:.4f}\")\n",
    "print(f\"  Recall (Binary):           {recall_binary:.4f}\")\n",
    "\n",
    "print(\"\\n\" + classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to outputs_cem_extreme/results/\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "metrics_dict = {\n",
    "    \"model_type\": \"cem_extreme\",\n",
    "    \"data_source\": \"extreme_alternative_attention_pipeline\",\n",
    "    \"threshold\": float(best_threshold),\n",
    "    \"n_samples\": int(len(y_true)),\n",
    "    \"n_positive\": int(np.sum(y_true)),\n",
    "    \"n_negative\": int(len(y_true) - np.sum(y_true)),\n",
    "    \"accuracy\": float(acc),\n",
    "    \"balanced_accuracy\": float(balanced_acc),\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "    \"mcc\": float(mcc),\n",
    "    \"f1_binary\": float(f1_binary),\n",
    "    \"f1_macro\": float(f1_macro),\n",
    "    \"precision_binary\": float(precision_binary),\n",
    "    \"recall_binary\": float(recall_binary),\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)}\n",
    "}\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"results\"), exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"results/test_metrics.json\"), 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'subject_id': test_subject_ids,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'y_prob': y_prob\n",
    "})\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    predictions_df[concept_name] = concept_probs[:, i]\n",
    "\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_DIR, \"results/test_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"✓ Results saved to {OUTPUT_DIR}/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "              CEM MAX-GOLD TRAINING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Generated files:\n",
      "  Model checkpoint: outputs_cem_extreme/models/\n",
      "  Metrics JSON:     outputs_cem_extreme/results/test_metrics.json\n",
      "  Predictions CSV:  outputs_cem_extreme/results/test_predictions.csv\n",
      "\n",
      "Data source: max_alternative_attention_pipeline\n",
      "  - Uses MAX-based concept similarity (specialist posts)\n",
      "  - Captures posts highly relevant to at least ONE concept\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"              CEM MAX-GOLD TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  Model checkpoint: {OUTPUT_DIR}/models/\")\n",
    "print(f\"  Metrics JSON:     {OUTPUT_DIR}/results/test_metrics.json\")\n",
    "print(f\"  Predictions CSV:  {OUTPUT_DIR}/results/test_predictions.csv\")\n",
    "print(f\"\\nData source: max_alternative_attention_pipeline\")\n",
    "print(f\"  - Uses MAX-based concept similarity (specialist posts)\")\n",
    "print(f\"  - Captures posts highly relevant to at least ONE concept\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38concept_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
