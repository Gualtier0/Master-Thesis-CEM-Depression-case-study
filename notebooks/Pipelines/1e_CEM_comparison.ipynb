{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CEM Model - MAX Alternative Pipeline Comparison\n",
    "\n",
    "**Runtime:** ~15-20 minutes\n",
    "\n",
    "This notebook:\n",
    "1. Implements CEM from scratch using PyTorch\n",
    "2. Uses LDAM Loss + WeightedRandomSampler for class imbalance\n",
    "3. **Uses MAX-based concept scoring dataset** (from 0c_prepare_max_alt_dataset)\n",
    "4. **Validation set has TRUE concept labels** (from 20% train split)\n",
    "\n",
    "**Key Difference from 1c (SUM-based):**\n",
    "- Uses data from `max_alternative_attention_pipeline` (MAX of concept similarities)\n",
    "- Captures posts highly relevant to at least ONE concept (specialists)\n",
    "- SUM-based (1c) captures posts relevant to MULTIPLE concepts (generalists)\n",
    "\n",
    "**Prerequisites:** Run `0c_prepare_max_alt_dataset.ipynb` first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using MacBook GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Detect device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configured\n",
      "  Dataset dir: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/max_alternative_attention_pipeline\n",
      "  Output dir: outputs_cem_max_comparison\n",
      "\n",
      "  NOTE: Using MAX ALTERNATIVE dataset (MAX-based concept scoring)\n",
      "        Captures posts highly relevant to at least ONE concept (specialists)\n",
      "        Validation set has TRUE concept labels!\n"
     ]
    }
   ],
   "source": [
    "# Define paths - CHANGED FOR MAX ALTERNATIVE PIPELINE\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "DATASET_DIR = os.path.join(DATA_PROCESSED, \"max_alternative_attention_pipeline\")  # CHANGED TO MAX\n",
    "OUTPUT_DIR = \"outputs_cem_max_comparison\"  # CHANGED\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n",
    "print(\"\\n  NOTE: Using MAX ALTERNATIVE dataset (MAX-based concept scoring)\")\n",
    "print(\"        Captures posts highly relevant to at least ONE concept (specialists)\")\n",
    "print(\"        Validation set has TRUE concept labels!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined 21 BDI-II concepts\n"
     ]
    }
   ],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters configured\n",
      "  Using LDAM LOSS (margin=0.5, scale=40)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    # Model architecture\n",
    "    \"embedding_dim\": 384,\n",
    "    \"n_concepts\": 21,\n",
    "    \"n_tasks\": 1,\n",
    "    \"emb_size\": 128,\n",
    "    \n",
    "    # CEM-specific\n",
    "    \"shared_prob_gen\": True,        # Share probability generator across concepts\n",
    "    \"intervention_prob\": 0.1,      # Training intervention probability\n",
    "    \"concept_temperature\": 2.0,\n",
    "\n",
    "    # Training\n",
    "    \"batch_size_train\": 32,\n",
    "    \"batch_size_eval\": 64,\n",
    "    \"max_epochs\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 4e-05,\n",
    "    \n",
    "    # Loss\n",
    "    \"concept_loss_weight\": 1.0,\n",
    "    \n",
    "    # LDAM Loss\n",
    "    \"use_ldam_loss\": True,\n",
    "    \"n_positive\": None,               # Will be set after loading data\n",
    "    \"n_negative\": None,               # Will be set after loading data\n",
    "    \"ldam_max_margin\": 0.5,           # Try: 0.3, 0.5, 0.7, 1.0\n",
    "    \"ldam_scale\": 40,                 # Try: 20, 30, 40, 50\n",
    "    \n",
    "    # Weighted Sampler\n",
    "    \"use_weighted_sampler\": False,\n",
    "}\n",
    "\n",
    "print(\"✓ Hyperparameters configured\")\n",
    "if HYPERPARAMS['use_ldam_loss']:\n",
    "    print(f\"  Using LDAM LOSS (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\n",
    "else:\n",
    "    print(f\"  Using standard BCE loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed datasets...\n",
      "✓ Loaded training data: (388, 384)\n",
      "  Embedding dimensions: 384 (MAX-based)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "print(\"Loading preprocessed datasets...\")\n",
    "\n",
    "train_data = np.load(os.path.join(DATASET_DIR, \"train_data.npz\"))\n",
    "X_train = train_data['X']\n",
    "C_train = train_data['C']\n",
    "y_train = train_data['y']\n",
    "train_subject_ids = train_data['subject_ids']\n",
    "\n",
    "# Concept loss weighting\n",
    "concept_pos_counts = C_train.sum(axis=0)\n",
    "concept_neg_counts = C_train.shape[0] - concept_pos_counts\n",
    "\n",
    "concept_pos_weight = torch.tensor(\n",
    "    concept_neg_counts / (concept_pos_counts + 1e-6),\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded training data: {X_train.shape}\")\n",
    "print(f\"  Embedding dimensions: {X_train.shape[1]} (MAX-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept pos_weight stats:\n",
      " min: 3.9743590354919434\n",
      " mean: 26.74332618713379\n",
      " max: 95.9999771118164\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "print(\"Concept pos_weight stats:\")\n",
    "print(\" min:\", concept_pos_weight.min().item())\n",
    "print(\" mean:\", concept_pos_weight.mean().item())\n",
    "print(\" max:\", concept_pos_weight.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded validation data: (98, 384)\n",
      "  Validation concept matrix has 126 non-zero values\n",
      "  (Has TRUE labels for concept evaluation!)\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "val_data = np.load(os.path.join(DATASET_DIR, \"val_data.npz\"))\n",
    "X_val = val_data['X']\n",
    "C_val = val_data['C']\n",
    "y_val = val_data['y']\n",
    "val_subject_ids = val_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded validation data: {X_val.shape}\")\n",
    "print(f\"  Validation concept matrix has {np.count_nonzero(C_val)} non-zero values\")\n",
    "print(f\"  (Has TRUE labels for concept evaluation!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test data: (401, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = np.load(os.path.join(DATASET_DIR, \"test_data.npz\"))\n",
    "X_test = test_data['X']\n",
    "C_test = test_data['C']\n",
    "y_test = test_data['y']\n",
    "test_subject_ids = test_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded class weights:\n",
      "  Negative: 322, Positive: 66\n",
      "  Ratio: 1:4.88\n"
     ]
    }
   ],
   "source": [
    "# Load class weights\n",
    "with open(os.path.join(DATASET_DIR, \"class_weights.json\"), 'r') as f:\n",
    "    class_info = json.load(f)\n",
    "\n",
    "n_positive = class_info['n_positive']\n",
    "n_negative = class_info['n_negative']\n",
    "pos_weight = class_info['pos_weight']\n",
    "\n",
    "# Update HYPERPARAMS with actual class counts for LDAM\n",
    "HYPERPARAMS['n_positive'] = n_positive\n",
    "HYPERPARAMS['n_negative'] = n_negative\n",
    "\n",
    "print(f\"✓ Loaded class weights:\")\n",
    "print(f\"  Negative: {n_negative}, Positive: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: PyTorch Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using standard DataLoader (shuffle=True)\n",
      "✓ All DataLoaders created\n"
     ]
    }
   ],
   "source": [
    "class CEMDataset(Dataset):\n",
    "    def __init__(self, X, C, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.C = torch.tensor(C, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.C[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CEMDataset(X_train, C_train, y_train)\n",
    "val_dataset = CEMDataset(X_val, C_val, y_val)\n",
    "test_dataset = CEMDataset(X_test, C_test, y_test)\n",
    "\n",
    "# Create WeightedRandomSampler for batch-level oversampling (if enabled)\n",
    "if HYPERPARAMS['use_weighted_sampler']:\n",
    "    # Compute class sample counts\n",
    "    class_sample_counts = np.bincount(y_train.astype(int))  # [n_negative, n_positive]\n",
    "    weights = 1. / class_sample_counts\n",
    "    sample_weights = weights[y_train.astype(int)]\n",
    "    \n",
    "    # Create sampler\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True  # Allow positive samples to appear multiple times\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ WeightedRandomSampler created:\")\n",
    "    print(f\"  Negative weight: {weights[0]:.4f}\")\n",
    "    print(f\"  Positive weight: {weights[1]:.4f}\")\n",
    "    print(f\"  Expected positive ratio per batch: ~{weights[1]/(weights[0]+weights[1]):.1%}\")\n",
    "    \n",
    "    # Create train loader with sampler (shuffle=False when using sampler)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], sampler=train_sampler)\n",
    "else:\n",
    "    # Standard train loader with shuffle\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], shuffle=True)\n",
    "    print(\"✓ Using standard DataLoader (shuffle=True)\")\n",
    "\n",
    "# Validation and test loaders (no sampling)\n",
    "val_loader = DataLoader(val_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "\n",
    "print(\"✓ All DataLoaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Custom CEM Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom CEM model defined\n"
     ]
    }
   ],
   "source": [
    "# LDAM Loss (for class imbalance)\n",
    "class LDAMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Label-Distribution-Aware Margin (LDAM) Loss for long-tailed recognition.\n",
    "    \n",
    "    Creates class-dependent margins to make decision boundaries harder for minority classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_positive, n_negative, max_margin=0.5, scale=30):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        self.max_margin = max_margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Compute class frequencies\n",
    "        total = n_positive + n_negative\n",
    "        freq_pos = n_positive / total\n",
    "        freq_neg = n_negative / total\n",
    "        \n",
    "        # Compute margins: minority class gets larger margin\n",
    "        margin_pos = max_margin * (freq_pos ** (-0.25))\n",
    "        margin_neg = max_margin * (freq_neg ** (-0.25))\n",
    "        \n",
    "        self.register_buffer('margin_pos', torch.tensor(margin_pos))\n",
    "        self.register_buffer('margin_neg', torch.tensor(margin_neg))\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1).float()\n",
    "        \n",
    "        # Apply class-dependent margins\n",
    "        margin = targets * self.margin_pos + (1 - targets) * (-self.margin_neg)\n",
    "        adjusted_logits = (logits - margin) * self.scale\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(adjusted_logits, targets, reduction='mean')\n",
    "\n",
    "\n",
    "# Custom CEM Implementation\n",
    "class CustomCEM(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Custom Concept Embedding Model (CEM) implementation.\n",
    "    \n",
    "    Architecture:\n",
    "      X → concept_extractor → context_layers → prob_generator → dual_embeddings → task_classifier → y\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_concepts=21,\n",
    "        emb_size=128,\n",
    "        input_dim=384,\n",
    "        shared_prob_gen=True,\n",
    "        intervention_prob=0.25,\n",
    "        concept_loss_weight=1.0,\n",
    "        learning_rate=0.01,\n",
    "        weight_decay=4e-05,\n",
    "        use_ldam_loss=True,\n",
    "        n_positive=83,\n",
    "        n_negative=403,\n",
    "        ldam_max_margin=0.5,\n",
    "        ldam_scale=30,\n",
    "        concept_temperature=2.0,\n",
    "        concept_pos_weight=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.n_concepts = n_concepts\n",
    "        self.emb_size = emb_size\n",
    "        self.intervention_prob = intervention_prob\n",
    "        self.concept_loss_weight = concept_loss_weight\n",
    "        self.concept_temperature = concept_temperature\n",
    "\n",
    "        # Stage 1: Concept Extractor (X → Pre-Concept Features)\n",
    "        self.concept_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 256)  # Pre-concept features\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Context Generators (Features → Dual Embeddings)\n",
    "        # Each concept gets its own context generator\n",
    "        self.context_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, emb_size * 2),  # Dual embeddings (true/false)\n",
    "                nn.LeakyReLU()\n",
    "            ) for _ in range(n_concepts)\n",
    "        ])\n",
    "        \n",
    "        # Stage 3: Probability Generator (Contexts → Concept Probabilities)\n",
    "        if shared_prob_gen:\n",
    "            # Single shared generator for all concepts\n",
    "            self.prob_generator = nn.Linear(emb_size * 2, 1)\n",
    "        else:\n",
    "            # Per-concept probability generators\n",
    "            self.prob_generators = nn.ModuleList([\n",
    "                nn.Linear(emb_size * 2, 1) for _ in range(n_concepts)\n",
    "            ])\n",
    "        \n",
    "        self.shared_prob_gen = shared_prob_gen\n",
    "        \n",
    "        # Stage 4: Task Classifier (Concept Embeddings → Task Output)\n",
    "        self.task_classifier = nn.Sequential(\n",
    "            nn.Linear(n_concepts * emb_size, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        # Loss functions\n",
    "        self.concept_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        if use_ldam_loss:\n",
    "            self.task_loss_fn = LDAMLoss(n_positive, n_negative, ldam_max_margin, ldam_scale)\n",
    "        else:\n",
    "            self.task_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, x, c_true=None, train=False):\n",
    "        # Step 1: Extract pre-concept features\n",
    "        pre_features = self.concept_extractor(x)  # (B, 256)\n",
    "        \n",
    "        # Step 2: Generate contexts and probabilities per concept\n",
    "        contexts = []\n",
    "        c_logits_list = []\n",
    "        \n",
    "        for i, context_layer in enumerate(self.context_layers):\n",
    "            context = context_layer(pre_features)  # (B, emb_size*2)\n",
    "            \n",
    "            # Get probability logit\n",
    "            if self.shared_prob_gen:\n",
    "                logit = self.prob_generator(context)  # (B, 1)\n",
    "            else:\n",
    "                logit = self.prob_generators[i](context)\n",
    "            \n",
    "            contexts.append(context)\n",
    "            c_logits_list.append(logit)\n",
    "        \n",
    "        c_logits = torch.cat(c_logits_list, dim=1)  # (B, 21)\n",
    "        c_probs = torch.sigmoid(c_logits / self.concept_temperature)  # (B, 21)\n",
    "        \n",
    "        # Step 3: Apply intervention (optional during training)\n",
    "        if train and self.intervention_prob > 0 and c_true is not None:\n",
    "            intervention_mask = torch.bernoulli(\n",
    "                torch.ones_like(c_probs) * self.intervention_prob\n",
    "            )\n",
    "            c_probs = c_probs * (1 - intervention_mask) + c_true * intervention_mask\n",
    "        \n",
    "        # Step 4: Mix dual embeddings based on probabilities\n",
    "        concept_embeddings = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            # Split into true/false embeddings\n",
    "            emb_true = context[:, :self.emb_size]       # (B, emb_size)\n",
    "            emb_false = context[:, self.emb_size:]      # (B, emb_size)\n",
    "            \n",
    "            # Concept probability\n",
    "            prob = c_probs[:, i:i+1]  # (B, 1)\n",
    "\n",
    "            # Concept confidence (distance from uncertainty)\n",
    "            # Soft confidence gating\n",
    "            confidence = torch.abs(prob - 0.5) * 2.0  # in [0, 1]\n",
    "\n",
    "            # Add floor to preserve weak signals\n",
    "            confidence = 0.2 + 0.8 * confidence\n",
    "\n",
    "            mixed_emb = emb_true * prob + emb_false * (1 - prob)\n",
    "            mixed_emb = mixed_emb * confidence\n",
    "\n",
    "            concept_embeddings.append(mixed_emb)\n",
    "        \n",
    "        # Concatenate all concept embeddings\n",
    "        c_embeddings = torch.cat(concept_embeddings, dim=1)  # (B, 21*emb_size)\n",
    "        \n",
    "        # Step 5: Task prediction\n",
    "        y_logits = self.task_classifier(c_embeddings)  # (B, 1)\n",
    "        \n",
    "        return c_logits, y_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, c_true = batch\n",
    "        c_logits, y_logits = self.forward(x, c_true=c_true, train=True)\n",
    "        \n",
    "        # Task loss (LDAM)\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss (BCE)\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = task_loss + self.concept_loss_weight * concept_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('train_concept_loss', concept_loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, c_true = batch\n",
    "        c_logits, y_logits = self.forward(x, c_true=c_true, train=False)\n",
    "        \n",
    "        # Task loss\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss (NOW MEANINGFUL - validation has true concept labels!)\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = task_loss + self.concept_loss_weight * concept_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('val_concept_loss', concept_loss, on_epoch=True)\n",
    "        with torch.no_grad():\n",
    "            self.log(\n",
    "                \"train_c_logit_mean\",\n",
    "                c_logits.mean(),\n",
    "                on_epoch=True,\n",
    "                prog_bar=False\n",
    "            )\n",
    "            self.log(\n",
    "                \"train_c_logit_std\",\n",
    "                c_logits.std(),\n",
    "                on_epoch=True,\n",
    "                prog_bar=False\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "print(\"✓ Custom CEM model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom CEM model initialized\n",
      "  Using LDAM Loss (margin=0.5, scale=40)\n",
      "  Concept embedding size: 128\n",
      "  Intervention probability: 0.1\n",
      "  Shared probability generator: True\n",
      "  Class counts: 66 positive, 322 negative\n",
      "  Data source: MAX alternative pipeline (specialist posts)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Custom CEM model\n",
    "custom_cem = CustomCEM(\n",
    "    n_concepts=HYPERPARAMS['n_concepts'],\n",
    "    emb_size=HYPERPARAMS['emb_size'],\n",
    "    input_dim=HYPERPARAMS['embedding_dim'],\n",
    "    shared_prob_gen=HYPERPARAMS['shared_prob_gen'],\n",
    "    intervention_prob=HYPERPARAMS['intervention_prob'],\n",
    "    concept_loss_weight=HYPERPARAMS['concept_loss_weight'],\n",
    "    learning_rate=HYPERPARAMS['learning_rate'],\n",
    "    weight_decay=HYPERPARAMS['weight_decay'],\n",
    "    use_ldam_loss=HYPERPARAMS['use_ldam_loss'],\n",
    "    n_positive=HYPERPARAMS['n_positive'],\n",
    "    n_negative=HYPERPARAMS['n_negative'],\n",
    "    ldam_max_margin=HYPERPARAMS['ldam_max_margin'],\n",
    "    ldam_scale=HYPERPARAMS['ldam_scale'],\n",
    "    concept_temperature=HYPERPARAMS[\"concept_temperature\"],\n",
    "    concept_pos_weight=concept_pos_weight,\n",
    ")\n",
    "\n",
    "print(\"✓ Custom CEM model initialized\")\n",
    "print(f\"  Using LDAM Loss (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\n",
    "print(f\"  Concept embedding size: {HYPERPARAMS['emb_size']}\")\n",
    "print(f\"  Intervention probability: {HYPERPARAMS['intervention_prob']}\")\n",
    "print(f\"  Shared probability generator: {HYPERPARAMS['shared_prob_gen']}\")\n",
    "print(f\"  Class counts: {HYPERPARAMS['n_positive']} positive, {HYPERPARAMS['n_negative']} negative\")\n",
    "print(f\"  Data source: MAX alternative pipeline (specialist posts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer configured\n"
     ]
    }
   ],
   "source": [
    "# Setup trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(OUTPUT_DIR, \"models\"),\n",
    "    filename=\"cem-max-comparison-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "early_stop_cb = EarlyStopping(\n",
    "    monitor=\"val_concept_loss\",\n",
    "    patience=10,\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=HYPERPARAMS['max_epochs'],\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    logger=CSVLogger(save_dir=os.path.join(OUTPUT_DIR, \"logs\"), name=\"cem_max_comparison\"),\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback, early_stop_cb],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type              | Params\n",
      "--------------------------------------------------------\n",
      "0 | concept_extractor | Sequential        | 164 K \n",
      "1 | context_layers    | ModuleList        | 1.4 M \n",
      "2 | prob_generator    | Linear            | 257   \n",
      "3 | task_classifier   | Sequential        | 344 K \n",
      "4 | concept_loss_fn   | BCEWithLogitsLoss | 0     \n",
      "5 | task_loss_fn      | LDAMLoss          | 0     \n",
      "--------------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.562     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd59e862b364b37b950f0b3d391b7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6f5fcfb31047928cdbfda9c8908b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88df0a8f913c4dcdaa9f69ff201eb3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved. New best score: 0.666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ceefb84589442c99ad3d9be07f2ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.064 >= min_delta = 0.0. New best score: 0.603\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92aff58cd3604559af40235bf3b15d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.077 >= min_delta = 0.0. New best score: 0.526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c072c8b2f8514574b9befb49deb47aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097ba0310532487ca083eac96b736b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d29e3b826545d0a85dce3e6278c6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.063 >= min_delta = 0.0. New best score: 0.461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898687d16f0d43c6946683480c53278c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d10f82ee314b70b12259521664b664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.197 >= min_delta = 0.0. New best score: 0.264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48de5599fe94b0d9f0a80f6d41cf3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21004097978e497cb231e8c88e96e5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216e7740ee1a48e7bd7339244b8a2853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.223\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7530346b42478294e733a194e1fb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d42080786ea44cfad96d4d014ad63fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1af78577785412a87ab479445a92aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.025 >= min_delta = 0.0. New best score: 0.198\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce31718e623d45468793f063a6721de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21980c99a268478da1fff62ca3e482fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3127de925341428613b8617c8e8c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025495e3dadd47b2b3834661ed995c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5232a9ea2fa4f2ab0c2776234cd5ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2fbf2ef8fe42088bf4c9286eeba7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fce129866504b25ba3764cf67bea1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e725a1a53c234a4fb52ce798b141b008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0018f8883c44e5b007efb0bd04ba4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad533026d7942a6840bd035f53e8cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_concept_loss did not improve in the last 10 records. Best score: 0.198. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\\n\")\n",
    "trainer.fit(custom_cem, train_loader, val_loader)\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best concept temperature found: 0.500\n"
     ]
    }
   ],
   "source": [
    "# Concept temperature calibration (validation)\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "temps = torch.linspace(0.5, 5.0, steps=20)\n",
    "best_temp = None\n",
    "best_concept_loss = float(\"inf\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for T in temps:\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x_batch, _, c_batch in val_loader:\n",
    "            x_batch = x_batch.to(device_obj)\n",
    "            c_batch = c_batch.to(device_obj)\n",
    "\n",
    "            c_logits, _ = custom_cem(x_batch)\n",
    "            c_probs = torch.sigmoid(c_logits / T)\n",
    "\n",
    "            loss = F.binary_cross_entropy(\n",
    "                c_probs, c_batch, reduction=\"mean\"\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "\n",
    "        if avg_loss < best_concept_loss:\n",
    "            best_concept_loss = avg_loss\n",
    "            best_temp = T.item()\n",
    "\n",
    "print(f\"✓ Best concept temperature found: {best_temp:.3f}\")\n",
    "custom_cem.concept_temperature = best_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "            CONCEPT IMPORTANCE (GRADIENT-BASED)\n",
      "======================================================================\n",
      "Sadness                            : 0.0114\n",
      "Pessimism                          : 0.0580\n",
      "Past failure                       : 0.1018\n",
      "Loss of pleasure                   : 0.0263\n",
      "Guilty feelings                    : 0.0037\n",
      "Punishment feelings                : 0.1249\n",
      "Self-dislike                       : 0.0518\n",
      "Self-criticalness                  : 0.0543\n",
      "Suicidal thoughts or wishes        : 0.0091\n",
      "Crying                             : 0.0665\n",
      "Agitation                          : 0.0077\n",
      "Loss of interest                   : 0.0762\n",
      "Indecisiveness                     : 0.0440\n",
      "Worthlessness                      : 0.0147\n",
      "Loss of energy                     : 0.0703\n",
      "Changes in sleeping pattern        : 0.0326\n",
      "Irritability                       : 0.0217\n",
      "Changes in appetite                : 0.0642\n",
      "Concentration difficulty           : 0.0214\n",
      "Tiredness or fatigue               : 0.0384\n",
      "Loss of interest in sex            : 0.1008\n"
     ]
    }
   ],
   "source": [
    "# Concept importance via gradients\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "concept_grad_accumulator = torch.zeros(N_CONCEPTS, device=device_obj)\n",
    "n_samples = 0\n",
    "\n",
    "for x_batch, _, _ in val_loader:\n",
    "    x_batch = x_batch.to(device_obj)\n",
    "    x_batch.requires_grad = True\n",
    "\n",
    "    c_logits, y_logits = custom_cem(x_batch)\n",
    "    y_prob = torch.sigmoid(y_logits).mean()\n",
    "\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=y_prob,\n",
    "        inputs=c_logits,\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )[0]\n",
    "\n",
    "    concept_grad_accumulator += grads.abs().mean(dim=0)\n",
    "    n_samples += 1\n",
    "\n",
    "concept_importance = (concept_grad_accumulator / n_samples).cpu().numpy()\n",
    "\n",
    "# Normalize for readability\n",
    "concept_importance /= concept_importance.sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"            CONCEPT IMPORTANCE (GRADIENT-BASED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, score in zip(CONCEPT_NAMES, concept_importance):\n",
    "    print(f\"{name:<35}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting decision threshold on validation set...\n",
      "✓ Selected validation threshold: 0.42\n"
     ]
    }
   ],
   "source": [
    "# Validation threshold selection\n",
    "print(\"\\nSelecting decision threshold on validation set...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "\n",
    "# Move model to device\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "y_val_true = []\n",
    "y_val_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in val_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "\n",
    "        _, y_logits = custom_cem(x_batch)\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "\n",
    "        y_val_true.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_val_prob.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "\n",
    "y_val_true = np.array(y_val_true)\n",
    "y_val_prob = np.array(y_val_prob)\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_mcc = -1.0\n",
    "\n",
    "for threshold in np.linspace(0.05, 0.95, 91):\n",
    "    y_pred_temp = (y_val_prob >= threshold).astype(int)\n",
    "    mcc = matthews_corrcoef(y_val_true, y_pred_temp)\n",
    "\n",
    "    if mcc > best_mcc:\n",
    "        best_mcc = mcc\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"✓ Selected validation threshold: {best_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on test set...\n",
      "✓ Inference complete\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test set\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "y_true_list = []\n",
    "y_prob_list = []\n",
    "concept_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in test_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "        \n",
    "        c_logits, y_logits = custom_cem(x_batch)\n",
    "        c_probs = torch.sigmoid(c_logits).cpu().numpy()\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "        \n",
    "        y_true_list.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_prob_list.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "        concept_probs_list.extend(c_probs.tolist())\n",
    "\n",
    "y_true = np.array(y_true_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "concept_probs = np.array(concept_probs_list)\n",
    "\n",
    "print(\"✓ Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "            CONCEPT ACTIVATION PROBABILITY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Concept: Sadness\n",
      "  Mean probability:      0.1295\n",
      "  Std deviation:         0.1687\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4588\n",
      "  Median probability:    0.0263\n",
      "  25th percentile:       0.0008\n",
      "  75th percentile:       0.2378\n",
      "\n",
      "Concept: Pessimism\n",
      "  Mean probability:      0.2051\n",
      "  Std deviation:         0.1584\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4858\n",
      "  Median probability:    0.1773\n",
      "  25th percentile:       0.0531\n",
      "  75th percentile:       0.3493\n",
      "\n",
      "Concept: Past failure\n",
      "  Mean probability:      0.1654\n",
      "  Std deviation:         0.1346\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3956\n",
      "  Median probability:    0.1425\n",
      "  25th percentile:       0.0273\n",
      "  75th percentile:       0.2981\n",
      "\n",
      "Concept: Loss of pleasure\n",
      "  Mean probability:      0.1492\n",
      "  Std deviation:         0.1374\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4052\n",
      "  Median probability:    0.1090\n",
      "  25th percentile:       0.0121\n",
      "  75th percentile:       0.2832\n",
      "\n",
      "Concept: Guilty feelings\n",
      "  Mean probability:      0.1309\n",
      "  Std deviation:         0.1424\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4160\n",
      "  Median probability:    0.0590\n",
      "  25th percentile:       0.0050\n",
      "  75th percentile:       0.2578\n",
      "\n",
      "Concept: Punishment feelings\n",
      "  Mean probability:      0.1767\n",
      "  Std deviation:         0.1606\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4671\n",
      "  Median probability:    0.1256\n",
      "  25th percentile:       0.0198\n",
      "  75th percentile:       0.3376\n",
      "\n",
      "Concept: Self-dislike\n",
      "  Mean probability:      0.1197\n",
      "  Std deviation:         0.1041\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3322\n",
      "  Median probability:    0.0974\n",
      "  25th percentile:       0.0150\n",
      "  75th percentile:       0.2230\n",
      "\n",
      "Concept: Self-criticalness\n",
      "  Mean probability:      0.0905\n",
      "  Std deviation:         0.1023\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3223\n",
      "  Median probability:    0.0327\n",
      "  25th percentile:       0.0015\n",
      "  75th percentile:       0.1865\n",
      "\n",
      "Concept: Suicidal thoughts or wishes\n",
      "  Mean probability:      0.1604\n",
      "  Std deviation:         0.1700\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4604\n",
      "  Median probability:    0.0735\n",
      "  25th percentile:       0.0075\n",
      "  75th percentile:       0.3303\n",
      "\n",
      "Concept: Crying\n",
      "  Mean probability:      0.1103\n",
      "  Std deviation:         0.1141\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3382\n",
      "  Median probability:    0.0557\n",
      "  25th percentile:       0.0034\n",
      "  75th percentile:       0.2223\n",
      "\n",
      "Concept: Agitation\n",
      "  Mean probability:      0.1532\n",
      "  Std deviation:         0.1608\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4450\n",
      "  Median probability:    0.0785\n",
      "  25th percentile:       0.0073\n",
      "  75th percentile:       0.2981\n",
      "\n",
      "Concept: Loss of interest\n",
      "  Mean probability:      0.1541\n",
      "  Std deviation:         0.1687\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4613\n",
      "  Median probability:    0.0691\n",
      "  25th percentile:       0.0061\n",
      "  75th percentile:       0.3029\n",
      "\n",
      "Concept: Indecisiveness\n",
      "  Mean probability:      0.1773\n",
      "  Std deviation:         0.1545\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4221\n",
      "  Median probability:    0.1301\n",
      "  25th percentile:       0.0249\n",
      "  75th percentile:       0.3352\n",
      "\n",
      "Concept: Worthlessness\n",
      "  Mean probability:      0.1680\n",
      "  Std deviation:         0.1751\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4628\n",
      "  Median probability:    0.0846\n",
      "  25th percentile:       0.0087\n",
      "  75th percentile:       0.3357\n",
      "\n",
      "Concept: Loss of energy\n",
      "  Mean probability:      0.1400\n",
      "  Std deviation:         0.1070\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3355\n",
      "  Median probability:    0.1305\n",
      "  25th percentile:       0.0334\n",
      "  75th percentile:       0.2508\n",
      "\n",
      "Concept: Changes in sleeping pattern\n",
      "  Mean probability:      0.1520\n",
      "  Std deviation:         0.1639\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4504\n",
      "  Median probability:    0.0654\n",
      "  25th percentile:       0.0057\n",
      "  75th percentile:       0.3193\n",
      "\n",
      "Concept: Irritability\n",
      "  Mean probability:      0.1956\n",
      "  Std deviation:         0.1644\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4546\n",
      "  Median probability:    0.1548\n",
      "  25th percentile:       0.0325\n",
      "  75th percentile:       0.3656\n",
      "\n",
      "Concept: Changes in appetite\n",
      "  Mean probability:      0.1453\n",
      "  Std deviation:         0.1545\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4292\n",
      "  Median probability:    0.0688\n",
      "  25th percentile:       0.0068\n",
      "  75th percentile:       0.2876\n",
      "\n",
      "Concept: Concentration difficulty\n",
      "  Mean probability:      0.1738\n",
      "  Std deviation:         0.1498\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4151\n",
      "  Median probability:    0.1345\n",
      "  25th percentile:       0.0234\n",
      "  75th percentile:       0.3306\n",
      "\n",
      "Concept: Tiredness or fatigue\n",
      "  Mean probability:      0.1642\n",
      "  Std deviation:         0.1431\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4114\n",
      "  Median probability:    0.1245\n",
      "  25th percentile:       0.0217\n",
      "  75th percentile:       0.3103\n",
      "\n",
      "Concept: Loss of interest in sex\n",
      "  Mean probability:      0.1385\n",
      "  Std deviation:         0.1521\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4194\n",
      "  Median probability:    0.0578\n",
      "  25th percentile:       0.0040\n",
      "  75th percentile:       0.2825\n",
      "\n",
      "======================================================================\n",
      "            SAMPLE CONCEPT PROBABILITIES (FIRST 5)\n",
      "======================================================================\n",
      "Sadness                       : 0.002, 0.111, 0.443, 0.038, 0.016\n",
      "Pessimism                     : 0.067, 0.289, 0.424, 0.195, 0.159\n",
      "Past failure                  : 0.041, 0.268, 0.363, 0.168, 0.131\n",
      "Loss of pleasure              : 0.023, 0.249, 0.335, 0.140, 0.079\n",
      "Guilty feelings               : 0.007, 0.158, 0.408, 0.081, 0.043\n",
      "Punishment feelings           : 0.027, 0.248, 0.444, 0.161, 0.098\n",
      "Self-dislike                  : 0.024, 0.200, 0.243, 0.102, 0.088\n",
      "Self-criticalness             : 0.003, 0.124, 0.280, 0.042, 0.024\n",
      "Suicidal thoughts or wishes   : 0.011, 0.199, 0.451, 0.088, 0.069\n",
      "Crying                        : 0.007, 0.180, 0.271, 0.068, 0.046\n",
      "Agitation                     : 0.011, 0.187, 0.432, 0.100, 0.059\n",
      "Loss of interest              : 0.012, 0.180, 0.432, 0.084, 0.050\n",
      "Indecisiveness                : 0.035, 0.255, 0.413, 0.149, 0.117\n",
      "Worthlessness                 : 0.011, 0.214, 0.453, 0.104, 0.079\n",
      "Loss of energy                : 0.051, 0.200, 0.288, 0.135, 0.103\n",
      "Changes in sleeping pattern   : 0.009, 0.192, 0.430, 0.082, 0.058\n",
      "Irritability                  : 0.049, 0.285, 0.449, 0.176, 0.128\n",
      "Changes in appetite           : 0.010, 0.180, 0.421, 0.087, 0.055\n",
      "Concentration difficulty      : 0.037, 0.253, 0.395, 0.156, 0.107\n",
      "Tiredness or fatigue          : 0.034, 0.250, 0.387, 0.142, 0.108\n",
      "Loss of interest in sex       : 0.005, 0.173, 0.401, 0.086, 0.042\n"
     ]
    }
   ],
   "source": [
    "# Printing concept stats and probabilities\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            CONCEPT ACTIVATION PROBABILITY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    probs = concept_probs[:, i]\n",
    "\n",
    "    print(f\"\\nConcept: {concept_name}\")\n",
    "    print(f\"  Mean probability:      {np.mean(probs):.4f}\")\n",
    "    print(f\"  Std deviation:         {np.std(probs):.4f}\")\n",
    "    print(f\"  Min probability:       {np.min(probs):.4f}\")\n",
    "    print(f\"  Max probability:       {np.max(probs):.4f}\")\n",
    "    print(f\"  Median probability:    {np.median(probs):.4f}\")\n",
    "    print(f\"  25th percentile:       {np.percentile(probs, 25):.4f}\")\n",
    "    print(f\"  75th percentile:       {np.percentile(probs, 75):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            SAMPLE CONCEPT PROBABILITIES (FIRST 5)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_samples_to_show = min(5, concept_probs.shape[0])\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    probs = concept_probs[:n_samples_to_show, i]\n",
    "    probs_str = \", \".join([f\"{p:.3f}\" for p in probs])\n",
    "    print(f\"{concept_name:<30}: {probs_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_prob >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Results Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    TEST SET EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Decision Threshold: 0.42\n",
      "\n",
      "                 CONFUSION MATRIX                 \n",
      "==================================================\n",
      "                     │ Predicted Negative │ Predicted Positive\n",
      "──────────────────────────────────────────────────\n",
      "     Actual Negative │   TN = 295   │   FP = 54   \n",
      "     Actual Positive │   FN = 13    │   TP = 39   \n",
      "==================================================\n",
      "\n",
      "  True Positives:   39/52  ( 75.0% of depression cases caught)\n",
      "  False Negatives:  13/52  ( 25.0% of depression cases MISSED)\n",
      "  True Negatives:  295/349 ( 84.5% of healthy correctly identified)\n",
      "  False Positives:  54/349 ( 15.5% false alarms)\n",
      "\n",
      "Performance Metrics:\n",
      "  Accuracy:                  0.8329\n",
      "  Balanced Accuracy:         0.7976\n",
      "  ROC-AUC:                   0.8932\n",
      "  Matthews Correlation:      0.4738\n",
      "\n",
      "  F1 Score (Binary):         0.5379\n",
      "  F1 Score (Macro):          0.7180\n",
      "  Precision (Binary):        0.4194\n",
      "  Recall (Binary):           0.7500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.96      0.85      0.90       349\n",
      "    Positive       0.42      0.75      0.54        52\n",
      "\n",
      "    accuracy                           0.83       401\n",
      "   macro avg       0.69      0.80      0.72       401\n",
      "weighted avg       0.89      0.83      0.85       401\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute all metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "f1_binary = f1_score(y_true, y_pred, pos_label=1)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "precision_binary = precision_score(y_true, y_pred, pos_label=1)\n",
    "recall_binary = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDecision Threshold: {best_threshold:.2f}\")\n",
    "\n",
    "# Enhanced Confusion Matrix Display\n",
    "print(f\"\\n{'CONFUSION MATRIX':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'':>20} │ {'Predicted Negative':^12} │ {'Predicted Positive':^12}\")\n",
    "print(\"─\"*50)\n",
    "print(f\"{'Actual Negative':>20} │ {f'TN = {tn}':^12} │ {f'FP = {fp}':^12}\")\n",
    "print(f\"{'Actual Positive':>20} │ {f'FN = {fn}':^12} │ {f'TP = {tp}':^12}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n  True Positives:  {tp:>3}/{int(np.sum(y_true)):<3} ({100*tp/np.sum(y_true):>5.1f}% of depression cases caught)\")\n",
    "print(f\"  False Negatives: {fn:>3}/{int(np.sum(y_true)):<3} ({100*fn/np.sum(y_true):>5.1f}% of depression cases MISSED)\")\n",
    "print(f\"  True Negatives:  {tn:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*tn/(len(y_true)-np.sum(y_true)):>5.1f}% of healthy correctly identified)\")\n",
    "print(f\"  False Positives: {fp:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*fp/(len(y_true)-np.sum(y_true)):>5.1f}% false alarms)\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:                  {acc:.4f}\")\n",
    "print(f\"  Balanced Accuracy:         {balanced_acc:.4f}\")\n",
    "print(f\"  ROC-AUC:                   {roc_auc:.4f}\")\n",
    "print(f\"  Matthews Correlation:      {mcc:.4f}\")\n",
    "print(f\"\\n  F1 Score (Binary):         {f1_binary:.4f}\")\n",
    "print(f\"  F1 Score (Macro):          {f1_macro:.4f}\")\n",
    "print(f\"  Precision (Binary):        {precision_binary:.4f}\")\n",
    "print(f\"  Recall (Binary):           {recall_binary:.4f}\")\n",
    "\n",
    "print(\"\\n\" + classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to outputs_cem_max_comparison/results/\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "metrics_dict = {\n",
    "    \"model_type\": \"cem_max_comparison\",\n",
    "    \"dataset\": \"max_alternative_attention_pipeline\",\n",
    "    \"threshold\": float(best_threshold),\n",
    "    \"n_samples\": int(len(y_true)),\n",
    "    \"n_positive\": int(np.sum(y_true)),\n",
    "    \"n_negative\": int(len(y_true) - np.sum(y_true)),\n",
    "    \"accuracy\": float(acc),\n",
    "    \"balanced_accuracy\": float(balanced_acc),\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "    \"mcc\": float(mcc),\n",
    "    \"f1_binary\": float(f1_binary),\n",
    "    \"f1_macro\": float(f1_macro),\n",
    "    \"precision_binary\": float(precision_binary),\n",
    "    \"recall_binary\": float(recall_binary),\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)}\n",
    "}\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"results\"), exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"results/test_metrics.json\"), 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'subject_id': test_subject_ids,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'y_prob': y_prob\n",
    "})\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    predictions_df[concept_name] = concept_probs[:, i]\n",
    "\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_DIR, \"results/test_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"✓ Results saved to {OUTPUT_DIR}/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "     CEM MAX COMPARISON TRAINING COMPLETE (MAX-BASED DATASET)\n",
      "======================================================================\n",
      "\n",
      "Generated files:\n",
      "  Model checkpoint: outputs_cem_max_comparison/models/\n",
      "  Metrics JSON:     outputs_cem_max_comparison/results/test_metrics.json\n",
      "  Predictions CSV:  outputs_cem_max_comparison/results/test_predictions.csv\n",
      "\n",
      "Data source: max_alternative_attention_pipeline\n",
      "  - Uses MAX-based concept scoring (specialist posts)\n",
      "  - Captures posts highly relevant to at least ONE concept\n",
      "  - Comparison with 1c (SUM-based generalist posts)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     CEM MAX COMPARISON TRAINING COMPLETE (MAX-BASED DATASET)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  Model checkpoint: {OUTPUT_DIR}/models/\")\n",
    "print(f\"  Metrics JSON:     {OUTPUT_DIR}/results/test_metrics.json\")\n",
    "print(f\"  Predictions CSV:  {OUTPUT_DIR}/results/test_predictions.csv\")\n",
    "print(f\"\\nData source: max_alternative_attention_pipeline\")\n",
    "print(f\"  - Uses MAX-based concept scoring (specialist posts)\")\n",
    "print(f\"  - Captures posts highly relevant to at least ONE concept\")\n",
    "print(f\"  - Comparison with 1c (SUM-based generalist posts)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38concept_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
