{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Custom CEM Model - Alternative Attention Pipeline (SUM-based)\n",
    "\n",
    "**Runtime:** ~15-20 minutes\n",
    "\n",
    "This notebook:\n",
    "1. Implements CEM from scratch using PyTorch\n",
    "2. Uses LDAM Loss + WeightedRandomSampler for class imbalance\n",
    "3. **Uses alternative dataset with SUM-based concept scoring**\n",
    "4. **Validation set now has TRUE concept labels** (from 20% train split)\n",
    "\n",
    "**Key Difference from Original:**\n",
    "- Uses data from `alternative_attention_pipeline` (SUM of concept similarities)\n",
    "- Validation set has ground-truth concept labels (not zeros)\n",
    "- Can properly evaluate concept prediction during validation\n",
    "\n",
    "**Prerequisites:** Run `0_prepare_alternative_attention_dataset.ipynb` first!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using MacBook GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Detect device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configured\n",
      "  Dataset dir: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/alternative_attention_pipeline\n",
      "  Output dir: outputs_alternate_cem\n",
      "\n",
      "  NOTE: Using ALTERNATIVE dataset (SUM-based concept scoring)\n",
      "        Validation set has TRUE concept labels!\n"
     ]
    }
   ],
   "source": [
    "# Define paths - CHANGED FOR ALTERNATIVE PIPELINE\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT, \"data/processed\")\n",
    "DATASET_DIR = os.path.join(DATA_PROCESSED, \"alternative_attention_pipeline\")  # CHANGED\n",
    "OUTPUT_DIR = \"outputs_alternate_cem\"  # CHANGED\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n",
    "print(\"\\n  NOTE: Using ALTERNATIVE dataset (SUM-based concept scoring)\")\n",
    "print(\"        Validation set has TRUE concept labels!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined 21 BDI-II concepts\n"
     ]
    }
   ],
   "source": [
    "# Define 21 BDI-II concept names\n",
    "CONCEPT_NAMES = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "N_CONCEPTS = len(CONCEPT_NAMES)\n",
    "\n",
    "print(f\"✓ Defined {N_CONCEPTS} BDI-II concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters configured\n",
      "  Using LDAM LOSS (margin=0.7, scale=40)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    # Model architecture\n",
    "    \"embedding_dim\": 384,\n",
    "    \"n_concepts\": 21,\n",
    "    \"n_tasks\": 1,\n",
    "    \"emb_size\": 128,\n",
    "    \n",
    "    # CEM-specific\n",
    "    \"shared_prob_gen\": True,        # Share probability generator across concepts\n",
    "    \"intervention_prob\": 0.1,      # Training intervention probability\n",
    "    \"concept_temperature\": 2.0,\n",
    "\n",
    "    # Training\n",
    "    \"batch_size_train\": 32,\n",
    "    \"batch_size_eval\": 64,\n",
    "    \"max_epochs\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 4e-05,\n",
    "    \n",
    "    # Loss\n",
    "    \"concept_loss_weight\": 1.0,\n",
    "    \n",
    "    # LDAM Loss\n",
    "    \"use_ldam_loss\": True,\n",
    "    \"n_positive\": None,               # Will be set after loading data\n",
    "    \"n_negative\": None,               # Will be set after loading data\n",
    "    \"ldam_max_margin\": 0.7,           # Try: 0.3, 0.5, 0.7, 1.0\n",
    "    \"ldam_scale\": 40,                 # Try: 20, 30, 40, 50\n",
    "    \n",
    "    # Weighted Sampler\n",
    "    \"use_weighted_sampler\": True,\n",
    "}\n",
    "\n",
    "print(\"✓ Hyperparameters configured\")\n",
    "if HYPERPARAMS['use_ldam_loss']:\n",
    "    print(f\"  Using LDAM LOSS (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\n",
    "else:\n",
    "    print(f\"  Using standard BCE loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Section 1: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed datasets...\n",
      "✓ Loaded training data: (388, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "print(\"Loading preprocessed datasets...\")\n",
    "\n",
    "train_data = np.load(os.path.join(DATASET_DIR, \"train_data.npz\"))\n",
    "X_train = train_data['X']\n",
    "C_train = train_data['C']\n",
    "y_train = train_data['y']\n",
    "train_subject_ids = train_data['subject_ids']\n",
    "# ===============================\n",
    "# STEP 4.1: concept loss weighting\n",
    "# ===============================\n",
    "\n",
    "# C_train: (N_train_samples, n_concepts), values in {0,1}\n",
    "concept_pos_counts = C_train.sum(axis=0)\n",
    "concept_neg_counts = C_train.shape[0] - concept_pos_counts\n",
    "\n",
    "concept_pos_weight = torch.tensor(\n",
    "    concept_neg_counts / (concept_pos_counts + 1e-6),\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded training data: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c97f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept pos_weight stats:\n",
      " min: 3.9743590354919434\n",
      " mean: 26.74332618713379\n",
      " max: 95.9999771118164\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "print(\"Concept pos_weight stats:\")\n",
    "print(\" min:\", concept_pos_weight.min().item())\n",
    "print(\" mean:\", concept_pos_weight.mean().item())\n",
    "print(\" max:\", concept_pos_weight.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded validation data: (98, 384)\n",
      "  Validation concept matrix has 126 non-zero values\n",
      "  (Previously was all zeros, now has TRUE labels!)\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "val_data = np.load(os.path.join(DATASET_DIR, \"val_data.npz\"))\n",
    "X_val = val_data['X']\n",
    "C_val = val_data['C']\n",
    "y_val = val_data['y']\n",
    "val_subject_ids = val_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded validation data: {X_val.shape}\")\n",
    "print(f\"  Validation concept matrix has {np.count_nonzero(C_val)} non-zero values\")\n",
    "print(f\"  (Previously was all zeros, now has TRUE labels!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test data: (401, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = np.load(os.path.join(DATASET_DIR, \"test_data.npz\"))\n",
    "X_test = test_data['X']\n",
    "C_test = test_data['C']\n",
    "y_test = test_data['y']\n",
    "test_subject_ids = test_data['subject_ids']\n",
    "\n",
    "print(f\"✓ Loaded test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded class weights:\n",
      "  Negative: 322, Positive: 66\n",
      "  Ratio: 1:4.88\n"
     ]
    }
   ],
   "source": [
    "# Load class weights\n",
    "with open(os.path.join(DATASET_DIR, \"class_weights.json\"), 'r') as f:\n",
    "    class_info = json.load(f)\n",
    "\n",
    "n_positive = class_info['n_positive']\n",
    "n_negative = class_info['n_negative']\n",
    "pos_weight = class_info['pos_weight']\n",
    "\n",
    "# Update HYPERPARAMS with actual class counts for LDAM\n",
    "HYPERPARAMS['n_positive'] = n_positive\n",
    "HYPERPARAMS['n_negative'] = n_negative\n",
    "\n",
    "print(f\"✓ Loaded class weights:\")\n",
    "print(f\"  Negative: {n_negative}, Positive: {n_positive}\")\n",
    "print(f\"  Ratio: 1:{pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Section 2: PyTorch Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ WeightedRandomSampler created:\n",
      "  Negative weight: 0.0031\n",
      "  Positive weight: 0.0152\n",
      "  Expected positive ratio per batch: ~83.0%\n",
      "✓ All DataLoaders created\n"
     ]
    }
   ],
   "source": [
    "class CEMDataset(Dataset):\n",
    "    def __init__(self, X, C, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.C = torch.tensor(C, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.C[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CEMDataset(X_train, C_train, y_train)\n",
    "val_dataset = CEMDataset(X_val, C_val, y_val)\n",
    "test_dataset = CEMDataset(X_test, C_test, y_test)\n",
    "\n",
    "# Create WeightedRandomSampler for batch-level oversampling (if enabled)\n",
    "if HYPERPARAMS['use_weighted_sampler']:\n",
    "    # Compute class sample counts\n",
    "    class_sample_counts = np.bincount(y_train.astype(int))  # [n_negative, n_positive]\n",
    "    weights = 1. / class_sample_counts\n",
    "    sample_weights = weights[y_train.astype(int)]\n",
    "    \n",
    "    # Create sampler\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True  # Allow positive samples to appear multiple times\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ WeightedRandomSampler created:\")\n",
    "    print(f\"  Negative weight: {weights[0]:.4f}\")\n",
    "    print(f\"  Positive weight: {weights[1]:.4f}\")\n",
    "    print(f\"  Expected positive ratio per batch: ~{weights[1]/(weights[0]+weights[1]):.1%}\")\n",
    "    \n",
    "    # Create train loader with sampler (shuffle=False when using sampler)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], sampler=train_sampler)\n",
    "else:\n",
    "    # Standard train loader with shuffle\n",
    "    train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size_train'], shuffle=True)\n",
    "    print(\"✓ Using standard DataLoader (shuffle=True)\")\n",
    "\n",
    "# Validation and test loaders (no sampling)\n",
    "val_loader = DataLoader(val_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=HYPERPARAMS['batch_size_eval'], shuffle=False)\n",
    "\n",
    "print(\"✓ All DataLoaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Section 3: Custom CEM Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom CEM model defined\n"
     ]
    }
   ],
   "source": [
    "# LDAM Loss (for class imbalance)\n",
    "class LDAMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Label-Distribution-Aware Margin (LDAM) Loss for long-tailed recognition.\n",
    "    \n",
    "    Creates class-dependent margins to make decision boundaries harder for minority classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_positive, n_negative, max_margin=0.5, scale=30):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        self.max_margin = max_margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Compute class frequencies\n",
    "        total = n_positive + n_negative\n",
    "        freq_pos = n_positive / total\n",
    "        freq_neg = n_negative / total\n",
    "        \n",
    "        # Compute margins: minority class gets larger margin\n",
    "        margin_pos = max_margin * (freq_pos ** (-0.25))\n",
    "        margin_neg = max_margin * (freq_neg ** (-0.25))\n",
    "        \n",
    "        self.register_buffer('margin_pos', torch.tensor(margin_pos))\n",
    "        self.register_buffer('margin_neg', torch.tensor(margin_neg))\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1).float()\n",
    "        \n",
    "        # Apply class-dependent margins\n",
    "        margin = targets * self.margin_pos + (1 - targets) * (-self.margin_neg)\n",
    "        adjusted_logits = (logits - margin) * self.scale\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(adjusted_logits, targets, reduction='mean')\n",
    "\n",
    "\n",
    "# Custom CEM Implementation\n",
    "class CustomCEM(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Custom Concept Embedding Model (CEM) implementation.\n",
    "    \n",
    "    Architecture:\n",
    "      X → concept_extractor → context_layers → prob_generator → dual_embeddings → task_classifier → y\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_concepts=21,\n",
    "        emb_size=128,\n",
    "        input_dim=384,\n",
    "        shared_prob_gen=True,\n",
    "        intervention_prob=0.25,\n",
    "        concept_loss_weight=1.0,\n",
    "        learning_rate=0.01,\n",
    "        weight_decay=4e-05,\n",
    "        use_ldam_loss=True,\n",
    "        n_positive=83,\n",
    "        n_negative=403,\n",
    "        ldam_max_margin=0.5,\n",
    "        ldam_scale=30,\n",
    "        concept_temperature=2.0,\n",
    "        concept_pos_weight=None,\n",
    "\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.n_concepts = n_concepts\n",
    "        self.emb_size = emb_size\n",
    "        self.intervention_prob = intervention_prob\n",
    "        self.concept_loss_weight = concept_loss_weight\n",
    "        self.concept_temperature = concept_temperature\n",
    "\n",
    "        # Stage 1: Concept Extractor (X → Pre-Concept Features)\n",
    "        self.concept_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 256)  # Pre-concept features\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Context Generators (Features → Dual Embeddings)\n",
    "        # Each concept gets its own context generator\n",
    "        self.context_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, emb_size * 2),  # Dual embeddings (true/false)\n",
    "                nn.LeakyReLU()\n",
    "            ) for _ in range(n_concepts)\n",
    "        ])\n",
    "        \n",
    "        # Stage 3: Probability Generator (Contexts → Concept Probabilities)\n",
    "        if shared_prob_gen:\n",
    "            # Single shared generator for all concepts\n",
    "            self.prob_generator = nn.Linear(emb_size * 2, 1)\n",
    "        else:\n",
    "            # Per-concept probability generators\n",
    "            self.prob_generators = nn.ModuleList([\n",
    "                nn.Linear(emb_size * 2, 1) for _ in range(n_concepts)\n",
    "            ])\n",
    "        \n",
    "        self.shared_prob_gen = shared_prob_gen\n",
    "        \n",
    "        # Stage 4: Task Classifier (Concept Embeddings → Task Output)\n",
    "        self.task_classifier = nn.Sequential(\n",
    "            nn.Linear(n_concepts * emb_size, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        # Loss functions\n",
    "        self.concept_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        if use_ldam_loss:\n",
    "            self.task_loss_fn = LDAMLoss(n_positive, n_negative, ldam_max_margin, ldam_scale)\n",
    "        else:\n",
    "            self.task_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, x, c_true=None, train=False):\n",
    "        # Step 1: Extract pre-concept features\n",
    "        pre_features = self.concept_extractor(x)  # (B, 256)\n",
    "        \n",
    "        # Step 2: Generate contexts and probabilities per concept\n",
    "        contexts = []\n",
    "        c_logits_list = []\n",
    "        \n",
    "        for i, context_layer in enumerate(self.context_layers):\n",
    "            context = context_layer(pre_features)  # (B, emb_size*2)\n",
    "            \n",
    "            # Get probability logit\n",
    "            if self.shared_prob_gen:\n",
    "                logit = self.prob_generator(context)  # (B, 1)\n",
    "            else:\n",
    "                logit = self.prob_generators[i](context)\n",
    "            \n",
    "            contexts.append(context)\n",
    "            c_logits_list.append(logit)\n",
    "        \n",
    "        c_logits = torch.cat(c_logits_list, dim=1)  # (B, 21)\n",
    "        c_probs = torch.sigmoid(c_logits / self.concept_temperature)  # (B, 21)\n",
    "        \n",
    "        # Step 3: Apply intervention (optional during training)\n",
    "        if train and self.intervention_prob > 0 and c_true is not None:\n",
    "            intervention_mask = torch.bernoulli(\n",
    "                torch.ones_like(c_probs) * self.intervention_prob\n",
    "            )\n",
    "            c_probs = c_probs * (1 - intervention_mask) + c_true * intervention_mask\n",
    "        \n",
    "        # Step 4: Mix dual embeddings based on probabilities\n",
    "        concept_embeddings = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            # Split into true/false embeddings\n",
    "            emb_true = context[:, :self.emb_size]       # (B, emb_size)\n",
    "            emb_false = context[:, self.emb_size:]      # (B, emb_size)\n",
    "            \n",
    "            # Concept probability\n",
    "            prob = c_probs[:, i:i+1]  # (B, 1)\n",
    "\n",
    "            # Concept confidence (distance from uncertainty)\n",
    "            # Soft confidence gating\n",
    "            confidence = torch.abs(prob - 0.5) * 2.0  # in [0, 1]\n",
    "\n",
    "            # Add floor to preserve weak signals\n",
    "            confidence = 0.3 + 0.7 * confidence\n",
    "\n",
    "            mixed_emb = emb_true * prob + emb_false * (1 - prob)\n",
    "            mixed_emb = mixed_emb * confidence\n",
    "\n",
    "\n",
    "            concept_embeddings.append(mixed_emb)\n",
    "\n",
    "        \n",
    "        # Concatenate all concept embeddings\n",
    "        c_embeddings = torch.cat(concept_embeddings, dim=1)  # (B, 21*emb_size)\n",
    "        \n",
    "        # Step 5: Task prediction\n",
    "        y_logits = self.task_classifier(c_embeddings)  # (B, 1)\n",
    "        \n",
    "        return c_logits, y_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, c_true = batch\n",
    "        c_logits, y_logits = self.forward(x, c_true=c_true, train=True)\n",
    "        \n",
    "        # Task loss (LDAM)\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss (BCE)\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = task_loss + self.concept_loss_weight * concept_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('train_concept_loss', concept_loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, c_true = batch\n",
    "        c_logits, y_logits = self.forward(x, c_true=c_true, train=False)\n",
    "        \n",
    "        # Task loss\n",
    "        task_loss = self.task_loss_fn(y_logits.squeeze(), y.squeeze())\n",
    "        \n",
    "        # Concept loss (NOW MEANINGFUL - validation has true concept labels!)\n",
    "        concept_loss = self.concept_loss_fn(c_logits, c_true)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = task_loss + self.concept_loss_weight * concept_loss\n",
    "        \n",
    "        # Logging\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_task_loss', task_loss, on_epoch=True)\n",
    "        self.log('val_concept_loss', concept_loss, on_epoch=True)\n",
    "        with torch.no_grad():\n",
    "            self.log(\n",
    "                \"train_c_logit_mean\",\n",
    "                c_logits.mean(),\n",
    "                on_epoch=True,\n",
    "                prog_bar=False\n",
    "            )\n",
    "            self.log(\n",
    "                \"train_c_logit_std\",\n",
    "                c_logits.std(),\n",
    "                on_epoch=True,\n",
    "                prog_bar=False\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "print(\"✓ Custom CEM model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Section 4: Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom CEM model initialized\n",
      "  Using LDAM Loss (margin=0.7, scale=40)\n",
      "  Concept embedding size: 128\n",
      "  Intervention probability: 0.1\n",
      "  Shared probability generator: True\n",
      "  Class counts: 66 positive, 322 negative\n"
     ]
    }
   ],
   "source": [
    "# Initialize Custom CEM model\n",
    "custom_cem = CustomCEM(\n",
    "    n_concepts=HYPERPARAMS['n_concepts'],\n",
    "    emb_size=HYPERPARAMS['emb_size'],\n",
    "    input_dim=HYPERPARAMS['embedding_dim'],\n",
    "    shared_prob_gen=HYPERPARAMS['shared_prob_gen'],\n",
    "    intervention_prob=HYPERPARAMS['intervention_prob'],\n",
    "    concept_loss_weight=HYPERPARAMS['concept_loss_weight'],\n",
    "    learning_rate=HYPERPARAMS['learning_rate'],\n",
    "    weight_decay=HYPERPARAMS['weight_decay'],\n",
    "    use_ldam_loss=HYPERPARAMS['use_ldam_loss'],\n",
    "    n_positive=HYPERPARAMS['n_positive'],\n",
    "    n_negative=HYPERPARAMS['n_negative'],\n",
    "    ldam_max_margin=HYPERPARAMS['ldam_max_margin'],\n",
    "    ldam_scale=HYPERPARAMS['ldam_scale'],\n",
    "    concept_temperature=HYPERPARAMS[\"concept_temperature\"],\n",
    "    concept_pos_weight=concept_pos_weight,\n",
    ")\n",
    "\n",
    "print(\"✓ Custom CEM model initialized\")\n",
    "print(f\"  Using LDAM Loss (margin={HYPERPARAMS['ldam_max_margin']}, scale={HYPERPARAMS['ldam_scale']})\")\n",
    "print(f\"  Concept embedding size: {HYPERPARAMS['emb_size']}\")\n",
    "print(f\"  Intervention probability: {HYPERPARAMS['intervention_prob']}\")\n",
    "print(f\"  Shared probability generator: {HYPERPARAMS['shared_prob_gen']}\")\n",
    "print(f\"  Class counts: {HYPERPARAMS['n_positive']} positive, {HYPERPARAMS['n_negative']} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Section 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer configured\n"
     ]
    }
   ],
   "source": [
    "# Setup trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(OUTPUT_DIR, \"models\"),\n",
    "    filename=\"alternate-cem-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "early_stop_cb = EarlyStopping(\n",
    "    monitor=\"val_concept_loss\",\n",
    "    patience=10,\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=HYPERPARAMS['max_epochs'],\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    logger=CSVLogger(save_dir=os.path.join(OUTPUT_DIR, \"logs\"), name=\"alternate_cem_pipeline\"),\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback, early_stop_cb],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory outputs_alternate_cem/models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name              | Type              | Params\n",
      "--------------------------------------------------------\n",
      "0 | concept_extractor | Sequential        | 164 K \n",
      "1 | context_layers    | ModuleList        | 1.4 M \n",
      "2 | prob_generator    | Linear            | 257   \n",
      "3 | task_classifier   | Sequential        | 344 K \n",
      "4 | concept_loss_fn   | BCEWithLogitsLoss | 0     \n",
      "5 | task_loss_fn      | LDAMLoss          | 0     \n",
      "--------------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.562     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81e4bc9b92b45d5ba4c178f65a60d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a6ed2df1d34c90a6ea395e960243fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82e0f7d10b142eb943915aa709636cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved. New best score: 0.549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541af5c87be541409f32128d8cf8dcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98706a480dc14548a9627137c21439b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.538\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaaa686d1f542148ce6f46dbde3dd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4d1c822ea04866ad04c0f66e0b9664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8cfae1ee1646a9af50a2546f1695c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.102 >= min_delta = 0.0. New best score: 0.435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17e1e42af56485eb662125152951b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93417b6b822f4a0ab23f0027ed70da2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c8affc7cfe4bc3895b03d775d2706f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a3f30dbe6e4dec8f2aa809039938e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedc007e653f48e5aced0e6b55c3f1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aaa6f9324f4e1595d4e4a0a666b1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_concept_loss improved by 0.097 >= min_delta = 0.0. New best score: 0.338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62104efa2eed499c968fc57867a3c76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db24486142574639956f78105befbada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3340e13fff40daa1390af846d643b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7448572e70a43109dab888dc02339f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18792553a56471bb6413c699bd02773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f669d9dd61f24f6abf1a70a20dd09534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4c14149cd14dc08b9ac5a028299232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046e317d51b14d5692b89551c70052d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ef0f76667944d899c6c8ba337fe69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d512eebd37ff40c8a62a7fdf46b6ea88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_concept_loss did not improve in the last 10 records. Best score: 0.338. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\\n\")\n",
    "trainer.fit(custom_cem, train_loader, val_loader)\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e895266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best concept temperature found: 0.500\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# STEP 5.1: Concept temperature calibration (validation)\n",
    "# ===============================\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "temps = torch.linspace(0.5, 5.0, steps=20)\n",
    "best_temp = None\n",
    "best_concept_loss = float(\"inf\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for T in temps:\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x_batch, _, c_batch in val_loader:\n",
    "            x_batch = x_batch.to(device_obj)\n",
    "            c_batch = c_batch.to(device_obj)\n",
    "\n",
    "            c_logits, _ = custom_cem(x_batch)\n",
    "            c_probs = torch.sigmoid(c_logits / T)\n",
    "\n",
    "            loss = F.binary_cross_entropy(\n",
    "                c_probs, c_batch, reduction=\"mean\"\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "\n",
    "        if avg_loss < best_concept_loss:\n",
    "            best_concept_loss = avg_loss\n",
    "            best_temp = T.item()\n",
    "\n",
    "print(f\"✓ Best concept temperature found: {best_temp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "856d09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cem.concept_temperature = best_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55b42b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "            CONCEPT IMPORTANCE (GRADIENT-BASED)\n",
      "======================================================================\n",
      "Sadness                            : 0.0287\n",
      "Pessimism                          : 0.0601\n",
      "Past failure                       : 0.0060\n",
      "Loss of pleasure                   : 0.0050\n",
      "Guilty feelings                    : 0.0837\n",
      "Punishment feelings                : 0.0528\n",
      "Self-dislike                       : 0.1029\n",
      "Self-criticalness                  : 0.0612\n",
      "Suicidal thoughts or wishes        : 0.0815\n",
      "Crying                             : 0.0099\n",
      "Agitation                          : 0.0098\n",
      "Loss of interest                   : 0.0087\n",
      "Indecisiveness                     : 0.0175\n",
      "Worthlessness                      : 0.0551\n",
      "Loss of energy                     : 0.0866\n",
      "Changes in sleeping pattern        : 0.0475\n",
      "Irritability                       : 0.0067\n",
      "Changes in appetite                : 0.0336\n",
      "Concentration difficulty           : 0.0625\n",
      "Tiredness or fatigue               : 0.1636\n",
      "Loss of interest in sex            : 0.0166\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Concept importance via gradients\n",
    "# ===============================\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "concept_grad_accumulator = torch.zeros(N_CONCEPTS, device=device_obj)\n",
    "n_samples = 0\n",
    "\n",
    "for x_batch, _, _ in val_loader:\n",
    "    x_batch = x_batch.to(device_obj)\n",
    "    x_batch.requires_grad = True\n",
    "\n",
    "    c_logits, y_logits = custom_cem(x_batch)\n",
    "    y_prob = torch.sigmoid(y_logits).mean()\n",
    "\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=y_prob,\n",
    "        inputs=c_logits,\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )[0]\n",
    "\n",
    "    concept_grad_accumulator += grads.abs().mean(dim=0)\n",
    "    n_samples += 1\n",
    "\n",
    "concept_importance = (concept_grad_accumulator / n_samples).cpu().numpy()\n",
    "\n",
    "# Normalize for readability\n",
    "concept_importance /= concept_importance.sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"            CONCEPT IMPORTANCE (GRADIENT-BASED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, score in zip(CONCEPT_NAMES, concept_importance):\n",
    "    print(f\"{name:<35}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a744638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline MCC: 0.2220\n",
      "\n",
      "Concept ablation MCC drops:\n",
      "======================================================================\n",
      "Sadness                            : ΔMCC = +0.0000\n",
      "Pessimism                          : ΔMCC = +0.0000\n",
      "Past failure                       : ΔMCC = +0.0000\n",
      "Loss of pleasure                   : ΔMCC = +0.0000\n",
      "Guilty feelings                    : ΔMCC = +0.0000\n",
      "Punishment feelings                : ΔMCC = +0.0000\n",
      "Self-dislike                       : ΔMCC = +0.0000\n",
      "Self-criticalness                  : ΔMCC = +0.0000\n",
      "Suicidal thoughts or wishes        : ΔMCC = +0.0000\n",
      "Crying                             : ΔMCC = +0.0000\n",
      "Agitation                          : ΔMCC = +0.0000\n",
      "Loss of interest                   : ΔMCC = +0.0000\n",
      "Indecisiveness                     : ΔMCC = +0.0000\n",
      "Worthlessness                      : ΔMCC = +0.0000\n",
      "Loss of energy                     : ΔMCC = +0.0000\n",
      "Changes in sleeping pattern        : ΔMCC = +0.0000\n",
      "Irritability                       : ΔMCC = +0.0000\n",
      "Changes in appetite                : ΔMCC = +0.0000\n",
      "Concentration difficulty           : ΔMCC = +0.0000\n",
      "Tiredness or fatigue               : ΔMCC = +0.0000\n",
      "Loss of interest in sex            : ΔMCC = +0.0000\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Concept ablation study\n",
    "# ===============================\n",
    "\n",
    "custom_cem.eval()\n",
    "\n",
    "baseline_probs = []\n",
    "baseline_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, _ in val_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "        _, y_logits = custom_cem(x_batch)\n",
    "        baseline_probs.extend(torch.sigmoid(y_logits).cpu().numpy().squeeze().tolist())\n",
    "        baseline_true.extend(y_batch.numpy().astype(int).tolist())\n",
    "\n",
    "baseline_probs = np.array(baseline_probs)\n",
    "baseline_true = np.array(baseline_true)\n",
    "best_threshold = 0.5\n",
    "\n",
    "baseline_mcc = matthews_corrcoef(\n",
    "    baseline_true,\n",
    "    (baseline_probs >= best_threshold).astype(int)\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline MCC: {baseline_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nConcept ablation MCC drops:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, name in enumerate(CONCEPT_NAMES):\n",
    "    probs = []\n",
    "    true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, c_batch in val_loader:\n",
    "            x_batch = x_batch.to(device_obj)\n",
    "\n",
    "            c_logits, y_logits = custom_cem(x_batch)\n",
    "            c_logits[:, i] = 0.0\n",
    "\n",
    "            y_probs = torch.sigmoid(y_logits).cpu().numpy().squeeze()\n",
    "\n",
    "            probs.extend(y_probs.tolist())\n",
    "            true.extend(y_batch.numpy().astype(int).tolist())\n",
    "\n",
    "    mcc = matthews_corrcoef(true, (np.array(probs) >= best_threshold).astype(int))\n",
    "    drop = baseline_mcc - mcc\n",
    "\n",
    "    print(f\"{name:<35}: ΔMCC = {drop:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Section 6: Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7365f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting decision threshold on validation set...\n",
      "✓ Selected validation threshold: 0.50\n",
      "✓ Validation recall ≥ 0.8, precision = 0.000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# VALIDATION THRESHOLD SELECTION\n",
    "# -----------------------------\n",
    "print(\"\\nSelecting decision threshold on validation set...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "\n",
    "# Move model to device\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "y_val_true = []\n",
    "y_val_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in val_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "\n",
    "        _, y_logits = custom_cem(x_batch)\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "\n",
    "        y_val_true.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_val_prob.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "\n",
    "y_val_true = np.array(y_val_true)\n",
    "y_val_prob = np.array(y_val_prob)\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_precision = 0.0\n",
    "target_recall = 0.80\n",
    "\n",
    "for threshold in np.linspace(0.01, 0.50, 50):\n",
    "    y_pred_temp = (y_val_prob >= threshold).astype(int)\n",
    "\n",
    "    if np.sum(y_pred_temp) == 0:\n",
    "        continue\n",
    "\n",
    "    recall = recall_score(y_val_true, y_pred_temp)\n",
    "    precision = precision_score(y_val_true, y_pred_temp)\n",
    "\n",
    "    if recall >= target_recall and precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"✓ Selected validation threshold: {best_threshold:.2f}\")\n",
    "print(f\"✓ Validation recall ≥ {target_recall}, precision = {best_precision:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on test set...\n",
      "✓ Inference complete\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test set\n",
    "print(\"Running inference on test set...\")\n",
    "\n",
    "custom_cem.eval()\n",
    "device_obj = torch.device(DEVICE)\n",
    "custom_cem = custom_cem.to(device_obj)\n",
    "\n",
    "y_true_list = []\n",
    "y_prob_list = []\n",
    "concept_probs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch, c_batch in test_loader:\n",
    "        x_batch = x_batch.to(device_obj)\n",
    "        \n",
    "        c_logits, y_logits = custom_cem(x_batch)\n",
    "        c_probs = torch.sigmoid(c_logits).cpu().numpy()\n",
    "        y_probs = torch.sigmoid(y_logits).cpu().squeeze().numpy()\n",
    "        \n",
    "        y_true_list.extend(y_batch.numpy().astype(int).tolist())\n",
    "        y_prob_list.extend(y_probs.tolist() if isinstance(y_probs, np.ndarray) else [y_probs])\n",
    "        concept_probs_list.extend(c_probs.tolist())\n",
    "\n",
    "y_true = np.array(y_true_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "concept_probs = np.array(concept_probs_list)\n",
    "\n",
    "print(\"✓ Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28e34a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "            CONCEPT ACTIVATION PROBABILITY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Concept: Sadness\n",
      "  Mean probability:      0.3065\n",
      "  Std deviation:         0.1001\n",
      "  Min probability:       0.0007\n",
      "  Max probability:       0.4374\n",
      "  Median probability:    0.3314\n",
      "  25th percentile:       0.2575\n",
      "  75th percentile:       0.3874\n",
      "\n",
      "Concept: Pessimism\n",
      "  Mean probability:      0.3237\n",
      "  Std deviation:         0.2254\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.9893\n",
      "  Median probability:    0.2551\n",
      "  25th percentile:       0.1536\n",
      "  75th percentile:       0.4543\n",
      "\n",
      "Concept: Past failure\n",
      "  Mean probability:      0.0944\n",
      "  Std deviation:         0.1647\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.7055\n",
      "  Median probability:    0.0018\n",
      "  25th percentile:       0.0002\n",
      "  75th percentile:       0.1119\n",
      "\n",
      "Concept: Loss of pleasure\n",
      "  Mean probability:      0.2404\n",
      "  Std deviation:         0.0943\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3928\n",
      "  Median probability:    0.2543\n",
      "  25th percentile:       0.1866\n",
      "  75th percentile:       0.3139\n",
      "\n",
      "Concept: Guilty feelings\n",
      "  Mean probability:      0.1826\n",
      "  Std deviation:         0.1104\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3784\n",
      "  Median probability:    0.1773\n",
      "  25th percentile:       0.0865\n",
      "  75th percentile:       0.2838\n",
      "\n",
      "Concept: Punishment feelings\n",
      "  Mean probability:      0.3656\n",
      "  Std deviation:         0.0902\n",
      "  Min probability:       0.0104\n",
      "  Max probability:       0.4501\n",
      "  Median probability:    0.4046\n",
      "  25th percentile:       0.3491\n",
      "  75th percentile:       0.4234\n",
      "\n",
      "Concept: Self-dislike\n",
      "  Mean probability:      0.4441\n",
      "  Std deviation:         0.1945\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.8856\n",
      "  Median probability:    0.4673\n",
      "  25th percentile:       0.3143\n",
      "  75th percentile:       0.6033\n",
      "\n",
      "Concept: Self-criticalness\n",
      "  Mean probability:      0.3182\n",
      "  Std deviation:         0.0882\n",
      "  Min probability:       0.0007\n",
      "  Max probability:       0.4501\n",
      "  Median probability:    0.3395\n",
      "  25th percentile:       0.2776\n",
      "  75th percentile:       0.3802\n",
      "\n",
      "Concept: Suicidal thoughts or wishes\n",
      "  Mean probability:      0.2996\n",
      "  Std deviation:         0.0983\n",
      "  Min probability:       0.0007\n",
      "  Max probability:       0.4213\n",
      "  Median probability:    0.3216\n",
      "  25th percentile:       0.2572\n",
      "  75th percentile:       0.3766\n",
      "\n",
      "Concept: Crying\n",
      "  Mean probability:      0.2411\n",
      "  Std deviation:         0.0932\n",
      "  Min probability:       0.0005\n",
      "  Max probability:       0.3593\n",
      "  Median probability:    0.2708\n",
      "  25th percentile:       0.1939\n",
      "  75th percentile:       0.3149\n",
      "\n",
      "Concept: Agitation\n",
      "  Mean probability:      0.3046\n",
      "  Std deviation:         0.1050\n",
      "  Min probability:       0.0002\n",
      "  Max probability:       0.4214\n",
      "  Median probability:    0.3429\n",
      "  25th percentile:       0.2647\n",
      "  75th percentile:       0.3790\n",
      "\n",
      "Concept: Loss of interest\n",
      "  Mean probability:      0.2926\n",
      "  Std deviation:         0.1086\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.4236\n",
      "  Median probability:    0.3260\n",
      "  25th percentile:       0.2339\n",
      "  75th percentile:       0.3799\n",
      "\n",
      "Concept: Indecisiveness\n",
      "  Mean probability:      0.3795\n",
      "  Std deviation:         0.0758\n",
      "  Min probability:       0.0373\n",
      "  Max probability:       0.4413\n",
      "  Median probability:    0.4111\n",
      "  25th percentile:       0.3716\n",
      "  75th percentile:       0.4254\n",
      "\n",
      "Concept: Worthlessness\n",
      "  Mean probability:      0.3758\n",
      "  Std deviation:         0.1084\n",
      "  Min probability:       0.0004\n",
      "  Max probability:       0.5175\n",
      "  Median probability:    0.4077\n",
      "  25th percentile:       0.3261\n",
      "  75th percentile:       0.4597\n",
      "\n",
      "Concept: Loss of energy\n",
      "  Mean probability:      0.3510\n",
      "  Std deviation:         0.0724\n",
      "  Min probability:       0.0171\n",
      "  Max probability:       0.4961\n",
      "  Median probability:    0.3616\n",
      "  25th percentile:       0.3312\n",
      "  75th percentile:       0.3979\n",
      "\n",
      "Concept: Changes in sleeping pattern\n",
      "  Mean probability:      0.1858\n",
      "  Std deviation:         0.0855\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3113\n",
      "  Median probability:    0.2080\n",
      "  25th percentile:       0.1313\n",
      "  75th percentile:       0.2586\n",
      "\n",
      "Concept: Irritability\n",
      "  Mean probability:      0.2262\n",
      "  Std deviation:         0.0993\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       0.3932\n",
      "  Median probability:    0.2402\n",
      "  25th percentile:       0.1636\n",
      "  75th percentile:       0.3096\n",
      "\n",
      "Concept: Changes in appetite\n",
      "  Mean probability:      0.3420\n",
      "  Std deviation:         0.1199\n",
      "  Min probability:       0.0005\n",
      "  Max probability:       0.4774\n",
      "  Median probability:    0.3783\n",
      "  25th percentile:       0.2897\n",
      "  75th percentile:       0.4365\n",
      "\n",
      "Concept: Concentration difficulty\n",
      "  Mean probability:      0.2672\n",
      "  Std deviation:         0.1001\n",
      "  Min probability:       0.0002\n",
      "  Max probability:       0.3893\n",
      "  Median probability:    0.2967\n",
      "  25th percentile:       0.2202\n",
      "  75th percentile:       0.3463\n",
      "\n",
      "Concept: Tiredness or fatigue\n",
      "  Mean probability:      0.4431\n",
      "  Std deviation:         0.0747\n",
      "  Min probability:       0.0354\n",
      "  Max probability:       0.5140\n",
      "  Median probability:    0.4684\n",
      "  25th percentile:       0.4339\n",
      "  75th percentile:       0.4870\n",
      "\n",
      "Concept: Loss of interest in sex\n",
      "  Mean probability:      0.3430\n",
      "  Std deviation:         0.1094\n",
      "  Min probability:       0.0002\n",
      "  Max probability:       0.4593\n",
      "  Median probability:    0.3769\n",
      "  25th percentile:       0.3007\n",
      "  75th percentile:       0.4251\n",
      "\n",
      "======================================================================\n",
      "            SAMPLE CONCEPT PROBABILITIES (FIRST 5)\n",
      "======================================================================\n",
      "Sadness                       : 0.401, 0.349, 0.204, 0.362, 0.396\n",
      "Pessimism                     : 0.282, 0.547, 0.084, 0.232, 0.271\n",
      "Past failure                  : 0.003, 0.276, 0.000, 0.001, 0.003\n",
      "Loss of pleasure              : 0.326, 0.304, 0.131, 0.276, 0.317\n",
      "Guilty feelings               : 0.239, 0.323, 0.042, 0.165, 0.224\n",
      "Punishment feelings           : 0.422, 0.317, 0.415, 0.418, 0.424\n",
      "Self-dislike                  : 0.484, 0.623, 0.101, 0.432, 0.468\n",
      "Self-criticalness             : 0.363, 0.398, 0.196, 0.340, 0.358\n",
      "Suicidal thoughts or wishes   : 0.386, 0.337, 0.195, 0.364, 0.382\n",
      "Crying                        : 0.337, 0.208, 0.207, 0.315, 0.337\n",
      "Agitation                     : 0.399, 0.245, 0.307, 0.381, 0.395\n",
      "Loss of interest              : 0.383, 0.384, 0.120, 0.346, 0.375\n",
      "Indecisiveness                : 0.437, 0.378, 0.366, 0.431, 0.437\n",
      "Worthlessness                 : 0.454, 0.460, 0.243, 0.422, 0.448\n",
      "Loss of energy                : 0.401, 0.431, 0.291, 0.378, 0.395\n",
      "Changes in sleeping pattern   : 0.286, 0.194, 0.095, 0.254, 0.281\n",
      "Irritability                  : 0.329, 0.269, 0.112, 0.278, 0.320\n",
      "Changes in appetite           : 0.450, 0.332, 0.253, 0.433, 0.449\n",
      "Concentration difficulty      : 0.373, 0.277, 0.171, 0.349, 0.370\n",
      "Tiredness or fatigue          : 0.486, 0.457, 0.386, 0.486, 0.486\n",
      "Loss of interest in sex       : 0.439, 0.405, 0.185, 0.405, 0.433\n"
     ]
    }
   ],
   "source": [
    "#Printing concept stats and probabilities\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            CONCEPT ACTIVATION PROBABILITY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    probs = concept_probs[:, i]\n",
    "\n",
    "    print(f\"\\nConcept: {concept_name}\")\n",
    "    print(f\"  Mean probability:      {np.mean(probs):.4f}\")\n",
    "    print(f\"  Std deviation:         {np.std(probs):.4f}\")\n",
    "    print(f\"  Min probability:       {np.min(probs):.4f}\")\n",
    "    print(f\"  Max probability:       {np.max(probs):.4f}\")\n",
    "    print(f\"  Median probability:    {np.median(probs):.4f}\")\n",
    "    print(f\"  25th percentile:       {np.percentile(probs, 25):.4f}\")\n",
    "    print(f\"  75th percentile:       {np.percentile(probs, 75):.4f}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"            SAMPLE CONCEPT PROBABILITIES (FIRST 5)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_samples_to_show = min(5, concept_probs.shape[0])\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    probs = concept_probs[:n_samples_to_show, i]\n",
    "    probs_str = \", \".join([f\"{p:.3f}\" for p in probs])\n",
    "    print(f\"{concept_name:<30}: {probs_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8897964",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_prob >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Section 7: Results Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    TEST SET EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Decision Threshold: 0.50\n",
      "\n",
      "                 CONFUSION MATRIX                 \n",
      "==================================================\n",
      "                     │ Predicted Negative │ Predicted Positive\n",
      "──────────────────────────────────────────────────\n",
      "     Actual Negative │   TN = 250   │   FP = 99   \n",
      "     Actual Positive │   FN = 25    │   TP = 27   \n",
      "==================================================\n",
      "\n",
      "  True Positives:   27/52  ( 51.9% of depression cases caught)\n",
      "  False Negatives:  25/52  ( 48.1% of depression cases MISSED)\n",
      "  True Negatives:  250/349 ( 71.6% of healthy correctly identified)\n",
      "  False Positives:  99/349 ( 28.4% false alarms)\n",
      "\n",
      "Performance Metrics:\n",
      "  Accuracy:                  0.6908\n",
      "  Balanced Accuracy:         0.6178\n",
      "  ROC-AUC:                   0.6777\n",
      "  Matthews Correlation:      0.1705\n",
      "\n",
      "  F1 Score (Binary):         0.3034\n",
      "  F1 Score (Macro):          0.5523\n",
      "  Precision (Binary):        0.2143\n",
      "  Recall (Binary):           0.5192\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.72      0.80       349\n",
      "    Positive       0.21      0.52      0.30        52\n",
      "\n",
      "    accuracy                           0.69       401\n",
      "   macro avg       0.56      0.62      0.55       401\n",
      "weighted avg       0.82      0.69      0.74       401\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute all metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "f1_binary = f1_score(y_true, y_pred, pos_label=1)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "precision_binary = precision_score(y_true, y_pred, pos_label=1)\n",
    "recall_binary = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDecision Threshold: {best_threshold:.2f}\")\n",
    "\n",
    "# Enhanced Confusion Matrix Display\n",
    "print(f\"\\n{'CONFUSION MATRIX':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'':>20} │ {'Predicted Negative':^12} │ {'Predicted Positive':^12}\")\n",
    "print(\"─\"*50)\n",
    "print(f\"{'Actual Negative':>20} │ {f'TN = {tn}':^12} │ {f'FP = {fp}':^12}\")\n",
    "print(f\"{'Actual Positive':>20} │ {f'FN = {fn}':^12} │ {f'TP = {tp}':^12}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n  True Positives:  {tp:>3}/{int(np.sum(y_true)):<3} ({100*tp/np.sum(y_true):>5.1f}% of depression cases caught)\")\n",
    "print(f\"  False Negatives: {fn:>3}/{int(np.sum(y_true)):<3} ({100*fn/np.sum(y_true):>5.1f}% of depression cases MISSED)\")\n",
    "print(f\"  True Negatives:  {tn:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*tn/(len(y_true)-np.sum(y_true)):>5.1f}% of healthy correctly identified)\")\n",
    "print(f\"  False Positives: {fp:>3}/{int(len(y_true)-np.sum(y_true)):<3} ({100*fp/(len(y_true)-np.sum(y_true)):>5.1f}% false alarms)\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:                  {acc:.4f}\")\n",
    "print(f\"  Balanced Accuracy:         {balanced_acc:.4f}\")\n",
    "print(f\"  ROC-AUC:                   {roc_auc:.4f}\")\n",
    "print(f\"  Matthews Correlation:      {mcc:.4f}\")\n",
    "print(f\"\\n  F1 Score (Binary):         {f1_binary:.4f}\")\n",
    "print(f\"  F1 Score (Macro):          {f1_macro:.4f}\")\n",
    "print(f\"  Precision (Binary):        {precision_binary:.4f}\")\n",
    "print(f\"  Recall (Binary):           {recall_binary:.4f}\")\n",
    "\n",
    "print(\"\\n\" + classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to outputs_alternate_cem/results/\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "metrics_dict = {\n",
    "    \"model_type\": \"alternate_cem\",\n",
    "    \"dataset\": \"alternative_attention_pipeline\",\n",
    "    \"threshold\": float(best_threshold),\n",
    "    \"n_samples\": int(len(y_true)),\n",
    "    \"n_positive\": int(np.sum(y_true)),\n",
    "    \"n_negative\": int(len(y_true) - np.sum(y_true)),\n",
    "    \"accuracy\": float(acc),\n",
    "    \"balanced_accuracy\": float(balanced_acc),\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "    \"mcc\": float(mcc),\n",
    "    \"f1_binary\": float(f1_binary),\n",
    "    \"f1_macro\": float(f1_macro),\n",
    "    \"precision_binary\": float(precision_binary),\n",
    "    \"recall_binary\": float(recall_binary),\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)}\n",
    "}\n",
    "\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"results\"), exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"results/test_metrics.json\"), 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'subject_id': test_subject_ids,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'y_prob': y_prob\n",
    "})\n",
    "\n",
    "for i, concept_name in enumerate(CONCEPT_NAMES):\n",
    "    predictions_df[concept_name] = concept_probs[:, i]\n",
    "\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_DIR, \"results/test_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"✓ Results saved to {OUTPUT_DIR}/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "        ALTERNATE CEM TRAINING COMPLETE (SUM-BASED DATASET)\n",
      "======================================================================\n",
      "\n",
      "Generated files:\n",
      "  Model checkpoint: outputs_alternate_cem/models/\n",
      "  Metrics JSON:     outputs_alternate_cem/results/test_metrics.json\n",
      "  Predictions CSV:  outputs_alternate_cem/results/test_predictions.csv\n",
      "\n",
      "Key differences from original pipeline:\n",
      "  - Uses SUM-based concept scoring (captures multi-concept posts)\n",
      "  - Validation set has TRUE concept labels (from train split)\n",
      "  - Better concept supervision during validation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"        ALTERNATE CEM TRAINING COMPLETE (SUM-BASED DATASET)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  Model checkpoint: {OUTPUT_DIR}/models/\")\n",
    "print(f\"  Metrics JSON:     {OUTPUT_DIR}/results/test_metrics.json\")\n",
    "print(f\"  Predictions CSV:  {OUTPUT_DIR}/results/test_predictions.csv\")\n",
    "print(\"\\nKey differences from original pipeline:\")\n",
    "print(\"  - Uses SUM-based concept scoring (captures multi-concept posts)\")\n",
    "print(\"  - Validation set has TRUE concept labels (from train split)\")\n",
    "print(\"  - Better concept supervision during validation\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38concept_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
