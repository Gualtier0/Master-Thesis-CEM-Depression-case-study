{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Build Test Manifest\n",
    "\n",
    "This notebook processes the test data from `/data/raw/test/` and creates validation and test sets.\n",
    "\n",
    "Steps:\n",
    "1. Extract ZIP files containing test subject XMLs\n",
    "2. Load test_golden_truth.txt for labels\n",
    "3. Build manifest of test subjects\n",
    "4. Extract all posts from XMLs\n",
    "5. Retrieve top-20 posts per subject using concept-embedding similarity\n",
    "6. Split 50/50 into validation and test sets (stratified)\n",
    "7. Save two CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/38concept_embedding/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "paths",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test directory: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/raw/test\n",
      "Golden truth: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/raw/test/test_golden_truth.txt\n",
      "Output directory: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "test_dir = os.path.join(project_root, \"data/raw/test\")\n",
    "golden_truth_path = os.path.join(test_dir, \"test_golden_truth.txt\")\n",
    "output_dir = os.path.join(project_root, \"data/processed\")\n",
    "\n",
    "print(f\"Test directory: {test_dir}\")\n",
    "print(f\"Golden truth: {golden_truth_path}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extract_zips",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test chunks to: /var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000gn/T/test_chunks_wgvdks_5\n",
      "Extracted chunk 1\n",
      "Extracted chunk 2\n",
      "Extracted chunk 3\n",
      "Extracted chunk 4\n",
      "Extracted chunk 5\n",
      "Extracted chunk 6\n",
      "Extracted chunk 7\n",
      "Extracted chunk 8\n",
      "Extracted chunk 9\n",
      "Extracted chunk 10\n",
      "\n",
      "Extraction complete. Temporary directory: /var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000gn/T/test_chunks_wgvdks_5\n"
     ]
    }
   ],
   "source": [
    "# Extract ZIP files to temporary directory\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"test_chunks_\")\n",
    "print(f\"Extracting test chunks to: {temp_dir}\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    zip_path = os.path.join(test_dir, f\"chunk {i}.zip\")\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(temp_dir, f\"chunk_{i}\"))\n",
    "        print(f\"Extracted chunk {i}\")\n",
    "    else:\n",
    "        print(f\"Warning: {zip_path} not found\")\n",
    "\n",
    "print(f\"\\nExtraction complete. Temporary directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load_labels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 401 test subjects\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few subjects:\n",
      "         subject_id  label\n",
      "0  test_subject9942      1\n",
      "1  test_subject3986      1\n",
      "2  test_subject6794      1\n",
      "3  test_subject8969      1\n",
      "4  test_subject3988      1\n"
     ]
    }
   ],
   "source": [
    "# Load golden truth labels (TAB-separated)\n",
    "labels_df = pd.read_csv(golden_truth_path, sep='\\t', header=None, names=['subject_id', 'label'])\n",
    "\n",
    "# Strip whitespace from subject_id column (file has leading spaces)\n",
    "labels_df['subject_id'] = labels_df['subject_id'].str.strip()\n",
    "\n",
    "print(f\"Loaded {len(labels_df)} test subjects\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(labels_df['label'].value_counts())\n",
    "print(f\"\\nFirst few subjects:\")\n",
    "print(labels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Helper functions (reused from training notebook)\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def _normalize_text(t: str) -> str:\n",
    "    t = t or \"\"\n",
    "    t = t.replace(\"\\u0000\", \"\")\n",
    "    t = WHITESPACE_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def extract_texts_from_xml(path, min_chars=10):\n",
    "    \"\"\"\n",
    "    Given one subject chunk XML, extract posts.\n",
    "    Each <WRITING> becomes a post: TITLE + TEXT (concatenated).\n",
    "    Returns a list of strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"[XML-Parse-Error] {path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    posts = []\n",
    "    for writing in root.findall(\"WRITING\"):\n",
    "        title = writing.findtext(\"TITLE\") or \"\"\n",
    "        text  = writing.findtext(\"TEXT\") or \"\"\n",
    "\n",
    "        combined = _normalize_text(f\"{title} {text}\".strip())\n",
    "        if len(combined) >= min_chars:\n",
    "            posts.append(combined)\n",
    "\n",
    "    return posts\n",
    "\n",
    "print(\"Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "build_manifest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4010 XML files\n",
      "Found 401 unique subjects in XML files\n",
      "Labels file has 401 subjects\n",
      "\n",
      "Built manifest for 401 subjects\n",
      "\n",
      "Label distribution in manifest:\n",
      "label\n",
      "0    349\n",
      "1     52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few rows:\n",
      "         subject_id                                             chunks  label\n",
      "0  test_subject3081  [/var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000g...      0\n",
      "1  test_subject2751  [/var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000g...      0\n",
      "2  test_subject6974  [/var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000g...      0\n",
      "3   test_subject954  [/var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000g...      0\n",
      "4  test_subject4471  [/var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000g...      1\n"
     ]
    }
   ],
   "source": [
    "# Build manifest: find all XML files for each test subject\n",
    "manifest = {}\n",
    "\n",
    "# Find all XML files in temporary directory\n",
    "pattern = os.path.join(temp_dir, \"**\", \"*.xml\")\n",
    "all_files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "print(f\"Found {len(all_files)} XML files\")\n",
    "\n",
    "for filepath in all_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    # Extract subject ID from filename (e.g., \"test_subject1005_1.xml\")\n",
    "    match = re.match(r\"(test_subject\\d+)_\\d+\\.xml\", filename)\n",
    "    if match:\n",
    "        subject_id = match.group(1)\n",
    "        if subject_id not in manifest:\n",
    "            manifest[subject_id] = []\n",
    "        manifest[subject_id].append(filepath)\n",
    "\n",
    "print(f\"Found {len(manifest)} unique subjects in XML files\")\n",
    "print(f\"Labels file has {len(labels_df)} subjects\")\n",
    "\n",
    "# Build DataFrame\n",
    "manifest_data = []\n",
    "subjects_without_labels = []\n",
    "\n",
    "for subject_id, files in manifest.items():\n",
    "    # Get label from labels_df\n",
    "    label_row = labels_df[labels_df['subject_id'] == subject_id]\n",
    "    if len(label_row) > 0:\n",
    "        label = label_row.iloc[0]['label']\n",
    "        manifest_data.append({\n",
    "            \"subject_id\": subject_id,\n",
    "            \"chunks\": sorted(files),\n",
    "            \"label\": label\n",
    "        })\n",
    "    else:\n",
    "        subjects_without_labels.append(subject_id)\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest_data)\n",
    "\n",
    "print(f\"\\nBuilt manifest for {len(manifest_df)} subjects\")\n",
    "if subjects_without_labels:\n",
    "    print(f\"Warning: {len(subjects_without_labels)} subjects in XML files but not in labels file:\")\n",
    "    print(f\"  First few: {subjects_without_labels[:5]}\")\n",
    "\n",
    "print(f\"\\nLabel distribution in manifest:\")\n",
    "print(manifest_df['label'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(manifest_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extract_posts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 236326 posts from 401 subjects\n",
      "\n",
      "First few posts:\n",
      "         subject_id  label                                               text\n",
      "0  test_subject3081      0                                          Damn it..\n",
      "1  test_subject3081      0                    Easily my favorite song by Tool\n",
      "2  test_subject3081      0  Heard a rumor that they are in the studio and ...\n",
      "3  test_subject3081      0                      You can never be too prepared\n",
      "4  test_subject3081      0                      House of Pies in Houston, Tx?\n"
     ]
    }
   ],
   "source": [
    "# Extract all posts from XMLs\n",
    "def explode_manifest_to_posts(df_manifest):\n",
    "    \"\"\"\n",
    "    Expand manifest DataFrame into a DataFrame of posts.\n",
    "    Each row = one post with subject_id, label, text.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df_manifest.iterrows():\n",
    "        subject_id = row[\"subject_id\"]\n",
    "        label = row[\"label\"]\n",
    "        chunk_paths = row[\"chunks\"]\n",
    "\n",
    "        for file_path in chunk_paths:\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "            except Exception as e:\n",
    "                print(f\"[XML-Parse-Error] {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            for writing in root.findall(\"WRITING\"):\n",
    "                title = (writing.findtext(\"TITLE\") or \"\").strip()\n",
    "                text = (writing.findtext(\"TEXT\") or \"\").strip()\n",
    "\n",
    "                full_text = f\"{title}\\n{text}\" if title else text\n",
    "                full_text = full_text.strip()\n",
    "\n",
    "                if full_text:\n",
    "                    rows.append({\n",
    "                        \"subject_id\": subject_id,\n",
    "                        \"label\": label,\n",
    "                        \"text\": full_text\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "posts_df = explode_manifest_to_posts(manifest_df)\n",
    "print(f\"Extracted {len(posts_df)} posts from {posts_df['subject_id'].nunique()} subjects\")\n",
    "print(f\"\\nFirst few posts:\")\n",
    "print(posts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using MacBook GPU (MPS)\n",
      "SBERT model device: mps:0\n",
      "Loaded SBERT model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Load SBERT model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Check if GPU is available and move model to GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"✓ Using MacBook GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"⚠ Using CPU (this will be slow)\")\n",
    "\n",
    "# Move model to device\n",
    "sbert_model = sbert_model.to(device)\n",
    "print(f\"SBERT model device: {sbert_model.device}\")\n",
    "print(f\"Loaded SBERT model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "concept_embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for 21 concepts...\n",
      "Concept embeddings shape: torch.Size([21, 384])\n"
     ]
    }
   ],
   "source": [
    "# Create concept embeddings (same 21 BDI-II concepts as training)\n",
    "concept_names = [\n",
    "    \"Sadness\", \"Pessimism\", \"Past failure\", \"Loss of pleasure\",\n",
    "    \"Guilty feelings\", \"Punishment feelings\", \"Self-dislike\", \"Self-criticalness\",\n",
    "    \"Suicidal thoughts or wishes\", \"Crying\", \"Agitation\", \"Loss of interest\",\n",
    "    \"Indecisiveness\", \"Worthlessness\", \"Loss of energy\", \"Changes in sleeping pattern\",\n",
    "    \"Irritability\", \"Changes in appetite\", \"Concentration difficulty\",\n",
    "    \"Tiredness or fatigue\", \"Loss of interest in sex\"\n",
    "]\n",
    "\n",
    "print(f\"Creating embeddings for {len(concept_names)} concepts...\")\n",
    "concept_embeddings = sbert_model.encode(concept_names, convert_to_tensor=True)\n",
    "print(f\"Concept embeddings shape: {concept_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "retrieval_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval function defined\n"
     ]
    }
   ],
   "source": [
    "def retrieve_posts_for_subject_concept_sim(subject_id, posts_df, concept_embeddings, k=20):\n",
    "    \"\"\"\n",
    "    Retrieve top-k posts for a subject using concept-embedding similarity.\n",
    "    \n",
    "    This function:\n",
    "    1. Embeds all posts for the subject\n",
    "    2. Computes cosine similarity between each post and ALL 21 concept embeddings\n",
    "    3. For each post, takes the max similarity across all concepts as the relevance score\n",
    "    4. Returns top-k posts with highest concept-relevance scores\n",
    "    \"\"\"\n",
    "    # Get subject's posts\n",
    "    subj_posts = posts_df[posts_df[\"subject_id\"] == subject_id][\"text\"].tolist()\n",
    "    \n",
    "    if len(subj_posts) == 0:\n",
    "        return []\n",
    "    \n",
    "    # If subject has fewer posts than k, return all posts with padding\n",
    "    if len(subj_posts) <= k:\n",
    "        if len(subj_posts) < k:\n",
    "            extra_needed = k - len(subj_posts)\n",
    "            padding = list(np.random.choice(subj_posts, size=extra_needed, replace=True))\n",
    "            return subj_posts + padding\n",
    "        else:\n",
    "            return subj_posts\n",
    "    \n",
    "    # Embed all subject's posts\n",
    "    post_embeddings = sbert_model.encode(subj_posts, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity: [num_posts, num_concepts]\n",
    "    cos_scores = util.cos_sim(post_embeddings, concept_embeddings)\n",
    "    \n",
    "    # For each post, take the maximum similarity across all concepts\n",
    "    max_sim_scores = cos_scores.max(dim=1).values.cpu().numpy()\n",
    "    \n",
    "    # Select top-k posts by relevance score\n",
    "    top_k_indices = np.argpartition(-max_sim_scores, range(min(k, len(subj_posts))))[:k]\n",
    "    \n",
    "    selected_posts = [subj_posts[i] for i in top_k_indices]\n",
    "    \n",
    "    return selected_posts\n",
    "\n",
    "print(\"Retrieval function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "retrieve_all",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving posts using concept-embedding similarity...\n",
      "Processing 401 test subjects...\n",
      "  Processed 50/401 subjects (1.3min elapsed, ~9.2min remaining)\n",
      "  Processed 100/401 subjects (2.8min elapsed, ~8.6min remaining)\n",
      "  Processed 150/401 subjects (4.5min elapsed, ~7.5min remaining)\n",
      "  Processed 200/401 subjects (6.2min elapsed, ~6.2min remaining)\n",
      "  Processed 250/401 subjects (8.3min elapsed, ~5.0min remaining)\n",
      "  Processed 300/401 subjects (10.8min elapsed, ~3.7min remaining)\n",
      "  Processed 350/401 subjects (13.1min elapsed, ~1.9min remaining)\n",
      "  Processed 400/401 subjects (15.1min elapsed, ~0.0min remaining)\n",
      "\n",
      "Completed in 15.2 minutes\n",
      "\n",
      "Results:\n",
      "  Total retrieved posts: 8020\n",
      "  Unique subjects: 401\n",
      "  Posts per subject: 20.0\n",
      "\n",
      "First few rows:\n",
      "         subject_id  label                                               text\n",
      "0  test_subject3081      0             You can't blame him for being hopeful.\n",
      "1  test_subject3081      0                 Worst band ever. Rivals Nickelback\n",
      "2  test_subject3081      0                  Sometimes I feel like a monster..\n",
      "3  test_subject3081      0                             someone stabilize this\n",
      "4  test_subject3081      0  The only unfortunate thing is, I have to liste...\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top-20 posts for each subject\n",
    "retrieved_data = []\n",
    "\n",
    "print(\"Retrieving posts using concept-embedding similarity...\")\n",
    "print(f\"Processing {len(manifest_df)} test subjects...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in manifest_df.iterrows():\n",
    "    subject_id = row[\"subject_id\"]\n",
    "    label = row[\"label\"]\n",
    "    \n",
    "    # Progress indicator every 50 subjects\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time = elapsed / (idx + 1)\n",
    "        remaining = avg_time * (len(manifest_df) - idx - 1)\n",
    "        print(f\"  Processed {idx + 1}/{len(manifest_df)} subjects \"\n",
    "              f\"({elapsed/60:.1f}min elapsed, ~{remaining/60:.1f}min remaining)\")\n",
    "    \n",
    "    posts = retrieve_posts_for_subject_concept_sim(\n",
    "        subject_id, \n",
    "        posts_df, \n",
    "        concept_embeddings, \n",
    "        k=20\n",
    "    )\n",
    "    \n",
    "    for p in posts:\n",
    "        retrieved_data.append({\n",
    "            \"subject_id\": subject_id,\n",
    "            \"label\": label,\n",
    "            \"text\": p\n",
    "        })\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {total_time/60:.1f} minutes\")\n",
    "\n",
    "retrieved_df = pd.DataFrame(retrieved_data)\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total retrieved posts: {len(retrieved_df)}\")\n",
    "print(f\"  Unique subjects: {retrieved_df['subject_id'].nunique()}\")\n",
    "print(f\"  Posts per subject: {len(retrieved_df) / retrieved_df['subject_id'].nunique():.1f}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(retrieved_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "split_val_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: 200 subjects, 4000 posts\n",
      "Test set: 201 subjects, 4020 posts\n",
      "\n",
      "Validation label distribution:\n",
      "label\n",
      "0    174\n",
      "1     26\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "label\n",
      "0    175\n",
      "1     26\n",
      "Name: subject_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split 50/50 into validation and test (stratified by label)\n",
    "# First, get unique subjects with their labels\n",
    "subject_labels = retrieved_df.groupby('subject_id')['label'].first().reset_index()\n",
    "\n",
    "# Stratified split\n",
    "val_subjects, test_subjects = train_test_split(\n",
    "    subject_labels['subject_id'],\n",
    "    test_size=0.5,\n",
    "    stratify=subject_labels['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create validation and test dataframes\n",
    "val_df = retrieved_df[retrieved_df['subject_id'].isin(val_subjects)].copy()\n",
    "test_df = retrieved_df[retrieved_df['subject_id'].isin(test_subjects)].copy()\n",
    "\n",
    "print(f\"Validation set: {val_df['subject_id'].nunique()} subjects, {len(val_df)} posts\")\n",
    "print(f\"Test set: {test_df['subject_id'].nunique()} subjects, {len(test_df)} posts\")\n",
    "\n",
    "print(f\"\\nValidation label distribution:\")\n",
    "print(val_df.groupby('label')['subject_id'].nunique())\n",
    "\n",
    "print(f\"\\nTest label distribution:\")\n",
    "print(test_df.groupby('label')['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation set to: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/retrieved_test_validation.csv\n",
      "Saved test set to: /Users/gualtieromarencoturi/Desktop/thesis/Master-Thesis-CEM-Depression-etc-case-study/data/processed/retrieved_test_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Save validation and test sets\n",
    "val_path = os.path.join(output_dir, \"retrieved_test_validation.csv\")\n",
    "test_path = os.path.join(output_dir, \"retrieved_test_test.csv\")\n",
    "\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Saved validation set to: {val_path}\")\n",
    "print(f\"Saved test set to: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up temporary directory: /var/folders/gb/m6c_r5xx6_14p7mlfjwk29900000gn/T/test_chunks_wgvdks_5\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Cleaned up temporary directory: {temp_dir}\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38concept_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
